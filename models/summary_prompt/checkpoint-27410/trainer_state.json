{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 27410,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0036485032015615595,
      "grad_norm": 0.06168973445892334,
      "learning_rate": 0.00029978547975191534,
      "loss": 3.8209,
      "step": 50
    },
    {
      "epoch": 0.007297006403123119,
      "grad_norm": 0.06945208460092545,
      "learning_rate": 0.0002995665815395841,
      "loss": 3.807,
      "step": 100
    },
    {
      "epoch": 0.010945509604684678,
      "grad_norm": 0.08085992187261581,
      "learning_rate": 0.0002993476833272528,
      "loss": 3.8061,
      "step": 150
    },
    {
      "epoch": 0.014594012806246238,
      "grad_norm": 0.0659402385354042,
      "learning_rate": 0.00029912878511492155,
      "loss": 3.8285,
      "step": 200
    },
    {
      "epoch": 0.018242516007807796,
      "grad_norm": 0.05315134674310684,
      "learning_rate": 0.00029890988690259025,
      "loss": 3.8489,
      "step": 250
    },
    {
      "epoch": 0.021891019209369356,
      "grad_norm": 0.044151876121759415,
      "learning_rate": 0.000298690988690259,
      "loss": 3.8163,
      "step": 300
    },
    {
      "epoch": 0.025539522410930916,
      "grad_norm": 0.04935237020254135,
      "learning_rate": 0.00029847209047792776,
      "loss": 3.776,
      "step": 350
    },
    {
      "epoch": 0.029188025612492476,
      "grad_norm": 0.13402259349822998,
      "learning_rate": 0.00029825319226559646,
      "loss": 3.7775,
      "step": 400
    },
    {
      "epoch": 0.032836528814054036,
      "grad_norm": 0.03836560994386673,
      "learning_rate": 0.0002980342940532652,
      "loss": 3.7656,
      "step": 450
    },
    {
      "epoch": 0.03648503201561559,
      "grad_norm": 0.06438777595758438,
      "learning_rate": 0.00029781539584093397,
      "loss": 3.7798,
      "step": 500
    },
    {
      "epoch": 0.040133535217177156,
      "grad_norm": 0.06501829624176025,
      "learning_rate": 0.0002975964976286027,
      "loss": 3.7823,
      "step": 550
    },
    {
      "epoch": 0.04378203841873871,
      "grad_norm": 0.06458926200866699,
      "learning_rate": 0.0002973775994162714,
      "loss": 3.7709,
      "step": 600
    },
    {
      "epoch": 0.04743054162030027,
      "grad_norm": 0.1818145364522934,
      "learning_rate": 0.00029715870120394013,
      "loss": 3.7925,
      "step": 650
    },
    {
      "epoch": 0.05107904482186183,
      "grad_norm": 0.07774771749973297,
      "learning_rate": 0.0002969398029916089,
      "loss": 3.8046,
      "step": 700
    },
    {
      "epoch": 0.05472754802342339,
      "grad_norm": 0.044946275651454926,
      "learning_rate": 0.0002967209047792776,
      "loss": 3.7508,
      "step": 750
    },
    {
      "epoch": 0.05837605122498495,
      "grad_norm": 0.06420096009969711,
      "learning_rate": 0.00029650200656694634,
      "loss": 3.7801,
      "step": 800
    },
    {
      "epoch": 0.06202455442654651,
      "grad_norm": 0.05023753270506859,
      "learning_rate": 0.0002962831083546151,
      "loss": 3.7852,
      "step": 850
    },
    {
      "epoch": 0.06567305762810807,
      "grad_norm": 0.05120198428630829,
      "learning_rate": 0.0002960642101422838,
      "loss": 3.8047,
      "step": 900
    },
    {
      "epoch": 0.06932156082966963,
      "grad_norm": 0.0664324089884758,
      "learning_rate": 0.00029584531192995255,
      "loss": 3.745,
      "step": 950
    },
    {
      "epoch": 0.07297006403123119,
      "grad_norm": 0.053615815937519073,
      "learning_rate": 0.0002956264137176213,
      "loss": 3.7325,
      "step": 1000
    },
    {
      "epoch": 0.07661856723279274,
      "grad_norm": 0.055131010711193085,
      "learning_rate": 0.00029540751550529,
      "loss": 3.7752,
      "step": 1050
    },
    {
      "epoch": 0.08026707043435431,
      "grad_norm": 0.055140383541584015,
      "learning_rate": 0.00029518861729295877,
      "loss": 3.7759,
      "step": 1100
    },
    {
      "epoch": 0.08391557363591587,
      "grad_norm": 0.05693132057785988,
      "learning_rate": 0.0002949697190806275,
      "loss": 3.8138,
      "step": 1150
    },
    {
      "epoch": 0.08756407683747743,
      "grad_norm": 0.06278181076049805,
      "learning_rate": 0.0002947508208682962,
      "loss": 3.7514,
      "step": 1200
    },
    {
      "epoch": 0.09121258003903898,
      "grad_norm": 0.05038214474916458,
      "learning_rate": 0.0002945319226559649,
      "loss": 3.7932,
      "step": 1250
    },
    {
      "epoch": 0.09486108324060054,
      "grad_norm": 0.07193893194198608,
      "learning_rate": 0.0002943130244436337,
      "loss": 3.7459,
      "step": 1300
    },
    {
      "epoch": 0.09850958644216211,
      "grad_norm": 0.04489973187446594,
      "learning_rate": 0.00029409412623130244,
      "loss": 3.8312,
      "step": 1350
    },
    {
      "epoch": 0.10215808964372367,
      "grad_norm": 0.06196662783622742,
      "learning_rate": 0.00029387522801897114,
      "loss": 3.7967,
      "step": 1400
    },
    {
      "epoch": 0.10580659284528522,
      "grad_norm": 0.04057652875781059,
      "learning_rate": 0.0002936563298066399,
      "loss": 3.7156,
      "step": 1450
    },
    {
      "epoch": 0.10945509604684678,
      "grad_norm": 0.0447261743247509,
      "learning_rate": 0.00029343743159430865,
      "loss": 3.7739,
      "step": 1500
    },
    {
      "epoch": 0.11310359924840833,
      "grad_norm": 0.04624299332499504,
      "learning_rate": 0.00029321853338197735,
      "loss": 3.8316,
      "step": 1550
    },
    {
      "epoch": 0.1167521024499699,
      "grad_norm": 0.05665179342031479,
      "learning_rate": 0.0002929996351696461,
      "loss": 3.7806,
      "step": 1600
    },
    {
      "epoch": 0.12040060565153146,
      "grad_norm": 0.12215862423181534,
      "learning_rate": 0.0002927807369573148,
      "loss": 3.732,
      "step": 1650
    },
    {
      "epoch": 0.12404910885309302,
      "grad_norm": 0.037881702184677124,
      "learning_rate": 0.00029256183874498356,
      "loss": 3.7962,
      "step": 1700
    },
    {
      "epoch": 0.12769761205465457,
      "grad_norm": 0.05145750194787979,
      "learning_rate": 0.0002923429405326523,
      "loss": 3.7747,
      "step": 1750
    },
    {
      "epoch": 0.13134611525621614,
      "grad_norm": 0.04171350598335266,
      "learning_rate": 0.000292124042320321,
      "loss": 3.8249,
      "step": 1800
    },
    {
      "epoch": 0.1349946184577777,
      "grad_norm": 0.060327451676130295,
      "learning_rate": 0.0002919051441079898,
      "loss": 3.8035,
      "step": 1850
    },
    {
      "epoch": 0.13864312165933926,
      "grad_norm": 0.05733056738972664,
      "learning_rate": 0.0002916862458956585,
      "loss": 3.7642,
      "step": 1900
    },
    {
      "epoch": 0.14229162486090083,
      "grad_norm": 0.0331217423081398,
      "learning_rate": 0.00029146734768332723,
      "loss": 3.7772,
      "step": 1950
    },
    {
      "epoch": 0.14594012806246237,
      "grad_norm": 0.04495008662343025,
      "learning_rate": 0.00029124844947099593,
      "loss": 3.843,
      "step": 2000
    },
    {
      "epoch": 0.14958863126402394,
      "grad_norm": 0.03319648653268814,
      "learning_rate": 0.0002910295512586647,
      "loss": 3.7777,
      "step": 2050
    },
    {
      "epoch": 0.15323713446558548,
      "grad_norm": 0.11495686322450638,
      "learning_rate": 0.00029081065304633344,
      "loss": 3.8468,
      "step": 2100
    },
    {
      "epoch": 0.15688563766714705,
      "grad_norm": 0.03723183274269104,
      "learning_rate": 0.00029059175483400214,
      "loss": 3.7911,
      "step": 2150
    },
    {
      "epoch": 0.16053414086870862,
      "grad_norm": 0.3307992219924927,
      "learning_rate": 0.0002903728566216709,
      "loss": 3.7926,
      "step": 2200
    },
    {
      "epoch": 0.16418264407027017,
      "grad_norm": 0.04692172259092331,
      "learning_rate": 0.00029015395840933966,
      "loss": 3.7424,
      "step": 2250
    },
    {
      "epoch": 0.16783114727183174,
      "grad_norm": 0.03813797980546951,
      "learning_rate": 0.00028993506019700836,
      "loss": 3.8132,
      "step": 2300
    },
    {
      "epoch": 0.17147965047339328,
      "grad_norm": 0.08578605949878693,
      "learning_rate": 0.0002897161619846771,
      "loss": 3.7534,
      "step": 2350
    },
    {
      "epoch": 0.17512815367495485,
      "grad_norm": 0.16760919988155365,
      "learning_rate": 0.00028949726377234587,
      "loss": 3.7728,
      "step": 2400
    },
    {
      "epoch": 0.17877665687651642,
      "grad_norm": 0.0770140066742897,
      "learning_rate": 0.00028927836556001457,
      "loss": 3.7526,
      "step": 2450
    },
    {
      "epoch": 0.18242516007807796,
      "grad_norm": 0.046180251985788345,
      "learning_rate": 0.00028905946734768327,
      "loss": 3.8012,
      "step": 2500
    },
    {
      "epoch": 0.18607366327963953,
      "grad_norm": 0.03855273127555847,
      "learning_rate": 0.000288840569135352,
      "loss": 3.89,
      "step": 2550
    },
    {
      "epoch": 0.18972216648120108,
      "grad_norm": 0.033983685076236725,
      "learning_rate": 0.0002886216709230208,
      "loss": 3.7971,
      "step": 2600
    },
    {
      "epoch": 0.19337066968276265,
      "grad_norm": 0.052928801625967026,
      "learning_rate": 0.0002884027727106895,
      "loss": 3.7804,
      "step": 2650
    },
    {
      "epoch": 0.19701917288432422,
      "grad_norm": 0.04947362840175629,
      "learning_rate": 0.00028818387449835824,
      "loss": 3.7944,
      "step": 2700
    },
    {
      "epoch": 0.20066767608588576,
      "grad_norm": 0.03991773724555969,
      "learning_rate": 0.000287964976286027,
      "loss": 3.7954,
      "step": 2750
    },
    {
      "epoch": 0.20431617928744733,
      "grad_norm": 0.046088214963674545,
      "learning_rate": 0.0002877460780736957,
      "loss": 3.7915,
      "step": 2800
    },
    {
      "epoch": 0.20796468248900887,
      "grad_norm": 0.08090098947286606,
      "learning_rate": 0.00028752717986136445,
      "loss": 3.8283,
      "step": 2850
    },
    {
      "epoch": 0.21161318569057044,
      "grad_norm": 0.050669629126787186,
      "learning_rate": 0.0002873082816490332,
      "loss": 3.7318,
      "step": 2900
    },
    {
      "epoch": 0.215261688892132,
      "grad_norm": 0.04391877353191376,
      "learning_rate": 0.0002870893834367019,
      "loss": 3.7945,
      "step": 2950
    },
    {
      "epoch": 0.21891019209369356,
      "grad_norm": 0.0506870299577713,
      "learning_rate": 0.00028687048522437066,
      "loss": 3.8057,
      "step": 3000
    },
    {
      "epoch": 0.22255869529525513,
      "grad_norm": 0.13693498075008392,
      "learning_rate": 0.00028665158701203936,
      "loss": 3.7884,
      "step": 3050
    },
    {
      "epoch": 0.22620719849681667,
      "grad_norm": 0.059501904994249344,
      "learning_rate": 0.0002864326887997081,
      "loss": 3.86,
      "step": 3100
    },
    {
      "epoch": 0.22985570169837824,
      "grad_norm": 0.08309494704008102,
      "learning_rate": 0.0002862137905873768,
      "loss": 3.7692,
      "step": 3150
    },
    {
      "epoch": 0.2335042048999398,
      "grad_norm": 0.057229477912187576,
      "learning_rate": 0.0002859948923750456,
      "loss": 3.815,
      "step": 3200
    },
    {
      "epoch": 0.23715270810150135,
      "grad_norm": 0.05534157156944275,
      "learning_rate": 0.00028577599416271433,
      "loss": 3.8132,
      "step": 3250
    },
    {
      "epoch": 0.24080121130306292,
      "grad_norm": 0.05423249676823616,
      "learning_rate": 0.00028555709595038303,
      "loss": 3.8105,
      "step": 3300
    },
    {
      "epoch": 0.24444971450462447,
      "grad_norm": 0.1370498090982437,
      "learning_rate": 0.0002853381977380518,
      "loss": 3.7925,
      "step": 3350
    },
    {
      "epoch": 0.24809821770618604,
      "grad_norm": 0.050562504678964615,
      "learning_rate": 0.0002851192995257205,
      "loss": 3.7878,
      "step": 3400
    },
    {
      "epoch": 0.2517467209077476,
      "grad_norm": 0.03731625899672508,
      "learning_rate": 0.00028490040131338924,
      "loss": 3.8141,
      "step": 3450
    },
    {
      "epoch": 0.25539522410930915,
      "grad_norm": 0.057750578969717026,
      "learning_rate": 0.000284681503101058,
      "loss": 3.804,
      "step": 3500
    },
    {
      "epoch": 0.2590437273108707,
      "grad_norm": 0.08468908816576004,
      "learning_rate": 0.0002844626048887267,
      "loss": 3.804,
      "step": 3550
    },
    {
      "epoch": 0.2626922305124323,
      "grad_norm": 0.04053274542093277,
      "learning_rate": 0.00028424370667639546,
      "loss": 3.7404,
      "step": 3600
    },
    {
      "epoch": 0.26634073371399386,
      "grad_norm": 0.056923758238554,
      "learning_rate": 0.0002840248084640642,
      "loss": 3.8564,
      "step": 3650
    },
    {
      "epoch": 0.2699892369155554,
      "grad_norm": 0.061479076743125916,
      "learning_rate": 0.0002838059102517329,
      "loss": 3.7302,
      "step": 3700
    },
    {
      "epoch": 0.27363774011711695,
      "grad_norm": 0.03686347231268883,
      "learning_rate": 0.0002835870120394016,
      "loss": 3.7742,
      "step": 3750
    },
    {
      "epoch": 0.2772862433186785,
      "grad_norm": 0.09652872383594513,
      "learning_rate": 0.0002833681138270704,
      "loss": 3.7584,
      "step": 3800
    },
    {
      "epoch": 0.2809347465202401,
      "grad_norm": 0.04618903622031212,
      "learning_rate": 0.0002831492156147391,
      "loss": 3.794,
      "step": 3850
    },
    {
      "epoch": 0.28458324972180166,
      "grad_norm": 0.026881106197834015,
      "learning_rate": 0.0002829303174024078,
      "loss": 3.7537,
      "step": 3900
    },
    {
      "epoch": 0.28823175292336317,
      "grad_norm": 0.047772299498319626,
      "learning_rate": 0.0002827114191900766,
      "loss": 3.8091,
      "step": 3950
    },
    {
      "epoch": 0.29188025612492474,
      "grad_norm": 0.09770311415195465,
      "learning_rate": 0.00028249252097774534,
      "loss": 3.7222,
      "step": 4000
    },
    {
      "epoch": 0.2955287593264863,
      "grad_norm": 0.058687321841716766,
      "learning_rate": 0.00028227362276541404,
      "loss": 3.7192,
      "step": 4050
    },
    {
      "epoch": 0.2991772625280479,
      "grad_norm": 0.03348792344331741,
      "learning_rate": 0.0002820547245530828,
      "loss": 3.8284,
      "step": 4100
    },
    {
      "epoch": 0.30282576572960945,
      "grad_norm": 0.04036444425582886,
      "learning_rate": 0.00028183582634075155,
      "loss": 3.7396,
      "step": 4150
    },
    {
      "epoch": 0.30647426893117097,
      "grad_norm": 0.05977707356214523,
      "learning_rate": 0.00028161692812842025,
      "loss": 3.833,
      "step": 4200
    },
    {
      "epoch": 0.31012277213273254,
      "grad_norm": 0.05918131768703461,
      "learning_rate": 0.000281398029916089,
      "loss": 3.7626,
      "step": 4250
    },
    {
      "epoch": 0.3137712753342941,
      "grad_norm": 0.035686418414115906,
      "learning_rate": 0.00028117913170375776,
      "loss": 3.8577,
      "step": 4300
    },
    {
      "epoch": 0.3174197785358557,
      "grad_norm": 0.03383041173219681,
      "learning_rate": 0.00028096023349142646,
      "loss": 3.7575,
      "step": 4350
    },
    {
      "epoch": 0.32106828173741725,
      "grad_norm": 0.03892826288938522,
      "learning_rate": 0.0002807413352790952,
      "loss": 3.77,
      "step": 4400
    },
    {
      "epoch": 0.32471678493897876,
      "grad_norm": 0.058677367866039276,
      "learning_rate": 0.0002805224370667639,
      "loss": 3.7666,
      "step": 4450
    },
    {
      "epoch": 0.32836528814054033,
      "grad_norm": 0.043243780732154846,
      "learning_rate": 0.0002803035388544327,
      "loss": 3.8098,
      "step": 4500
    },
    {
      "epoch": 0.3320137913421019,
      "grad_norm": 0.048561178147792816,
      "learning_rate": 0.0002800846406421014,
      "loss": 3.7292,
      "step": 4550
    },
    {
      "epoch": 0.3356622945436635,
      "grad_norm": 0.050229985266923904,
      "learning_rate": 0.00027986574242977013,
      "loss": 3.7582,
      "step": 4600
    },
    {
      "epoch": 0.33931079774522505,
      "grad_norm": 0.04608413577079773,
      "learning_rate": 0.0002796468442174389,
      "loss": 3.7968,
      "step": 4650
    },
    {
      "epoch": 0.34295930094678656,
      "grad_norm": 0.03866652026772499,
      "learning_rate": 0.0002794279460051076,
      "loss": 3.7742,
      "step": 4700
    },
    {
      "epoch": 0.34660780414834813,
      "grad_norm": 0.046391092240810394,
      "learning_rate": 0.00027920904779277634,
      "loss": 3.8094,
      "step": 4750
    },
    {
      "epoch": 0.3502563073499097,
      "grad_norm": 0.05525911971926689,
      "learning_rate": 0.00027899014958044505,
      "loss": 3.7614,
      "step": 4800
    },
    {
      "epoch": 0.35390481055147127,
      "grad_norm": 0.2321861833333969,
      "learning_rate": 0.0002787712513681138,
      "loss": 3.8035,
      "step": 4850
    },
    {
      "epoch": 0.35755331375303284,
      "grad_norm": 0.032668691128492355,
      "learning_rate": 0.00027855235315578256,
      "loss": 3.7511,
      "step": 4900
    },
    {
      "epoch": 0.36120181695459436,
      "grad_norm": 0.036952581256628036,
      "learning_rate": 0.00027833345494345126,
      "loss": 3.7405,
      "step": 4950
    },
    {
      "epoch": 0.3648503201561559,
      "grad_norm": 0.03920332342386246,
      "learning_rate": 0.00027811455673112,
      "loss": 3.8023,
      "step": 5000
    },
    {
      "epoch": 0.3684988233577175,
      "grad_norm": 0.042424824088811874,
      "learning_rate": 0.00027789565851878877,
      "loss": 3.765,
      "step": 5050
    },
    {
      "epoch": 0.37214732655927907,
      "grad_norm": 0.05212566256523132,
      "learning_rate": 0.00027767676030645747,
      "loss": 3.7448,
      "step": 5100
    },
    {
      "epoch": 0.37579582976084064,
      "grad_norm": 0.03994728997349739,
      "learning_rate": 0.00027745786209412617,
      "loss": 3.8031,
      "step": 5150
    },
    {
      "epoch": 0.37944433296240215,
      "grad_norm": 0.05328117311000824,
      "learning_rate": 0.00027723896388179493,
      "loss": 3.8124,
      "step": 5200
    },
    {
      "epoch": 0.3830928361639637,
      "grad_norm": 0.03581259399652481,
      "learning_rate": 0.0002770200656694637,
      "loss": 3.7812,
      "step": 5250
    },
    {
      "epoch": 0.3867413393655253,
      "grad_norm": 0.09596124291419983,
      "learning_rate": 0.0002768011674571324,
      "loss": 3.805,
      "step": 5300
    },
    {
      "epoch": 0.39038984256708686,
      "grad_norm": 0.05078890174627304,
      "learning_rate": 0.00027658226924480114,
      "loss": 3.8448,
      "step": 5350
    },
    {
      "epoch": 0.39403834576864843,
      "grad_norm": 0.043389398604631424,
      "learning_rate": 0.0002763633710324699,
      "loss": 3.7753,
      "step": 5400
    },
    {
      "epoch": 0.39768684897020995,
      "grad_norm": 0.03501654788851738,
      "learning_rate": 0.0002761444728201386,
      "loss": 3.7624,
      "step": 5450
    },
    {
      "epoch": 0.4013353521717715,
      "grad_norm": 0.038352906703948975,
      "learning_rate": 0.00027592557460780735,
      "loss": 3.7308,
      "step": 5500
    },
    {
      "epoch": 0.4049838553733331,
      "grad_norm": 0.03946085274219513,
      "learning_rate": 0.0002757066763954761,
      "loss": 3.7477,
      "step": 5550
    },
    {
      "epoch": 0.40863235857489466,
      "grad_norm": 0.0423196405172348,
      "learning_rate": 0.0002754877781831448,
      "loss": 3.8115,
      "step": 5600
    },
    {
      "epoch": 0.41228086177645623,
      "grad_norm": 0.05357174947857857,
      "learning_rate": 0.00027526887997081356,
      "loss": 3.7472,
      "step": 5650
    },
    {
      "epoch": 0.41592936497801775,
      "grad_norm": 0.03342980891466141,
      "learning_rate": 0.0002750499817584823,
      "loss": 3.8031,
      "step": 5700
    },
    {
      "epoch": 0.4195778681795793,
      "grad_norm": 0.045062050223350525,
      "learning_rate": 0.000274831083546151,
      "loss": 3.7107,
      "step": 5750
    },
    {
      "epoch": 0.4232263713811409,
      "grad_norm": 0.04666491597890854,
      "learning_rate": 0.0002746121853338197,
      "loss": 3.7632,
      "step": 5800
    },
    {
      "epoch": 0.42687487458270246,
      "grad_norm": 0.029684990644454956,
      "learning_rate": 0.0002743932871214885,
      "loss": 3.7848,
      "step": 5850
    },
    {
      "epoch": 0.430523377784264,
      "grad_norm": 0.032057274132966995,
      "learning_rate": 0.00027417438890915723,
      "loss": 3.7159,
      "step": 5900
    },
    {
      "epoch": 0.43417188098582554,
      "grad_norm": 0.38756996393203735,
      "learning_rate": 0.00027395549069682593,
      "loss": 3.7925,
      "step": 5950
    },
    {
      "epoch": 0.4378203841873871,
      "grad_norm": 0.03629770502448082,
      "learning_rate": 0.0002737365924844947,
      "loss": 3.7831,
      "step": 6000
    },
    {
      "epoch": 0.4414688873889487,
      "grad_norm": 0.048876747488975525,
      "learning_rate": 0.00027351769427216345,
      "loss": 3.7472,
      "step": 6050
    },
    {
      "epoch": 0.44511739059051025,
      "grad_norm": 0.059854354709386826,
      "learning_rate": 0.00027329879605983215,
      "loss": 3.8124,
      "step": 6100
    },
    {
      "epoch": 0.4487658937920718,
      "grad_norm": 0.07895169407129288,
      "learning_rate": 0.0002730798978475009,
      "loss": 3.7448,
      "step": 6150
    },
    {
      "epoch": 0.45241439699363334,
      "grad_norm": 0.07532142847776413,
      "learning_rate": 0.0002728609996351696,
      "loss": 3.7715,
      "step": 6200
    },
    {
      "epoch": 0.4560629001951949,
      "grad_norm": 0.036540623754262924,
      "learning_rate": 0.00027264210142283836,
      "loss": 3.755,
      "step": 6250
    },
    {
      "epoch": 0.4597114033967565,
      "grad_norm": 0.05499234050512314,
      "learning_rate": 0.0002724232032105071,
      "loss": 3.7566,
      "step": 6300
    },
    {
      "epoch": 0.46335990659831805,
      "grad_norm": 0.20882061123847961,
      "learning_rate": 0.0002722043049981758,
      "loss": 3.7867,
      "step": 6350
    },
    {
      "epoch": 0.4670084097998796,
      "grad_norm": 0.24420703947544098,
      "learning_rate": 0.00027198540678584457,
      "loss": 3.8599,
      "step": 6400
    },
    {
      "epoch": 0.47065691300144114,
      "grad_norm": 0.08462422341108322,
      "learning_rate": 0.00027176650857351327,
      "loss": 3.7701,
      "step": 6450
    },
    {
      "epoch": 0.4743054162030027,
      "grad_norm": 0.058603543788194656,
      "learning_rate": 0.00027154761036118203,
      "loss": 3.7709,
      "step": 6500
    },
    {
      "epoch": 0.4779539194045643,
      "grad_norm": 0.036258019506931305,
      "learning_rate": 0.00027132871214885073,
      "loss": 3.7483,
      "step": 6550
    },
    {
      "epoch": 0.48160242260612585,
      "grad_norm": 0.056518469005823135,
      "learning_rate": 0.0002711098139365195,
      "loss": 3.7614,
      "step": 6600
    },
    {
      "epoch": 0.4852509258076874,
      "grad_norm": 0.04797118529677391,
      "learning_rate": 0.00027089091572418824,
      "loss": 3.8187,
      "step": 6650
    },
    {
      "epoch": 0.48889942900924893,
      "grad_norm": 0.037991564720869064,
      "learning_rate": 0.00027067201751185694,
      "loss": 3.7599,
      "step": 6700
    },
    {
      "epoch": 0.4925479322108105,
      "grad_norm": 0.15909089148044586,
      "learning_rate": 0.0002704531192995257,
      "loss": 3.734,
      "step": 6750
    },
    {
      "epoch": 0.49619643541237207,
      "grad_norm": 0.05104684457182884,
      "learning_rate": 0.00027023422108719445,
      "loss": 3.8102,
      "step": 6800
    },
    {
      "epoch": 0.49984493861393364,
      "grad_norm": 0.04384302347898483,
      "learning_rate": 0.00027001532287486315,
      "loss": 3.736,
      "step": 6850
    },
    {
      "epoch": 0.5034934418154952,
      "grad_norm": 0.03357456624507904,
      "learning_rate": 0.0002697964246625319,
      "loss": 3.7849,
      "step": 6900
    },
    {
      "epoch": 0.5071419450170568,
      "grad_norm": 0.08118319511413574,
      "learning_rate": 0.00026957752645020066,
      "loss": 3.7579,
      "step": 6950
    },
    {
      "epoch": 0.5107904482186183,
      "grad_norm": 0.05475417152047157,
      "learning_rate": 0.00026935862823786937,
      "loss": 3.7692,
      "step": 7000
    },
    {
      "epoch": 0.5144389514201799,
      "grad_norm": 0.038107018917798996,
      "learning_rate": 0.00026913973002553807,
      "loss": 3.7602,
      "step": 7050
    },
    {
      "epoch": 0.5180874546217414,
      "grad_norm": 0.05635403096675873,
      "learning_rate": 0.0002689208318132069,
      "loss": 3.7427,
      "step": 7100
    },
    {
      "epoch": 0.521735957823303,
      "grad_norm": 0.04352715238928795,
      "learning_rate": 0.0002687019336008756,
      "loss": 3.7403,
      "step": 7150
    },
    {
      "epoch": 0.5253844610248646,
      "grad_norm": 0.05898762494325638,
      "learning_rate": 0.0002684830353885443,
      "loss": 3.7781,
      "step": 7200
    },
    {
      "epoch": 0.5290329642264261,
      "grad_norm": 0.03815086930990219,
      "learning_rate": 0.00026826413717621303,
      "loss": 3.816,
      "step": 7250
    },
    {
      "epoch": 0.5326814674279877,
      "grad_norm": 0.039125699549913406,
      "learning_rate": 0.0002680452389638818,
      "loss": 3.7858,
      "step": 7300
    },
    {
      "epoch": 0.5363299706295492,
      "grad_norm": 0.1979663372039795,
      "learning_rate": 0.0002678263407515505,
      "loss": 3.7172,
      "step": 7350
    },
    {
      "epoch": 0.5399784738311108,
      "grad_norm": 0.06419413536787033,
      "learning_rate": 0.00026760744253921925,
      "loss": 3.7665,
      "step": 7400
    },
    {
      "epoch": 0.5436269770326724,
      "grad_norm": 0.044370170682668686,
      "learning_rate": 0.000267388544326888,
      "loss": 3.7519,
      "step": 7450
    },
    {
      "epoch": 0.5472754802342339,
      "grad_norm": 0.04969647526741028,
      "learning_rate": 0.0002671696461145567,
      "loss": 3.8177,
      "step": 7500
    },
    {
      "epoch": 0.5509239834357955,
      "grad_norm": 0.21754583716392517,
      "learning_rate": 0.00026695074790222546,
      "loss": 3.7718,
      "step": 7550
    },
    {
      "epoch": 0.554572486637357,
      "grad_norm": 0.0563935711979866,
      "learning_rate": 0.00026673184968989416,
      "loss": 3.7828,
      "step": 7600
    },
    {
      "epoch": 0.5582209898389185,
      "grad_norm": 0.0720403641462326,
      "learning_rate": 0.0002665129514775629,
      "loss": 3.716,
      "step": 7650
    },
    {
      "epoch": 0.5618694930404802,
      "grad_norm": 0.12610460817813873,
      "learning_rate": 0.0002662940532652316,
      "loss": 3.7034,
      "step": 7700
    },
    {
      "epoch": 0.5655179962420417,
      "grad_norm": 0.05261334776878357,
      "learning_rate": 0.00026607515505290037,
      "loss": 3.7634,
      "step": 7750
    },
    {
      "epoch": 0.5691664994436033,
      "grad_norm": 0.035278305411338806,
      "learning_rate": 0.00026585625684056913,
      "loss": 3.8057,
      "step": 7800
    },
    {
      "epoch": 0.5728150026451648,
      "grad_norm": 0.07556525617837906,
      "learning_rate": 0.00026563735862823783,
      "loss": 3.7993,
      "step": 7850
    },
    {
      "epoch": 0.5764635058467263,
      "grad_norm": 0.037905529141426086,
      "learning_rate": 0.0002654184604159066,
      "loss": 3.8026,
      "step": 7900
    },
    {
      "epoch": 0.580112009048288,
      "grad_norm": 0.04708358272910118,
      "learning_rate": 0.0002651995622035753,
      "loss": 3.725,
      "step": 7950
    },
    {
      "epoch": 0.5837605122498495,
      "grad_norm": 0.03775063157081604,
      "learning_rate": 0.00026498066399124404,
      "loss": 3.7516,
      "step": 8000
    },
    {
      "epoch": 0.5874090154514111,
      "grad_norm": 0.07850445061922073,
      "learning_rate": 0.0002647617657789128,
      "loss": 3.8048,
      "step": 8050
    },
    {
      "epoch": 0.5910575186529726,
      "grad_norm": 0.055150218307971954,
      "learning_rate": 0.0002645428675665815,
      "loss": 3.7952,
      "step": 8100
    },
    {
      "epoch": 0.5947060218545341,
      "grad_norm": 0.04658922925591469,
      "learning_rate": 0.00026432396935425025,
      "loss": 3.7206,
      "step": 8150
    },
    {
      "epoch": 0.5983545250560958,
      "grad_norm": 0.12437862902879715,
      "learning_rate": 0.000264105071141919,
      "loss": 3.7605,
      "step": 8200
    },
    {
      "epoch": 0.6020030282576573,
      "grad_norm": 0.04028536006808281,
      "learning_rate": 0.0002638861729295877,
      "loss": 3.7845,
      "step": 8250
    },
    {
      "epoch": 0.6056515314592189,
      "grad_norm": 0.04889799281954765,
      "learning_rate": 0.0002636672747172564,
      "loss": 3.7051,
      "step": 8300
    },
    {
      "epoch": 0.6093000346607804,
      "grad_norm": 0.0449523963034153,
      "learning_rate": 0.0002634483765049252,
      "loss": 3.8262,
      "step": 8350
    },
    {
      "epoch": 0.6129485378623419,
      "grad_norm": 0.03449857980012894,
      "learning_rate": 0.0002632294782925939,
      "loss": 3.7999,
      "step": 8400
    },
    {
      "epoch": 0.6165970410639036,
      "grad_norm": 0.040457263588905334,
      "learning_rate": 0.0002630105800802626,
      "loss": 3.7261,
      "step": 8450
    },
    {
      "epoch": 0.6202455442654651,
      "grad_norm": 0.03202451765537262,
      "learning_rate": 0.0002627916818679314,
      "loss": 3.7404,
      "step": 8500
    },
    {
      "epoch": 0.6238940474670267,
      "grad_norm": 0.035345982760190964,
      "learning_rate": 0.00026257278365560013,
      "loss": 3.7975,
      "step": 8550
    },
    {
      "epoch": 0.6275425506685882,
      "grad_norm": 0.07346878945827484,
      "learning_rate": 0.00026235388544326884,
      "loss": 3.7793,
      "step": 8600
    },
    {
      "epoch": 0.6311910538701497,
      "grad_norm": 0.041771192103624344,
      "learning_rate": 0.0002621349872309376,
      "loss": 3.763,
      "step": 8650
    },
    {
      "epoch": 0.6348395570717114,
      "grad_norm": 0.038838401436805725,
      "learning_rate": 0.00026191608901860635,
      "loss": 3.7834,
      "step": 8700
    },
    {
      "epoch": 0.6384880602732729,
      "grad_norm": 0.042152874171733856,
      "learning_rate": 0.00026169719080627505,
      "loss": 3.7367,
      "step": 8750
    },
    {
      "epoch": 0.6421365634748345,
      "grad_norm": 0.061544034630060196,
      "learning_rate": 0.0002614782925939438,
      "loss": 3.7704,
      "step": 8800
    },
    {
      "epoch": 0.645785066676396,
      "grad_norm": 0.07182334363460541,
      "learning_rate": 0.00026125939438161256,
      "loss": 3.8205,
      "step": 8850
    },
    {
      "epoch": 0.6494335698779575,
      "grad_norm": 0.023128552362322807,
      "learning_rate": 0.00026104049616928126,
      "loss": 3.7306,
      "step": 8900
    },
    {
      "epoch": 0.6530820730795192,
      "grad_norm": 0.1577276885509491,
      "learning_rate": 0.00026082159795695,
      "loss": 3.786,
      "step": 8950
    },
    {
      "epoch": 0.6567305762810807,
      "grad_norm": 0.03565877676010132,
      "learning_rate": 0.0002606026997446187,
      "loss": 3.7752,
      "step": 9000
    },
    {
      "epoch": 0.6603790794826423,
      "grad_norm": 0.027303671464323997,
      "learning_rate": 0.00026038380153228747,
      "loss": 3.7949,
      "step": 9050
    },
    {
      "epoch": 0.6640275826842038,
      "grad_norm": 0.054411400109529495,
      "learning_rate": 0.0002601649033199562,
      "loss": 3.7443,
      "step": 9100
    },
    {
      "epoch": 0.6676760858857653,
      "grad_norm": 0.04960021376609802,
      "learning_rate": 0.00025994600510762493,
      "loss": 3.7069,
      "step": 9150
    },
    {
      "epoch": 0.671324589087327,
      "grad_norm": 0.09200795739889145,
      "learning_rate": 0.0002597271068952937,
      "loss": 3.7692,
      "step": 9200
    },
    {
      "epoch": 0.6749730922888885,
      "grad_norm": 0.03438730165362358,
      "learning_rate": 0.0002595082086829624,
      "loss": 3.7694,
      "step": 9250
    },
    {
      "epoch": 0.6786215954904501,
      "grad_norm": 0.035373587161302567,
      "learning_rate": 0.00025928931047063114,
      "loss": 3.7894,
      "step": 9300
    },
    {
      "epoch": 0.6822700986920116,
      "grad_norm": 0.043179698288440704,
      "learning_rate": 0.00025907041225829984,
      "loss": 3.7602,
      "step": 9350
    },
    {
      "epoch": 0.6859186018935731,
      "grad_norm": 0.028434082865715027,
      "learning_rate": 0.0002588515140459686,
      "loss": 3.79,
      "step": 9400
    },
    {
      "epoch": 0.6895671050951347,
      "grad_norm": 0.023637497797608376,
      "learning_rate": 0.00025863261583363735,
      "loss": 3.6746,
      "step": 9450
    },
    {
      "epoch": 0.6932156082966963,
      "grad_norm": 0.04148092865943909,
      "learning_rate": 0.00025841371762130606,
      "loss": 3.7167,
      "step": 9500
    },
    {
      "epoch": 0.6968641114982579,
      "grad_norm": 0.037714675068855286,
      "learning_rate": 0.0002581948194089748,
      "loss": 3.8069,
      "step": 9550
    },
    {
      "epoch": 0.7005126146998194,
      "grad_norm": 0.1351778656244278,
      "learning_rate": 0.00025797592119664357,
      "loss": 3.7434,
      "step": 9600
    },
    {
      "epoch": 0.7041611179013809,
      "grad_norm": 0.04339450225234032,
      "learning_rate": 0.00025775702298431227,
      "loss": 3.7455,
      "step": 9650
    },
    {
      "epoch": 0.7078096211029425,
      "grad_norm": 0.0386631041765213,
      "learning_rate": 0.00025753812477198097,
      "loss": 3.7262,
      "step": 9700
    },
    {
      "epoch": 0.7114581243045041,
      "grad_norm": 0.029608257114887238,
      "learning_rate": 0.0002573192265596497,
      "loss": 3.7308,
      "step": 9750
    },
    {
      "epoch": 0.7151066275060657,
      "grad_norm": 0.044863514602184296,
      "learning_rate": 0.0002571003283473185,
      "loss": 3.7836,
      "step": 9800
    },
    {
      "epoch": 0.7187551307076272,
      "grad_norm": 0.03969186171889305,
      "learning_rate": 0.0002568814301349872,
      "loss": 3.7858,
      "step": 9850
    },
    {
      "epoch": 0.7224036339091887,
      "grad_norm": 0.033198028802871704,
      "learning_rate": 0.00025666253192265594,
      "loss": 3.805,
      "step": 9900
    },
    {
      "epoch": 0.7260521371107503,
      "grad_norm": 0.04711172729730606,
      "learning_rate": 0.0002564436337103247,
      "loss": 3.7364,
      "step": 9950
    },
    {
      "epoch": 0.7297006403123119,
      "grad_norm": 0.04380730539560318,
      "learning_rate": 0.0002562247354979934,
      "loss": 3.7527,
      "step": 10000
    },
    {
      "epoch": 0.7333491435138735,
      "grad_norm": 0.03492945432662964,
      "learning_rate": 0.00025600583728566215,
      "loss": 3.788,
      "step": 10050
    },
    {
      "epoch": 0.736997646715435,
      "grad_norm": 0.040144167840480804,
      "learning_rate": 0.0002557869390733309,
      "loss": 3.787,
      "step": 10100
    },
    {
      "epoch": 0.7406461499169965,
      "grad_norm": 0.3803487718105316,
      "learning_rate": 0.0002555680408609996,
      "loss": 3.7476,
      "step": 10150
    },
    {
      "epoch": 0.7442946531185581,
      "grad_norm": 0.03278876096010208,
      "learning_rate": 0.00025534914264866836,
      "loss": 3.7914,
      "step": 10200
    },
    {
      "epoch": 0.7479431563201197,
      "grad_norm": 0.050172772258520126,
      "learning_rate": 0.0002551302444363371,
      "loss": 3.8092,
      "step": 10250
    },
    {
      "epoch": 0.7515916595216813,
      "grad_norm": 0.0357128269970417,
      "learning_rate": 0.0002549113462240058,
      "loss": 3.7576,
      "step": 10300
    },
    {
      "epoch": 0.7552401627232428,
      "grad_norm": 0.02880084700882435,
      "learning_rate": 0.0002546924480116745,
      "loss": 3.7878,
      "step": 10350
    },
    {
      "epoch": 0.7588886659248043,
      "grad_norm": 0.041418302804231644,
      "learning_rate": 0.0002544735497993433,
      "loss": 3.7377,
      "step": 10400
    },
    {
      "epoch": 0.7625371691263659,
      "grad_norm": 0.05331307649612427,
      "learning_rate": 0.00025425465158701203,
      "loss": 3.7702,
      "step": 10450
    },
    {
      "epoch": 0.7661856723279274,
      "grad_norm": 0.037208449095487595,
      "learning_rate": 0.00025403575337468073,
      "loss": 3.8176,
      "step": 10500
    },
    {
      "epoch": 0.7698341755294891,
      "grad_norm": 0.1868438571691513,
      "learning_rate": 0.0002538168551623495,
      "loss": 3.7435,
      "step": 10550
    },
    {
      "epoch": 0.7734826787310506,
      "grad_norm": 0.03965349867939949,
      "learning_rate": 0.00025359795695001824,
      "loss": 3.7943,
      "step": 10600
    },
    {
      "epoch": 0.7771311819326121,
      "grad_norm": 0.04679163917899132,
      "learning_rate": 0.00025337905873768694,
      "loss": 3.7727,
      "step": 10650
    },
    {
      "epoch": 0.7807796851341737,
      "grad_norm": 0.05407506600022316,
      "learning_rate": 0.0002531601605253557,
      "loss": 3.7681,
      "step": 10700
    },
    {
      "epoch": 0.7844281883357352,
      "grad_norm": 0.052542489022016525,
      "learning_rate": 0.0002529412623130244,
      "loss": 3.6909,
      "step": 10750
    },
    {
      "epoch": 0.7880766915372969,
      "grad_norm": 0.04365282878279686,
      "learning_rate": 0.00025272236410069316,
      "loss": 3.8204,
      "step": 10800
    },
    {
      "epoch": 0.7917251947388584,
      "grad_norm": 0.03474139794707298,
      "learning_rate": 0.0002525034658883619,
      "loss": 3.7248,
      "step": 10850
    },
    {
      "epoch": 0.7953736979404199,
      "grad_norm": 0.03610363230109215,
      "learning_rate": 0.0002522845676760306,
      "loss": 3.7517,
      "step": 10900
    },
    {
      "epoch": 0.7990222011419815,
      "grad_norm": 0.039271511137485504,
      "learning_rate": 0.00025206566946369937,
      "loss": 3.7778,
      "step": 10950
    },
    {
      "epoch": 0.802670704343543,
      "grad_norm": 0.039804987609386444,
      "learning_rate": 0.00025184677125136807,
      "loss": 3.7286,
      "step": 11000
    },
    {
      "epoch": 0.8063192075451047,
      "grad_norm": 0.16726399958133698,
      "learning_rate": 0.0002516278730390368,
      "loss": 3.7227,
      "step": 11050
    },
    {
      "epoch": 0.8099677107466662,
      "grad_norm": 0.04762920364737511,
      "learning_rate": 0.0002514089748267055,
      "loss": 3.7642,
      "step": 11100
    },
    {
      "epoch": 0.8136162139482277,
      "grad_norm": 0.03606169670820236,
      "learning_rate": 0.0002511900766143743,
      "loss": 3.7091,
      "step": 11150
    },
    {
      "epoch": 0.8172647171497893,
      "grad_norm": 0.033103834837675095,
      "learning_rate": 0.00025097117840204304,
      "loss": 3.7931,
      "step": 11200
    },
    {
      "epoch": 0.8209132203513508,
      "grad_norm": 0.039384398609399796,
      "learning_rate": 0.00025075228018971174,
      "loss": 3.7181,
      "step": 11250
    },
    {
      "epoch": 0.8245617235529125,
      "grad_norm": 0.04030264914035797,
      "learning_rate": 0.0002505333819773805,
      "loss": 3.7372,
      "step": 11300
    },
    {
      "epoch": 0.828210226754474,
      "grad_norm": 0.0351419635117054,
      "learning_rate": 0.00025031448376504925,
      "loss": 3.761,
      "step": 11350
    },
    {
      "epoch": 0.8318587299560355,
      "grad_norm": 0.046962421387434006,
      "learning_rate": 0.00025009558555271795,
      "loss": 3.796,
      "step": 11400
    },
    {
      "epoch": 0.8355072331575971,
      "grad_norm": 0.042807839810848236,
      "learning_rate": 0.0002498766873403867,
      "loss": 3.7763,
      "step": 11450
    },
    {
      "epoch": 0.8391557363591586,
      "grad_norm": 0.054092634469270706,
      "learning_rate": 0.00024965778912805546,
      "loss": 3.7508,
      "step": 11500
    },
    {
      "epoch": 0.8428042395607203,
      "grad_norm": 0.03967931121587753,
      "learning_rate": 0.00024943889091572416,
      "loss": 3.7436,
      "step": 11550
    },
    {
      "epoch": 0.8464527427622818,
      "grad_norm": 0.03748660907149315,
      "learning_rate": 0.00024921999270339286,
      "loss": 3.7681,
      "step": 11600
    },
    {
      "epoch": 0.8501012459638433,
      "grad_norm": 0.06830700486898422,
      "learning_rate": 0.0002490010944910617,
      "loss": 3.7668,
      "step": 11650
    },
    {
      "epoch": 0.8537497491654049,
      "grad_norm": 0.04274776205420494,
      "learning_rate": 0.0002487821962787304,
      "loss": 3.8394,
      "step": 11700
    },
    {
      "epoch": 0.8573982523669664,
      "grad_norm": 0.03649476543068886,
      "learning_rate": 0.0002485632980663991,
      "loss": 3.7252,
      "step": 11750
    },
    {
      "epoch": 0.861046755568528,
      "grad_norm": 0.0675521269440651,
      "learning_rate": 0.00024834439985406783,
      "loss": 3.7771,
      "step": 11800
    },
    {
      "epoch": 0.8646952587700896,
      "grad_norm": 0.045794982463121414,
      "learning_rate": 0.0002481255016417366,
      "loss": 3.7628,
      "step": 11850
    },
    {
      "epoch": 0.8683437619716511,
      "grad_norm": 0.036884695291519165,
      "learning_rate": 0.0002479066034294053,
      "loss": 3.7131,
      "step": 11900
    },
    {
      "epoch": 0.8719922651732127,
      "grad_norm": 0.05138828232884407,
      "learning_rate": 0.00024768770521707404,
      "loss": 3.7466,
      "step": 11950
    },
    {
      "epoch": 0.8756407683747742,
      "grad_norm": 0.11742337793111801,
      "learning_rate": 0.0002474688070047428,
      "loss": 3.7503,
      "step": 12000
    },
    {
      "epoch": 0.8792892715763359,
      "grad_norm": 0.03942801430821419,
      "learning_rate": 0.0002472499087924115,
      "loss": 3.7665,
      "step": 12050
    },
    {
      "epoch": 0.8829377747778974,
      "grad_norm": 0.16825155913829803,
      "learning_rate": 0.00024703101058008026,
      "loss": 3.7422,
      "step": 12100
    },
    {
      "epoch": 0.8865862779794589,
      "grad_norm": 0.03790099173784256,
      "learning_rate": 0.00024681211236774896,
      "loss": 3.7001,
      "step": 12150
    },
    {
      "epoch": 0.8902347811810205,
      "grad_norm": 0.048273518681526184,
      "learning_rate": 0.0002465932141554177,
      "loss": 3.8242,
      "step": 12200
    },
    {
      "epoch": 0.893883284382582,
      "grad_norm": 0.05202070251107216,
      "learning_rate": 0.00024637431594308647,
      "loss": 3.7517,
      "step": 12250
    },
    {
      "epoch": 0.8975317875841436,
      "grad_norm": 0.0339135117828846,
      "learning_rate": 0.00024615541773075517,
      "loss": 3.7364,
      "step": 12300
    },
    {
      "epoch": 0.9011802907857052,
      "grad_norm": 0.05662667378783226,
      "learning_rate": 0.0002459365195184239,
      "loss": 3.7294,
      "step": 12350
    },
    {
      "epoch": 0.9048287939872667,
      "grad_norm": 0.04340604320168495,
      "learning_rate": 0.0002457176213060926,
      "loss": 3.7599,
      "step": 12400
    },
    {
      "epoch": 0.9084772971888283,
      "grad_norm": 0.03602934628725052,
      "learning_rate": 0.0002454987230937614,
      "loss": 3.7934,
      "step": 12450
    },
    {
      "epoch": 0.9121258003903898,
      "grad_norm": 0.051781877875328064,
      "learning_rate": 0.0002452798248814301,
      "loss": 3.757,
      "step": 12500
    },
    {
      "epoch": 0.9157743035919514,
      "grad_norm": 0.05975133180618286,
      "learning_rate": 0.00024506092666909884,
      "loss": 3.8141,
      "step": 12550
    },
    {
      "epoch": 0.919422806793513,
      "grad_norm": 0.02861228585243225,
      "learning_rate": 0.0002448420284567676,
      "loss": 3.7278,
      "step": 12600
    },
    {
      "epoch": 0.9230713099950745,
      "grad_norm": 0.03803494572639465,
      "learning_rate": 0.0002446231302444363,
      "loss": 3.7608,
      "step": 12650
    },
    {
      "epoch": 0.9267198131966361,
      "grad_norm": 0.040464699268341064,
      "learning_rate": 0.00024440423203210505,
      "loss": 3.7474,
      "step": 12700
    },
    {
      "epoch": 0.9303683163981976,
      "grad_norm": 0.035990022122859955,
      "learning_rate": 0.0002441853338197738,
      "loss": 3.7693,
      "step": 12750
    },
    {
      "epoch": 0.9340168195997592,
      "grad_norm": 0.12842823565006256,
      "learning_rate": 0.0002439664356074425,
      "loss": 3.7205,
      "step": 12800
    },
    {
      "epoch": 0.9376653228013208,
      "grad_norm": 0.04690799117088318,
      "learning_rate": 0.00024374753739511124,
      "loss": 3.7619,
      "step": 12850
    },
    {
      "epoch": 0.9413138260028823,
      "grad_norm": 0.03609974682331085,
      "learning_rate": 0.00024352863918278,
      "loss": 3.8023,
      "step": 12900
    },
    {
      "epoch": 0.9449623292044439,
      "grad_norm": 0.048785604536533356,
      "learning_rate": 0.00024330974097044872,
      "loss": 3.6927,
      "step": 12950
    },
    {
      "epoch": 0.9486108324060054,
      "grad_norm": 0.033076245337724686,
      "learning_rate": 0.00024309084275811745,
      "loss": 3.7412,
      "step": 13000
    },
    {
      "epoch": 0.952259335607567,
      "grad_norm": 0.028776904568076134,
      "learning_rate": 0.0002428719445457862,
      "loss": 3.7878,
      "step": 13050
    },
    {
      "epoch": 0.9559078388091286,
      "grad_norm": 0.02954328991472721,
      "learning_rate": 0.00024265304633345493,
      "loss": 3.7784,
      "step": 13100
    },
    {
      "epoch": 0.9595563420106901,
      "grad_norm": 0.08677668124437332,
      "learning_rate": 0.00024243414812112366,
      "loss": 3.7609,
      "step": 13150
    },
    {
      "epoch": 0.9632048452122517,
      "grad_norm": 0.03310631215572357,
      "learning_rate": 0.0002422152499087924,
      "loss": 3.7892,
      "step": 13200
    },
    {
      "epoch": 0.9668533484138132,
      "grad_norm": 0.03703955188393593,
      "learning_rate": 0.00024199635169646114,
      "loss": 3.7195,
      "step": 13250
    },
    {
      "epoch": 0.9705018516153748,
      "grad_norm": 0.0360308475792408,
      "learning_rate": 0.00024177745348412984,
      "loss": 3.7718,
      "step": 13300
    },
    {
      "epoch": 0.9741503548169363,
      "grad_norm": 0.041493192315101624,
      "learning_rate": 0.00024155855527179857,
      "loss": 3.6603,
      "step": 13350
    },
    {
      "epoch": 0.9777988580184979,
      "grad_norm": 0.07808513939380646,
      "learning_rate": 0.00024133965705946733,
      "loss": 3.6857,
      "step": 13400
    },
    {
      "epoch": 0.9814473612200595,
      "grad_norm": 0.13988623023033142,
      "learning_rate": 0.00024112075884713606,
      "loss": 3.7568,
      "step": 13450
    },
    {
      "epoch": 0.985095864421621,
      "grad_norm": 0.040506236255168915,
      "learning_rate": 0.00024090186063480479,
      "loss": 3.7456,
      "step": 13500
    },
    {
      "epoch": 0.9887443676231826,
      "grad_norm": 0.04095887765288353,
      "learning_rate": 0.00024068296242247351,
      "loss": 3.7838,
      "step": 13550
    },
    {
      "epoch": 0.9923928708247441,
      "grad_norm": 0.02780856378376484,
      "learning_rate": 0.00024046406421014227,
      "loss": 3.727,
      "step": 13600
    },
    {
      "epoch": 0.9960413740263057,
      "grad_norm": 0.03426589444279671,
      "learning_rate": 0.000240245165997811,
      "loss": 3.7816,
      "step": 13650
    },
    {
      "epoch": 0.9996898772278673,
      "grad_norm": 0.04505148530006409,
      "learning_rate": 0.00024002626778547973,
      "loss": 3.7641,
      "step": 13700
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.6197409629821777,
      "eval_runtime": 3.4942,
      "eval_samples_per_second": 28.619,
      "eval_steps_per_second": 3.72,
      "step": 13705
    },
    {
      "epoch": 1.0032836528814053,
      "grad_norm": 0.04090108722448349,
      "learning_rate": 0.00023980736957314848,
      "loss": 3.7404,
      "step": 13750
    },
    {
      "epoch": 1.006932156082967,
      "grad_norm": 0.04469461739063263,
      "learning_rate": 0.0002395884713608172,
      "loss": 3.7613,
      "step": 13800
    },
    {
      "epoch": 1.0105806592845286,
      "grad_norm": 0.19506309926509857,
      "learning_rate": 0.00023936957314848594,
      "loss": 3.7324,
      "step": 13850
    },
    {
      "epoch": 1.0142291624860902,
      "grad_norm": 0.04022873938083649,
      "learning_rate": 0.00023915067493615464,
      "loss": 3.7193,
      "step": 13900
    },
    {
      "epoch": 1.0178776656876516,
      "grad_norm": 0.04357985779643059,
      "learning_rate": 0.00023893177672382342,
      "loss": 3.6963,
      "step": 13950
    },
    {
      "epoch": 1.0215261688892132,
      "grad_norm": 0.03487132489681244,
      "learning_rate": 0.00023871287851149212,
      "loss": 3.7934,
      "step": 14000
    },
    {
      "epoch": 1.0251746720907748,
      "grad_norm": 0.036217644810676575,
      "learning_rate": 0.00023849398029916085,
      "loss": 3.7241,
      "step": 14050
    },
    {
      "epoch": 1.0288231752923362,
      "grad_norm": 0.4628518521785736,
      "learning_rate": 0.0002382750820868296,
      "loss": 3.8456,
      "step": 14100
    },
    {
      "epoch": 1.0324716784938979,
      "grad_norm": 0.05662817135453224,
      "learning_rate": 0.00023805618387449834,
      "loss": 3.7859,
      "step": 14150
    },
    {
      "epoch": 1.0361201816954595,
      "grad_norm": 0.054693058133125305,
      "learning_rate": 0.00023783728566216706,
      "loss": 3.7488,
      "step": 14200
    },
    {
      "epoch": 1.039768684897021,
      "grad_norm": 0.0636269822716713,
      "learning_rate": 0.0002376183874498358,
      "loss": 3.7195,
      "step": 14250
    },
    {
      "epoch": 1.0434171880985825,
      "grad_norm": 0.16742122173309326,
      "learning_rate": 0.00023739948923750455,
      "loss": 3.7932,
      "step": 14300
    },
    {
      "epoch": 1.0470656913001442,
      "grad_norm": 0.027793755754828453,
      "learning_rate": 0.00023718059102517328,
      "loss": 3.7855,
      "step": 14350
    },
    {
      "epoch": 1.0507141945017058,
      "grad_norm": 0.03833455964922905,
      "learning_rate": 0.000236961692812842,
      "loss": 3.7791,
      "step": 14400
    },
    {
      "epoch": 1.0543626977032672,
      "grad_norm": 0.028694558888673782,
      "learning_rate": 0.00023674279460051076,
      "loss": 3.7536,
      "step": 14450
    },
    {
      "epoch": 1.0580112009048288,
      "grad_norm": 0.05112793669104576,
      "learning_rate": 0.0002365238963881795,
      "loss": 3.7776,
      "step": 14500
    },
    {
      "epoch": 1.0616597041063904,
      "grad_norm": 0.03966891020536423,
      "learning_rate": 0.00023630499817584822,
      "loss": 3.7033,
      "step": 14550
    },
    {
      "epoch": 1.0653082073079518,
      "grad_norm": 0.034847281873226166,
      "learning_rate": 0.00023608609996351692,
      "loss": 3.7573,
      "step": 14600
    },
    {
      "epoch": 1.0689567105095135,
      "grad_norm": 0.0321110337972641,
      "learning_rate": 0.00023586720175118567,
      "loss": 3.6903,
      "step": 14650
    },
    {
      "epoch": 1.072605213711075,
      "grad_norm": 0.03850401192903519,
      "learning_rate": 0.0002356483035388544,
      "loss": 3.7537,
      "step": 14700
    },
    {
      "epoch": 1.0762537169126365,
      "grad_norm": 0.05369709059596062,
      "learning_rate": 0.00023542940532652313,
      "loss": 3.7825,
      "step": 14750
    },
    {
      "epoch": 1.0799022201141981,
      "grad_norm": 0.18880654871463776,
      "learning_rate": 0.00023521050711419189,
      "loss": 3.6863,
      "step": 14800
    },
    {
      "epoch": 1.0835507233157597,
      "grad_norm": 0.04152613505721092,
      "learning_rate": 0.00023499160890186061,
      "loss": 3.7209,
      "step": 14850
    },
    {
      "epoch": 1.0871992265173214,
      "grad_norm": 0.03712688386440277,
      "learning_rate": 0.00023477271068952934,
      "loss": 3.7094,
      "step": 14900
    },
    {
      "epoch": 1.0908477297188828,
      "grad_norm": 0.05150223150849342,
      "learning_rate": 0.00023455381247719807,
      "loss": 3.7697,
      "step": 14950
    },
    {
      "epoch": 1.0944962329204444,
      "grad_norm": 0.05034307762980461,
      "learning_rate": 0.00023433491426486683,
      "loss": 3.7455,
      "step": 15000
    },
    {
      "epoch": 1.098144736122006,
      "grad_norm": 0.048461925238370895,
      "learning_rate": 0.00023411601605253555,
      "loss": 3.7075,
      "step": 15050
    },
    {
      "epoch": 1.1017932393235674,
      "grad_norm": 0.12226790934801102,
      "learning_rate": 0.00023389711784020428,
      "loss": 3.7211,
      "step": 15100
    },
    {
      "epoch": 1.105441742525129,
      "grad_norm": 0.050078921020030975,
      "learning_rate": 0.00023367821962787304,
      "loss": 3.7438,
      "step": 15150
    },
    {
      "epoch": 1.1090902457266907,
      "grad_norm": 0.03300346061587334,
      "learning_rate": 0.00023345932141554177,
      "loss": 3.7432,
      "step": 15200
    },
    {
      "epoch": 1.112738748928252,
      "grad_norm": 0.0669669657945633,
      "learning_rate": 0.00023324042320321047,
      "loss": 3.7875,
      "step": 15250
    },
    {
      "epoch": 1.1163872521298137,
      "grad_norm": 0.037628013640642166,
      "learning_rate": 0.0002330215249908792,
      "loss": 3.7482,
      "step": 15300
    },
    {
      "epoch": 1.1200357553313753,
      "grad_norm": 0.04143832251429558,
      "learning_rate": 0.00023280262677854795,
      "loss": 3.7686,
      "step": 15350
    },
    {
      "epoch": 1.123684258532937,
      "grad_norm": 0.03838564082980156,
      "learning_rate": 0.00023258372856621668,
      "loss": 3.759,
      "step": 15400
    },
    {
      "epoch": 1.1273327617344984,
      "grad_norm": 1.517522931098938,
      "learning_rate": 0.0002323648303538854,
      "loss": 3.7963,
      "step": 15450
    },
    {
      "epoch": 1.13098126493606,
      "grad_norm": 0.060505349189043045,
      "learning_rate": 0.00023214593214155416,
      "loss": 3.7151,
      "step": 15500
    },
    {
      "epoch": 1.1346297681376216,
      "grad_norm": 0.04080701619386673,
      "learning_rate": 0.0002319270339292229,
      "loss": 3.7585,
      "step": 15550
    },
    {
      "epoch": 1.138278271339183,
      "grad_norm": 0.05273197218775749,
      "learning_rate": 0.00023170813571689162,
      "loss": 3.8283,
      "step": 15600
    },
    {
      "epoch": 1.1419267745407446,
      "grad_norm": 0.04702245444059372,
      "learning_rate": 0.00023148923750456035,
      "loss": 3.7583,
      "step": 15650
    },
    {
      "epoch": 1.1455752777423063,
      "grad_norm": 0.035292260348796844,
      "learning_rate": 0.0002312703392922291,
      "loss": 3.7589,
      "step": 15700
    },
    {
      "epoch": 1.149223780943868,
      "grad_norm": 0.15973253548145294,
      "learning_rate": 0.00023105144107989783,
      "loss": 3.7187,
      "step": 15750
    },
    {
      "epoch": 1.1528722841454293,
      "grad_norm": 0.08204863965511322,
      "learning_rate": 0.00023083254286756656,
      "loss": 3.7326,
      "step": 15800
    },
    {
      "epoch": 1.156520787346991,
      "grad_norm": 0.048146408051252365,
      "learning_rate": 0.00023061364465523532,
      "loss": 3.7682,
      "step": 15850
    },
    {
      "epoch": 1.1601692905485526,
      "grad_norm": 0.04632381349802017,
      "learning_rate": 0.00023039474644290405,
      "loss": 3.763,
      "step": 15900
    },
    {
      "epoch": 1.163817793750114,
      "grad_norm": 0.04307112470269203,
      "learning_rate": 0.00023017584823057275,
      "loss": 3.7247,
      "step": 15950
    },
    {
      "epoch": 1.1674662969516756,
      "grad_norm": 0.04215457662940025,
      "learning_rate": 0.00022995695001824148,
      "loss": 3.7508,
      "step": 16000
    },
    {
      "epoch": 1.1711148001532372,
      "grad_norm": 0.08878426998853683,
      "learning_rate": 0.00022973805180591023,
      "loss": 3.7979,
      "step": 16050
    },
    {
      "epoch": 1.1747633033547986,
      "grad_norm": 0.033042531460523605,
      "learning_rate": 0.00022951915359357896,
      "loss": 3.7261,
      "step": 16100
    },
    {
      "epoch": 1.1784118065563602,
      "grad_norm": 0.043988894671201706,
      "learning_rate": 0.0002293002553812477,
      "loss": 3.7267,
      "step": 16150
    },
    {
      "epoch": 1.1820603097579219,
      "grad_norm": 0.03956916928291321,
      "learning_rate": 0.00022908135716891644,
      "loss": 3.7577,
      "step": 16200
    },
    {
      "epoch": 1.1857088129594833,
      "grad_norm": 0.05795186385512352,
      "learning_rate": 0.00022886245895658517,
      "loss": 3.7828,
      "step": 16250
    },
    {
      "epoch": 1.189357316161045,
      "grad_norm": 0.04314567148685455,
      "learning_rate": 0.0002286435607442539,
      "loss": 3.7809,
      "step": 16300
    },
    {
      "epoch": 1.1930058193626065,
      "grad_norm": 0.04510662332177162,
      "learning_rate": 0.00022842466253192263,
      "loss": 3.74,
      "step": 16350
    },
    {
      "epoch": 1.196654322564168,
      "grad_norm": 0.03833683952689171,
      "learning_rate": 0.00022820576431959138,
      "loss": 3.7809,
      "step": 16400
    },
    {
      "epoch": 1.2003028257657296,
      "grad_norm": 0.21374206244945526,
      "learning_rate": 0.0002279868661072601,
      "loss": 3.7461,
      "step": 16450
    },
    {
      "epoch": 1.2039513289672912,
      "grad_norm": 0.03803669661283493,
      "learning_rate": 0.00022776796789492884,
      "loss": 3.7314,
      "step": 16500
    },
    {
      "epoch": 1.2075998321688528,
      "grad_norm": 0.041917238384485245,
      "learning_rate": 0.0002275490696825976,
      "loss": 3.6744,
      "step": 16550
    },
    {
      "epoch": 1.2112483353704142,
      "grad_norm": 0.03598570451140404,
      "learning_rate": 0.0002273301714702663,
      "loss": 3.7176,
      "step": 16600
    },
    {
      "epoch": 1.2148968385719758,
      "grad_norm": 0.03387374430894852,
      "learning_rate": 0.00022711127325793503,
      "loss": 3.7066,
      "step": 16650
    },
    {
      "epoch": 1.2185453417735375,
      "grad_norm": 0.08991953730583191,
      "learning_rate": 0.00022689237504560375,
      "loss": 3.6998,
      "step": 16700
    },
    {
      "epoch": 1.222193844975099,
      "grad_norm": 0.026019835844635963,
      "learning_rate": 0.0002266734768332725,
      "loss": 3.7563,
      "step": 16750
    },
    {
      "epoch": 1.2258423481766605,
      "grad_norm": 0.03544925898313522,
      "learning_rate": 0.00022645457862094124,
      "loss": 3.7542,
      "step": 16800
    },
    {
      "epoch": 1.2294908513782221,
      "grad_norm": 0.047079335898160934,
      "learning_rate": 0.00022623568040860997,
      "loss": 3.7759,
      "step": 16850
    },
    {
      "epoch": 1.2331393545797837,
      "grad_norm": 0.03889049217104912,
      "learning_rate": 0.00022601678219627872,
      "loss": 3.719,
      "step": 16900
    },
    {
      "epoch": 1.2367878577813451,
      "grad_norm": 0.06887563318014145,
      "learning_rate": 0.00022579788398394745,
      "loss": 3.8103,
      "step": 16950
    },
    {
      "epoch": 1.2404363609829068,
      "grad_norm": 0.04053463041782379,
      "learning_rate": 0.00022557898577161618,
      "loss": 3.7593,
      "step": 17000
    },
    {
      "epoch": 1.2440848641844684,
      "grad_norm": 0.051020774990320206,
      "learning_rate": 0.0002253600875592849,
      "loss": 3.7464,
      "step": 17050
    },
    {
      "epoch": 1.2477333673860298,
      "grad_norm": 0.03496795520186424,
      "learning_rate": 0.00022514118934695366,
      "loss": 3.6955,
      "step": 17100
    },
    {
      "epoch": 1.2513818705875914,
      "grad_norm": 0.10475992411375046,
      "learning_rate": 0.0002249222911346224,
      "loss": 3.7083,
      "step": 17150
    },
    {
      "epoch": 1.255030373789153,
      "grad_norm": 0.03078918531537056,
      "learning_rate": 0.0002247033929222911,
      "loss": 3.7339,
      "step": 17200
    },
    {
      "epoch": 1.2586788769907145,
      "grad_norm": 0.03111954964697361,
      "learning_rate": 0.00022448449470995987,
      "loss": 3.7282,
      "step": 17250
    },
    {
      "epoch": 1.262327380192276,
      "grad_norm": 0.034766826778650284,
      "learning_rate": 0.00022426559649762858,
      "loss": 3.724,
      "step": 17300
    },
    {
      "epoch": 1.2659758833938377,
      "grad_norm": 0.06815806031227112,
      "learning_rate": 0.0002240466982852973,
      "loss": 3.7303,
      "step": 17350
    },
    {
      "epoch": 1.2696243865953991,
      "grad_norm": 0.049190081655979156,
      "learning_rate": 0.00022382780007296603,
      "loss": 3.7126,
      "step": 17400
    },
    {
      "epoch": 1.2732728897969607,
      "grad_norm": 0.04145616292953491,
      "learning_rate": 0.0002236089018606348,
      "loss": 3.763,
      "step": 17450
    },
    {
      "epoch": 1.2769213929985224,
      "grad_norm": 0.05226116627454758,
      "learning_rate": 0.00022339000364830352,
      "loss": 3.7352,
      "step": 17500
    },
    {
      "epoch": 1.280569896200084,
      "grad_norm": 0.043208252638578415,
      "learning_rate": 0.00022317110543597224,
      "loss": 3.695,
      "step": 17550
    },
    {
      "epoch": 1.2842183994016454,
      "grad_norm": 0.04710938781499863,
      "learning_rate": 0.000222952207223641,
      "loss": 3.7899,
      "step": 17600
    },
    {
      "epoch": 1.287866902603207,
      "grad_norm": 0.04672008380293846,
      "learning_rate": 0.00022273330901130973,
      "loss": 3.7437,
      "step": 17650
    },
    {
      "epoch": 1.2915154058047686,
      "grad_norm": 0.06530438363552094,
      "learning_rate": 0.00022251441079897846,
      "loss": 3.7217,
      "step": 17700
    },
    {
      "epoch": 1.2951639090063303,
      "grad_norm": 0.0764741525053978,
      "learning_rate": 0.00022229551258664719,
      "loss": 3.6842,
      "step": 17750
    },
    {
      "epoch": 1.2988124122078917,
      "grad_norm": 0.038985252380371094,
      "learning_rate": 0.00022207661437431594,
      "loss": 3.7193,
      "step": 17800
    },
    {
      "epoch": 1.3024609154094533,
      "grad_norm": 0.05751195177435875,
      "learning_rate": 0.00022185771616198467,
      "loss": 3.6906,
      "step": 17850
    },
    {
      "epoch": 1.306109418611015,
      "grad_norm": 0.059702035039663315,
      "learning_rate": 0.00022163881794965337,
      "loss": 3.748,
      "step": 17900
    },
    {
      "epoch": 1.3097579218125763,
      "grad_norm": 0.03381015360355377,
      "learning_rate": 0.00022141991973732213,
      "loss": 3.7611,
      "step": 17950
    },
    {
      "epoch": 1.313406425014138,
      "grad_norm": 0.04454955458641052,
      "learning_rate": 0.00022120102152499085,
      "loss": 3.7557,
      "step": 18000
    },
    {
      "epoch": 1.3170549282156996,
      "grad_norm": 0.03766092658042908,
      "learning_rate": 0.00022098212331265958,
      "loss": 3.6921,
      "step": 18050
    },
    {
      "epoch": 1.320703431417261,
      "grad_norm": 0.030874619260430336,
      "learning_rate": 0.0002207632251003283,
      "loss": 3.7928,
      "step": 18100
    },
    {
      "epoch": 1.3243519346188226,
      "grad_norm": 0.034130651503801346,
      "learning_rate": 0.00022054432688799707,
      "loss": 3.722,
      "step": 18150
    },
    {
      "epoch": 1.3280004378203842,
      "grad_norm": 0.056567978113889694,
      "learning_rate": 0.0002203254286756658,
      "loss": 3.6631,
      "step": 18200
    },
    {
      "epoch": 1.3316489410219456,
      "grad_norm": 0.06647706776857376,
      "learning_rate": 0.00022010653046333452,
      "loss": 3.702,
      "step": 18250
    },
    {
      "epoch": 1.3352974442235073,
      "grad_norm": 0.065660260617733,
      "learning_rate": 0.00021988763225100328,
      "loss": 3.752,
      "step": 18300
    },
    {
      "epoch": 1.338945947425069,
      "grad_norm": 0.029660243541002274,
      "learning_rate": 0.000219668734038672,
      "loss": 3.662,
      "step": 18350
    },
    {
      "epoch": 1.3425944506266303,
      "grad_norm": 0.06708890944719315,
      "learning_rate": 0.00021944983582634074,
      "loss": 3.7586,
      "step": 18400
    },
    {
      "epoch": 1.346242953828192,
      "grad_norm": 0.05066681653261185,
      "learning_rate": 0.00021923093761400944,
      "loss": 3.7515,
      "step": 18450
    },
    {
      "epoch": 1.3498914570297535,
      "grad_norm": 0.07868752628564835,
      "learning_rate": 0.00021901203940167822,
      "loss": 3.7385,
      "step": 18500
    },
    {
      "epoch": 1.3535399602313152,
      "grad_norm": 0.1273961216211319,
      "learning_rate": 0.00021879314118934692,
      "loss": 3.713,
      "step": 18550
    },
    {
      "epoch": 1.3571884634328766,
      "grad_norm": 0.172705739736557,
      "learning_rate": 0.00021857424297701565,
      "loss": 3.7561,
      "step": 18600
    },
    {
      "epoch": 1.3608369666344382,
      "grad_norm": 0.10469924658536911,
      "learning_rate": 0.0002183553447646844,
      "loss": 3.7456,
      "step": 18650
    },
    {
      "epoch": 1.3644854698359998,
      "grad_norm": 0.3939546048641205,
      "learning_rate": 0.00021813644655235313,
      "loss": 3.7991,
      "step": 18700
    },
    {
      "epoch": 1.3681339730375615,
      "grad_norm": 0.21344639360904694,
      "learning_rate": 0.00021791754834002186,
      "loss": 3.7533,
      "step": 18750
    },
    {
      "epoch": 1.3717824762391229,
      "grad_norm": 0.03942576050758362,
      "learning_rate": 0.00021769865012769062,
      "loss": 3.7735,
      "step": 18800
    },
    {
      "epoch": 1.3754309794406845,
      "grad_norm": 0.1853078156709671,
      "learning_rate": 0.00021747975191535934,
      "loss": 3.7736,
      "step": 18850
    },
    {
      "epoch": 1.3790794826422461,
      "grad_norm": 0.04366566613316536,
      "learning_rate": 0.00021726085370302807,
      "loss": 3.772,
      "step": 18900
    },
    {
      "epoch": 1.3827279858438075,
      "grad_norm": 0.03695155307650566,
      "learning_rate": 0.0002170419554906968,
      "loss": 3.7625,
      "step": 18950
    },
    {
      "epoch": 1.3863764890453691,
      "grad_norm": 0.0532066747546196,
      "learning_rate": 0.00021682305727836556,
      "loss": 3.7601,
      "step": 19000
    },
    {
      "epoch": 1.3900249922469308,
      "grad_norm": 0.042456869035959244,
      "learning_rate": 0.00021660415906603429,
      "loss": 3.7164,
      "step": 19050
    },
    {
      "epoch": 1.3936734954484922,
      "grad_norm": 0.051930904388427734,
      "learning_rate": 0.00021638526085370301,
      "loss": 3.7482,
      "step": 19100
    },
    {
      "epoch": 1.3973219986500538,
      "grad_norm": 0.09603128582239151,
      "learning_rate": 0.00021616636264137177,
      "loss": 3.7329,
      "step": 19150
    },
    {
      "epoch": 1.4009705018516154,
      "grad_norm": 0.028102722018957138,
      "learning_rate": 0.0002159474644290405,
      "loss": 3.6888,
      "step": 19200
    },
    {
      "epoch": 1.4046190050531768,
      "grad_norm": 0.037440817803144455,
      "learning_rate": 0.0002157285662167092,
      "loss": 3.728,
      "step": 19250
    },
    {
      "epoch": 1.4082675082547385,
      "grad_norm": 0.060961104929447174,
      "learning_rate": 0.00021550966800437793,
      "loss": 3.7263,
      "step": 19300
    },
    {
      "epoch": 1.4119160114563,
      "grad_norm": 0.1901373714208603,
      "learning_rate": 0.00021529076979204668,
      "loss": 3.7841,
      "step": 19350
    },
    {
      "epoch": 1.4155645146578615,
      "grad_norm": 0.17188921570777893,
      "learning_rate": 0.0002150718715797154,
      "loss": 3.7217,
      "step": 19400
    },
    {
      "epoch": 1.419213017859423,
      "grad_norm": 0.06910090148448944,
      "learning_rate": 0.00021485297336738414,
      "loss": 3.7502,
      "step": 19450
    },
    {
      "epoch": 1.4228615210609847,
      "grad_norm": 0.3995209038257599,
      "learning_rate": 0.0002146340751550529,
      "loss": 3.737,
      "step": 19500
    },
    {
      "epoch": 1.4265100242625464,
      "grad_norm": 0.048238687217235565,
      "learning_rate": 0.00021441517694272162,
      "loss": 3.6937,
      "step": 19550
    },
    {
      "epoch": 1.430158527464108,
      "grad_norm": 0.11558366566896439,
      "learning_rate": 0.00021419627873039035,
      "loss": 3.7424,
      "step": 19600
    },
    {
      "epoch": 1.4338070306656694,
      "grad_norm": 0.03168173134326935,
      "learning_rate": 0.00021397738051805908,
      "loss": 3.6756,
      "step": 19650
    },
    {
      "epoch": 1.437455533867231,
      "grad_norm": 0.07648158073425293,
      "learning_rate": 0.00021375848230572784,
      "loss": 3.7567,
      "step": 19700
    },
    {
      "epoch": 1.4411040370687926,
      "grad_norm": 0.47223401069641113,
      "learning_rate": 0.00021353958409339656,
      "loss": 3.7227,
      "step": 19750
    },
    {
      "epoch": 1.444752540270354,
      "grad_norm": 0.044387441128492355,
      "learning_rate": 0.00021332068588106527,
      "loss": 3.7422,
      "step": 19800
    },
    {
      "epoch": 1.4484010434719157,
      "grad_norm": 0.0492052361369133,
      "learning_rate": 0.00021310178766873405,
      "loss": 3.7378,
      "step": 19850
    },
    {
      "epoch": 1.4520495466734773,
      "grad_norm": 0.029866209253668785,
      "learning_rate": 0.00021288288945640275,
      "loss": 3.7438,
      "step": 19900
    },
    {
      "epoch": 1.4556980498750387,
      "grad_norm": 0.09800336509943008,
      "learning_rate": 0.00021266399124407148,
      "loss": 3.7424,
      "step": 19950
    },
    {
      "epoch": 1.4593465530766003,
      "grad_norm": 0.041090868413448334,
      "learning_rate": 0.0002124450930317402,
      "loss": 3.6977,
      "step": 20000
    },
    {
      "epoch": 1.462995056278162,
      "grad_norm": 0.0812729001045227,
      "learning_rate": 0.00021222619481940896,
      "loss": 3.7003,
      "step": 20050
    },
    {
      "epoch": 1.4666435594797234,
      "grad_norm": 0.0362551286816597,
      "learning_rate": 0.0002120072966070777,
      "loss": 3.695,
      "step": 20100
    },
    {
      "epoch": 1.470292062681285,
      "grad_norm": 0.045014895498752594,
      "learning_rate": 0.00021178839839474642,
      "loss": 3.7217,
      "step": 20150
    },
    {
      "epoch": 1.4739405658828466,
      "grad_norm": 0.03407137095928192,
      "learning_rate": 0.00021156950018241517,
      "loss": 3.6941,
      "step": 20200
    },
    {
      "epoch": 1.477589069084408,
      "grad_norm": 0.044785305857658386,
      "learning_rate": 0.0002113506019700839,
      "loss": 3.7355,
      "step": 20250
    },
    {
      "epoch": 1.4812375722859696,
      "grad_norm": 0.04852747544646263,
      "learning_rate": 0.00021113170375775263,
      "loss": 3.7285,
      "step": 20300
    },
    {
      "epoch": 1.4848860754875313,
      "grad_norm": 0.04478832334280014,
      "learning_rate": 0.00021091280554542136,
      "loss": 3.7396,
      "step": 20350
    },
    {
      "epoch": 1.4885345786890927,
      "grad_norm": 0.05452612414956093,
      "learning_rate": 0.00021069390733309011,
      "loss": 3.6699,
      "step": 20400
    },
    {
      "epoch": 1.4921830818906543,
      "grad_norm": 0.34000536799430847,
      "learning_rate": 0.00021047500912075884,
      "loss": 3.7358,
      "step": 20450
    },
    {
      "epoch": 1.495831585092216,
      "grad_norm": 0.028732100501656532,
      "learning_rate": 0.00021025611090842754,
      "loss": 3.7201,
      "step": 20500
    },
    {
      "epoch": 1.4994800882937775,
      "grad_norm": 0.03826945647597313,
      "learning_rate": 0.00021003721269609633,
      "loss": 3.7455,
      "step": 20550
    },
    {
      "epoch": 1.5031285914953392,
      "grad_norm": 0.09516750276088715,
      "learning_rate": 0.00020981831448376503,
      "loss": 3.7119,
      "step": 20600
    },
    {
      "epoch": 1.5067770946969006,
      "grad_norm": 0.03188200294971466,
      "learning_rate": 0.00020959941627143376,
      "loss": 3.6845,
      "step": 20650
    },
    {
      "epoch": 1.5104255978984622,
      "grad_norm": 0.3965957462787628,
      "learning_rate": 0.00020938051805910248,
      "loss": 3.7461,
      "step": 20700
    },
    {
      "epoch": 1.5140741011000238,
      "grad_norm": 0.03341841697692871,
      "learning_rate": 0.00020916161984677124,
      "loss": 3.7246,
      "step": 20750
    },
    {
      "epoch": 1.5177226043015852,
      "grad_norm": 0.03954887017607689,
      "learning_rate": 0.00020894272163443997,
      "loss": 3.7148,
      "step": 20800
    },
    {
      "epoch": 1.5213711075031469,
      "grad_norm": 0.04117938503623009,
      "learning_rate": 0.0002087238234221087,
      "loss": 3.694,
      "step": 20850
    },
    {
      "epoch": 1.5250196107047085,
      "grad_norm": 0.031011167913675308,
      "learning_rate": 0.00020850492520977745,
      "loss": 3.7734,
      "step": 20900
    },
    {
      "epoch": 1.5286681139062699,
      "grad_norm": 0.03980780765414238,
      "learning_rate": 0.00020828602699744618,
      "loss": 3.6828,
      "step": 20950
    },
    {
      "epoch": 1.5323166171078315,
      "grad_norm": 0.08052249252796173,
      "learning_rate": 0.0002080671287851149,
      "loss": 3.7032,
      "step": 21000
    },
    {
      "epoch": 1.5359651203093931,
      "grad_norm": 0.04264044761657715,
      "learning_rate": 0.00020784823057278364,
      "loss": 3.7709,
      "step": 21050
    },
    {
      "epoch": 1.5396136235109545,
      "grad_norm": 0.07172401994466782,
      "learning_rate": 0.0002076293323604524,
      "loss": 3.7435,
      "step": 21100
    },
    {
      "epoch": 1.5432621267125162,
      "grad_norm": 0.05430204048752785,
      "learning_rate": 0.00020741043414812112,
      "loss": 3.7231,
      "step": 21150
    },
    {
      "epoch": 1.5469106299140778,
      "grad_norm": 0.025894206017255783,
      "learning_rate": 0.00020719153593578982,
      "loss": 3.7655,
      "step": 21200
    },
    {
      "epoch": 1.5505591331156392,
      "grad_norm": 0.05294632166624069,
      "learning_rate": 0.00020697263772345858,
      "loss": 3.7312,
      "step": 21250
    },
    {
      "epoch": 1.5542076363172008,
      "grad_norm": 0.0543757788836956,
      "learning_rate": 0.0002067537395111273,
      "loss": 3.7751,
      "step": 21300
    },
    {
      "epoch": 1.5578561395187625,
      "grad_norm": 0.19300492107868195,
      "learning_rate": 0.00020653484129879603,
      "loss": 3.6977,
      "step": 21350
    },
    {
      "epoch": 1.5615046427203239,
      "grad_norm": 0.02627233788371086,
      "learning_rate": 0.00020631594308646476,
      "loss": 3.7459,
      "step": 21400
    },
    {
      "epoch": 1.5651531459218857,
      "grad_norm": 0.038918063044548035,
      "learning_rate": 0.00020609704487413352,
      "loss": 3.6792,
      "step": 21450
    },
    {
      "epoch": 1.568801649123447,
      "grad_norm": 0.3862779140472412,
      "learning_rate": 0.00020587814666180225,
      "loss": 3.7494,
      "step": 21500
    },
    {
      "epoch": 1.5724501523250085,
      "grad_norm": 0.042077939957380295,
      "learning_rate": 0.00020565924844947097,
      "loss": 3.7232,
      "step": 21550
    },
    {
      "epoch": 1.5760986555265704,
      "grad_norm": 0.03379373624920845,
      "learning_rate": 0.00020544035023713973,
      "loss": 3.751,
      "step": 21600
    },
    {
      "epoch": 1.5797471587281318,
      "grad_norm": 0.09585865586996078,
      "learning_rate": 0.00020522145202480846,
      "loss": 3.7543,
      "step": 21650
    },
    {
      "epoch": 1.5833956619296934,
      "grad_norm": 0.03631490096449852,
      "learning_rate": 0.0002050025538124772,
      "loss": 3.749,
      "step": 21700
    },
    {
      "epoch": 1.587044165131255,
      "grad_norm": 0.04379186034202576,
      "learning_rate": 0.0002047836556001459,
      "loss": 3.6549,
      "step": 21750
    },
    {
      "epoch": 1.5906926683328164,
      "grad_norm": 0.03800224885344505,
      "learning_rate": 0.00020456475738781467,
      "loss": 3.7285,
      "step": 21800
    },
    {
      "epoch": 1.594341171534378,
      "grad_norm": 0.03520674258470535,
      "learning_rate": 0.00020434585917548337,
      "loss": 3.6773,
      "step": 21850
    },
    {
      "epoch": 1.5979896747359397,
      "grad_norm": 0.03876625746488571,
      "learning_rate": 0.0002041269609631521,
      "loss": 3.799,
      "step": 21900
    },
    {
      "epoch": 1.601638177937501,
      "grad_norm": 0.04502211883664131,
      "learning_rate": 0.00020390806275082086,
      "loss": 3.7079,
      "step": 21950
    },
    {
      "epoch": 1.6052866811390627,
      "grad_norm": 0.04586934670805931,
      "learning_rate": 0.00020368916453848958,
      "loss": 3.7554,
      "step": 22000
    },
    {
      "epoch": 1.6089351843406243,
      "grad_norm": 0.04659333825111389,
      "learning_rate": 0.0002034702663261583,
      "loss": 3.7483,
      "step": 22050
    },
    {
      "epoch": 1.6125836875421857,
      "grad_norm": 0.07218611240386963,
      "learning_rate": 0.00020325136811382704,
      "loss": 3.737,
      "step": 22100
    },
    {
      "epoch": 1.6162321907437474,
      "grad_norm": 0.04065568372607231,
      "learning_rate": 0.0002030324699014958,
      "loss": 3.6511,
      "step": 22150
    },
    {
      "epoch": 1.619880693945309,
      "grad_norm": 0.05431501567363739,
      "learning_rate": 0.00020281357168916453,
      "loss": 3.716,
      "step": 22200
    },
    {
      "epoch": 1.6235291971468704,
      "grad_norm": 0.036163125187158585,
      "learning_rate": 0.00020259467347683325,
      "loss": 3.7629,
      "step": 22250
    },
    {
      "epoch": 1.627177700348432,
      "grad_norm": 0.0995253175497055,
      "learning_rate": 0.000202375775264502,
      "loss": 3.7668,
      "step": 22300
    },
    {
      "epoch": 1.6308262035499936,
      "grad_norm": 0.024355482310056686,
      "learning_rate": 0.00020215687705217074,
      "loss": 3.7315,
      "step": 22350
    },
    {
      "epoch": 1.634474706751555,
      "grad_norm": 0.06413973122835159,
      "learning_rate": 0.00020193797883983947,
      "loss": 3.7258,
      "step": 22400
    },
    {
      "epoch": 1.6381232099531169,
      "grad_norm": 0.055735472589731216,
      "learning_rate": 0.00020171908062750817,
      "loss": 3.6792,
      "step": 22450
    },
    {
      "epoch": 1.6417717131546783,
      "grad_norm": 0.10247907787561417,
      "learning_rate": 0.00020150018241517695,
      "loss": 3.7569,
      "step": 22500
    },
    {
      "epoch": 1.6454202163562397,
      "grad_norm": 0.03374054655432701,
      "learning_rate": 0.00020128128420284565,
      "loss": 3.6881,
      "step": 22550
    },
    {
      "epoch": 1.6490687195578015,
      "grad_norm": 0.03926060348749161,
      "learning_rate": 0.00020106238599051438,
      "loss": 3.6941,
      "step": 22600
    },
    {
      "epoch": 1.652717222759363,
      "grad_norm": 0.03623991459608078,
      "learning_rate": 0.00020084348777818313,
      "loss": 3.7633,
      "step": 22650
    },
    {
      "epoch": 1.6563657259609246,
      "grad_norm": 0.041897013783454895,
      "learning_rate": 0.00020062458956585186,
      "loss": 3.7022,
      "step": 22700
    },
    {
      "epoch": 1.6600142291624862,
      "grad_norm": 0.03213215619325638,
      "learning_rate": 0.0002004056913535206,
      "loss": 3.6811,
      "step": 22750
    },
    {
      "epoch": 1.6636627323640476,
      "grad_norm": 0.03069021739065647,
      "learning_rate": 0.00020018679314118932,
      "loss": 3.7115,
      "step": 22800
    },
    {
      "epoch": 1.6673112355656092,
      "grad_norm": 0.028623683378100395,
      "learning_rate": 0.00019996789492885808,
      "loss": 3.7639,
      "step": 22850
    },
    {
      "epoch": 1.6709597387671709,
      "grad_norm": 0.03154635801911354,
      "learning_rate": 0.0001997489967165268,
      "loss": 3.7333,
      "step": 22900
    },
    {
      "epoch": 1.6746082419687323,
      "grad_norm": 0.04658370465040207,
      "learning_rate": 0.00019953009850419553,
      "loss": 3.6698,
      "step": 22950
    },
    {
      "epoch": 1.6782567451702939,
      "grad_norm": 0.0418594628572464,
      "learning_rate": 0.0001993112002918643,
      "loss": 3.7934,
      "step": 23000
    },
    {
      "epoch": 1.6819052483718555,
      "grad_norm": 0.03730812296271324,
      "learning_rate": 0.00019909230207953302,
      "loss": 3.6988,
      "step": 23050
    },
    {
      "epoch": 1.685553751573417,
      "grad_norm": 0.3426954746246338,
      "learning_rate": 0.00019887340386720172,
      "loss": 3.7112,
      "step": 23100
    },
    {
      "epoch": 1.6892022547749785,
      "grad_norm": 0.03945504128932953,
      "learning_rate": 0.00019865450565487045,
      "loss": 3.6631,
      "step": 23150
    },
    {
      "epoch": 1.6928507579765402,
      "grad_norm": 0.0314551517367363,
      "learning_rate": 0.0001984356074425392,
      "loss": 3.7211,
      "step": 23200
    },
    {
      "epoch": 1.6964992611781016,
      "grad_norm": 0.02280016429722309,
      "learning_rate": 0.00019821670923020793,
      "loss": 3.6601,
      "step": 23250
    },
    {
      "epoch": 1.7001477643796632,
      "grad_norm": 0.5990465879440308,
      "learning_rate": 0.00019799781101787666,
      "loss": 3.7243,
      "step": 23300
    },
    {
      "epoch": 1.7037962675812248,
      "grad_norm": 0.05704393610358238,
      "learning_rate": 0.0001977789128055454,
      "loss": 3.7162,
      "step": 23350
    },
    {
      "epoch": 1.7074447707827862,
      "grad_norm": 0.03614777699112892,
      "learning_rate": 0.00019756001459321414,
      "loss": 3.7398,
      "step": 23400
    },
    {
      "epoch": 1.711093273984348,
      "grad_norm": 0.14724422991275787,
      "learning_rate": 0.00019734111638088287,
      "loss": 3.8188,
      "step": 23450
    },
    {
      "epoch": 1.7147417771859095,
      "grad_norm": 0.04389943927526474,
      "learning_rate": 0.0001971222181685516,
      "loss": 3.7058,
      "step": 23500
    },
    {
      "epoch": 1.7183902803874709,
      "grad_norm": 0.03524565324187279,
      "learning_rate": 0.00019690331995622035,
      "loss": 3.7449,
      "step": 23550
    },
    {
      "epoch": 1.7220387835890327,
      "grad_norm": 0.03427056223154068,
      "learning_rate": 0.00019668442174388908,
      "loss": 3.7831,
      "step": 23600
    },
    {
      "epoch": 1.7256872867905941,
      "grad_norm": 0.0431627482175827,
      "learning_rate": 0.0001964655235315578,
      "loss": 3.6639,
      "step": 23650
    },
    {
      "epoch": 1.7293357899921558,
      "grad_norm": 0.06245267763733864,
      "learning_rate": 0.00019624662531922657,
      "loss": 3.763,
      "step": 23700
    },
    {
      "epoch": 1.7329842931937174,
      "grad_norm": 0.053598690778017044,
      "learning_rate": 0.0001960277271068953,
      "loss": 3.7407,
      "step": 23750
    },
    {
      "epoch": 1.7366327963952788,
      "grad_norm": 0.0344044454395771,
      "learning_rate": 0.000195808828894564,
      "loss": 3.7296,
      "step": 23800
    },
    {
      "epoch": 1.7402812995968404,
      "grad_norm": 0.047508012503385544,
      "learning_rate": 0.00019558993068223272,
      "loss": 3.71,
      "step": 23850
    },
    {
      "epoch": 1.743929802798402,
      "grad_norm": 0.038889892399311066,
      "learning_rate": 0.00019537103246990148,
      "loss": 3.7082,
      "step": 23900
    },
    {
      "epoch": 1.7475783059999634,
      "grad_norm": 0.039165135473012924,
      "learning_rate": 0.0001951521342575702,
      "loss": 3.7192,
      "step": 23950
    },
    {
      "epoch": 1.751226809201525,
      "grad_norm": 0.02955910749733448,
      "learning_rate": 0.00019493323604523894,
      "loss": 3.7085,
      "step": 24000
    },
    {
      "epoch": 1.7548753124030867,
      "grad_norm": 0.1282394677400589,
      "learning_rate": 0.0001947143378329077,
      "loss": 3.7096,
      "step": 24050
    },
    {
      "epoch": 1.758523815604648,
      "grad_norm": 0.256636381149292,
      "learning_rate": 0.00019449543962057642,
      "loss": 3.6639,
      "step": 24100
    },
    {
      "epoch": 1.7621723188062097,
      "grad_norm": 0.03235531598329544,
      "learning_rate": 0.00019427654140824515,
      "loss": 3.7336,
      "step": 24150
    },
    {
      "epoch": 1.7658208220077714,
      "grad_norm": 0.02946777455508709,
      "learning_rate": 0.00019405764319591388,
      "loss": 3.6831,
      "step": 24200
    },
    {
      "epoch": 1.7694693252093328,
      "grad_norm": 0.05889840051531792,
      "learning_rate": 0.00019383874498358263,
      "loss": 3.7598,
      "step": 24250
    },
    {
      "epoch": 1.7731178284108944,
      "grad_norm": 0.037389207631349564,
      "learning_rate": 0.00019361984677125136,
      "loss": 3.7918,
      "step": 24300
    },
    {
      "epoch": 1.776766331612456,
      "grad_norm": 0.03324613720178604,
      "learning_rate": 0.0001934009485589201,
      "loss": 3.7412,
      "step": 24350
    },
    {
      "epoch": 1.7804148348140174,
      "grad_norm": 0.04208570346236229,
      "learning_rate": 0.00019318205034658884,
      "loss": 3.813,
      "step": 24400
    },
    {
      "epoch": 1.7840633380155793,
      "grad_norm": 0.05217617005109787,
      "learning_rate": 0.00019296315213425755,
      "loss": 3.7057,
      "step": 24450
    },
    {
      "epoch": 1.7877118412171407,
      "grad_norm": 0.03759100288152695,
      "learning_rate": 0.00019274425392192627,
      "loss": 3.7126,
      "step": 24500
    },
    {
      "epoch": 1.791360344418702,
      "grad_norm": 0.03655564785003662,
      "learning_rate": 0.000192525355709595,
      "loss": 3.7218,
      "step": 24550
    },
    {
      "epoch": 1.795008847620264,
      "grad_norm": 0.03807259351015091,
      "learning_rate": 0.00019230645749726376,
      "loss": 3.7041,
      "step": 24600
    },
    {
      "epoch": 1.7986573508218253,
      "grad_norm": 0.07582968473434448,
      "learning_rate": 0.00019208755928493249,
      "loss": 3.6783,
      "step": 24650
    },
    {
      "epoch": 1.802305854023387,
      "grad_norm": 0.030767671763896942,
      "learning_rate": 0.00019186866107260121,
      "loss": 3.6681,
      "step": 24700
    },
    {
      "epoch": 1.8059543572249486,
      "grad_norm": 0.0707489475607872,
      "learning_rate": 0.00019164976286026997,
      "loss": 3.742,
      "step": 24750
    },
    {
      "epoch": 1.80960286042651,
      "grad_norm": 0.05240349471569061,
      "learning_rate": 0.0001914308646479387,
      "loss": 3.7894,
      "step": 24800
    },
    {
      "epoch": 1.8132513636280716,
      "grad_norm": 0.04019312188029289,
      "learning_rate": 0.00019121196643560743,
      "loss": 3.7302,
      "step": 24850
    },
    {
      "epoch": 1.8168998668296332,
      "grad_norm": 0.04476282000541687,
      "learning_rate": 0.00019099306822327616,
      "loss": 3.707,
      "step": 24900
    },
    {
      "epoch": 1.8205483700311946,
      "grad_norm": 0.04857993125915527,
      "learning_rate": 0.0001907741700109449,
      "loss": 3.7597,
      "step": 24950
    },
    {
      "epoch": 1.8241968732327563,
      "grad_norm": 0.03361140936613083,
      "learning_rate": 0.00019055527179861364,
      "loss": 3.7538,
      "step": 25000
    },
    {
      "epoch": 1.8278453764343179,
      "grad_norm": 0.09826559573411942,
      "learning_rate": 0.00019033637358628234,
      "loss": 3.6868,
      "step": 25050
    },
    {
      "epoch": 1.8314938796358793,
      "grad_norm": 0.03084491938352585,
      "learning_rate": 0.00019011747537395112,
      "loss": 3.7431,
      "step": 25100
    },
    {
      "epoch": 1.835142382837441,
      "grad_norm": 0.11798015981912613,
      "learning_rate": 0.00018989857716161982,
      "loss": 3.7929,
      "step": 25150
    },
    {
      "epoch": 1.8387908860390025,
      "grad_norm": 0.036520007997751236,
      "learning_rate": 0.00018967967894928855,
      "loss": 3.7462,
      "step": 25200
    },
    {
      "epoch": 1.842439389240564,
      "grad_norm": 0.023551637306809425,
      "learning_rate": 0.00018946078073695728,
      "loss": 3.76,
      "step": 25250
    },
    {
      "epoch": 1.8460878924421256,
      "grad_norm": 0.0640057772397995,
      "learning_rate": 0.00018924188252462604,
      "loss": 3.7177,
      "step": 25300
    },
    {
      "epoch": 1.8497363956436872,
      "grad_norm": 0.04241984337568283,
      "learning_rate": 0.00018902298431229476,
      "loss": 3.7021,
      "step": 25350
    },
    {
      "epoch": 1.8533848988452486,
      "grad_norm": 0.036146871745586395,
      "learning_rate": 0.0001888040860999635,
      "loss": 3.756,
      "step": 25400
    },
    {
      "epoch": 1.8570334020468104,
      "grad_norm": 0.03958416357636452,
      "learning_rate": 0.00018858518788763225,
      "loss": 3.7315,
      "step": 25450
    },
    {
      "epoch": 1.8606819052483718,
      "grad_norm": 0.07102249562740326,
      "learning_rate": 0.00018836628967530098,
      "loss": 3.7001,
      "step": 25500
    },
    {
      "epoch": 1.8643304084499333,
      "grad_norm": 0.04141505807638168,
      "learning_rate": 0.0001881473914629697,
      "loss": 3.7048,
      "step": 25550
    },
    {
      "epoch": 1.867978911651495,
      "grad_norm": 0.06838545948266983,
      "learning_rate": 0.00018792849325063843,
      "loss": 3.7255,
      "step": 25600
    },
    {
      "epoch": 1.8716274148530565,
      "grad_norm": 0.03120884858071804,
      "learning_rate": 0.0001877095950383072,
      "loss": 3.7672,
      "step": 25650
    },
    {
      "epoch": 1.8752759180546181,
      "grad_norm": 0.09379472583532333,
      "learning_rate": 0.00018749069682597592,
      "loss": 3.7397,
      "step": 25700
    },
    {
      "epoch": 1.8789244212561798,
      "grad_norm": 0.028936374932527542,
      "learning_rate": 0.00018727179861364462,
      "loss": 3.6687,
      "step": 25750
    },
    {
      "epoch": 1.8825729244577412,
      "grad_norm": 0.035228826105594635,
      "learning_rate": 0.0001870529004013134,
      "loss": 3.8183,
      "step": 25800
    },
    {
      "epoch": 1.8862214276593028,
      "grad_norm": 0.033734120428562164,
      "learning_rate": 0.0001868340021889821,
      "loss": 3.6821,
      "step": 25850
    },
    {
      "epoch": 1.8898699308608644,
      "grad_norm": 0.02987995371222496,
      "learning_rate": 0.00018661510397665083,
      "loss": 3.7407,
      "step": 25900
    },
    {
      "epoch": 1.8935184340624258,
      "grad_norm": 0.3574458956718445,
      "learning_rate": 0.00018639620576431956,
      "loss": 3.6965,
      "step": 25950
    },
    {
      "epoch": 1.8971669372639874,
      "grad_norm": 0.0339631587266922,
      "learning_rate": 0.00018617730755198832,
      "loss": 3.728,
      "step": 26000
    },
    {
      "epoch": 1.900815440465549,
      "grad_norm": 0.0729265958070755,
      "learning_rate": 0.00018595840933965704,
      "loss": 3.7234,
      "step": 26050
    },
    {
      "epoch": 1.9044639436671105,
      "grad_norm": 0.030001645907759666,
      "learning_rate": 0.00018573951112732577,
      "loss": 3.74,
      "step": 26100
    },
    {
      "epoch": 1.908112446868672,
      "grad_norm": 0.06935340911149979,
      "learning_rate": 0.00018552061291499453,
      "loss": 3.7572,
      "step": 26150
    },
    {
      "epoch": 1.9117609500702337,
      "grad_norm": 0.0831083431839943,
      "learning_rate": 0.00018530171470266326,
      "loss": 3.7083,
      "step": 26200
    },
    {
      "epoch": 1.9154094532717951,
      "grad_norm": 0.03957926481962204,
      "learning_rate": 0.00018508281649033198,
      "loss": 3.686,
      "step": 26250
    },
    {
      "epoch": 1.9190579564733568,
      "grad_norm": 0.028678100556135178,
      "learning_rate": 0.0001848639182780007,
      "loss": 3.7491,
      "step": 26300
    },
    {
      "epoch": 1.9227064596749184,
      "grad_norm": 0.033647097647190094,
      "learning_rate": 0.00018464502006566947,
      "loss": 3.649,
      "step": 26350
    },
    {
      "epoch": 1.9263549628764798,
      "grad_norm": 0.04425426945090294,
      "learning_rate": 0.00018442612185333817,
      "loss": 3.7429,
      "step": 26400
    },
    {
      "epoch": 1.9300034660780416,
      "grad_norm": 0.03666727989912033,
      "learning_rate": 0.0001842072236410069,
      "loss": 3.7211,
      "step": 26450
    },
    {
      "epoch": 1.933651969279603,
      "grad_norm": 0.059716686606407166,
      "learning_rate": 0.00018398832542867565,
      "loss": 3.7194,
      "step": 26500
    },
    {
      "epoch": 1.9373004724811644,
      "grad_norm": 0.036296624690294266,
      "learning_rate": 0.00018376942721634438,
      "loss": 3.6925,
      "step": 26550
    },
    {
      "epoch": 1.9409489756827263,
      "grad_norm": 0.03174708038568497,
      "learning_rate": 0.0001835505290040131,
      "loss": 3.7636,
      "step": 26600
    },
    {
      "epoch": 1.9445974788842877,
      "grad_norm": 0.038897812366485596,
      "learning_rate": 0.00018333163079168184,
      "loss": 3.7237,
      "step": 26650
    },
    {
      "epoch": 1.9482459820858493,
      "grad_norm": 0.09178526699542999,
      "learning_rate": 0.0001831127325793506,
      "loss": 3.7073,
      "step": 26700
    },
    {
      "epoch": 1.951894485287411,
      "grad_norm": 0.1691887378692627,
      "learning_rate": 0.00018289383436701932,
      "loss": 3.78,
      "step": 26750
    },
    {
      "epoch": 1.9555429884889723,
      "grad_norm": 0.04344664141535759,
      "learning_rate": 0.00018267493615468805,
      "loss": 3.7942,
      "step": 26800
    },
    {
      "epoch": 1.959191491690534,
      "grad_norm": 0.053674545139074326,
      "learning_rate": 0.0001824560379423568,
      "loss": 3.755,
      "step": 26850
    },
    {
      "epoch": 1.9628399948920956,
      "grad_norm": 0.026353230699896812,
      "learning_rate": 0.00018223713973002553,
      "loss": 3.7186,
      "step": 26900
    },
    {
      "epoch": 1.966488498093657,
      "grad_norm": 0.02703090012073517,
      "learning_rate": 0.00018201824151769426,
      "loss": 3.8022,
      "step": 26950
    },
    {
      "epoch": 1.9701370012952186,
      "grad_norm": 0.05088435858488083,
      "learning_rate": 0.00018179934330536296,
      "loss": 3.7215,
      "step": 27000
    },
    {
      "epoch": 1.9737855044967803,
      "grad_norm": 0.05746394768357277,
      "learning_rate": 0.00018158044509303175,
      "loss": 3.7214,
      "step": 27050
    },
    {
      "epoch": 1.9774340076983417,
      "grad_norm": 0.043972764164209366,
      "learning_rate": 0.00018136154688070045,
      "loss": 3.6459,
      "step": 27100
    },
    {
      "epoch": 1.9810825108999033,
      "grad_norm": 0.1445809304714203,
      "learning_rate": 0.00018114264866836918,
      "loss": 3.7044,
      "step": 27150
    },
    {
      "epoch": 1.984731014101465,
      "grad_norm": 0.06865675747394562,
      "learning_rate": 0.00018092375045603793,
      "loss": 3.7722,
      "step": 27200
    },
    {
      "epoch": 1.9883795173030263,
      "grad_norm": 0.050892237573862076,
      "learning_rate": 0.00018070485224370666,
      "loss": 3.7315,
      "step": 27250
    },
    {
      "epoch": 1.992028020504588,
      "grad_norm": 0.03812471404671669,
      "learning_rate": 0.0001804859540313754,
      "loss": 3.6713,
      "step": 27300
    },
    {
      "epoch": 1.9956765237061496,
      "grad_norm": 0.036164265125989914,
      "learning_rate": 0.00018026705581904412,
      "loss": 3.7273,
      "step": 27350
    },
    {
      "epoch": 1.999325026907711,
      "grad_norm": 0.05214216932654381,
      "learning_rate": 0.00018004815760671287,
      "loss": 3.6758,
      "step": 27400
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.5996508598327637,
      "eval_runtime": 2.9755,
      "eval_samples_per_second": 33.607,
      "eval_steps_per_second": 4.369,
      "step": 27410
    }
  ],
  "logging_steps": 50,
  "max_steps": 68525,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.0379919592718336e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
