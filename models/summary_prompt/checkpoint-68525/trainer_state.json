{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 68525,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0036485032015615595,
      "grad_norm": 0.06168973445892334,
      "learning_rate": 0.00029978547975191534,
      "loss": 3.8209,
      "step": 50
    },
    {
      "epoch": 0.007297006403123119,
      "grad_norm": 0.06945208460092545,
      "learning_rate": 0.0002995665815395841,
      "loss": 3.807,
      "step": 100
    },
    {
      "epoch": 0.010945509604684678,
      "grad_norm": 0.08085992187261581,
      "learning_rate": 0.0002993476833272528,
      "loss": 3.8061,
      "step": 150
    },
    {
      "epoch": 0.014594012806246238,
      "grad_norm": 0.0659402385354042,
      "learning_rate": 0.00029912878511492155,
      "loss": 3.8285,
      "step": 200
    },
    {
      "epoch": 0.018242516007807796,
      "grad_norm": 0.05315134674310684,
      "learning_rate": 0.00029890988690259025,
      "loss": 3.8489,
      "step": 250
    },
    {
      "epoch": 0.021891019209369356,
      "grad_norm": 0.044151876121759415,
      "learning_rate": 0.000298690988690259,
      "loss": 3.8163,
      "step": 300
    },
    {
      "epoch": 0.025539522410930916,
      "grad_norm": 0.04935237020254135,
      "learning_rate": 0.00029847209047792776,
      "loss": 3.776,
      "step": 350
    },
    {
      "epoch": 0.029188025612492476,
      "grad_norm": 0.13402259349822998,
      "learning_rate": 0.00029825319226559646,
      "loss": 3.7775,
      "step": 400
    },
    {
      "epoch": 0.032836528814054036,
      "grad_norm": 0.03836560994386673,
      "learning_rate": 0.0002980342940532652,
      "loss": 3.7656,
      "step": 450
    },
    {
      "epoch": 0.03648503201561559,
      "grad_norm": 0.06438777595758438,
      "learning_rate": 0.00029781539584093397,
      "loss": 3.7798,
      "step": 500
    },
    {
      "epoch": 0.040133535217177156,
      "grad_norm": 0.06501829624176025,
      "learning_rate": 0.0002975964976286027,
      "loss": 3.7823,
      "step": 550
    },
    {
      "epoch": 0.04378203841873871,
      "grad_norm": 0.06458926200866699,
      "learning_rate": 0.0002973775994162714,
      "loss": 3.7709,
      "step": 600
    },
    {
      "epoch": 0.04743054162030027,
      "grad_norm": 0.1818145364522934,
      "learning_rate": 0.00029715870120394013,
      "loss": 3.7925,
      "step": 650
    },
    {
      "epoch": 0.05107904482186183,
      "grad_norm": 0.07774771749973297,
      "learning_rate": 0.0002969398029916089,
      "loss": 3.8046,
      "step": 700
    },
    {
      "epoch": 0.05472754802342339,
      "grad_norm": 0.044946275651454926,
      "learning_rate": 0.0002967209047792776,
      "loss": 3.7508,
      "step": 750
    },
    {
      "epoch": 0.05837605122498495,
      "grad_norm": 0.06420096009969711,
      "learning_rate": 0.00029650200656694634,
      "loss": 3.7801,
      "step": 800
    },
    {
      "epoch": 0.06202455442654651,
      "grad_norm": 0.05023753270506859,
      "learning_rate": 0.0002962831083546151,
      "loss": 3.7852,
      "step": 850
    },
    {
      "epoch": 0.06567305762810807,
      "grad_norm": 0.05120198428630829,
      "learning_rate": 0.0002960642101422838,
      "loss": 3.8047,
      "step": 900
    },
    {
      "epoch": 0.06932156082966963,
      "grad_norm": 0.0664324089884758,
      "learning_rate": 0.00029584531192995255,
      "loss": 3.745,
      "step": 950
    },
    {
      "epoch": 0.07297006403123119,
      "grad_norm": 0.053615815937519073,
      "learning_rate": 0.0002956264137176213,
      "loss": 3.7325,
      "step": 1000
    },
    {
      "epoch": 0.07661856723279274,
      "grad_norm": 0.055131010711193085,
      "learning_rate": 0.00029540751550529,
      "loss": 3.7752,
      "step": 1050
    },
    {
      "epoch": 0.08026707043435431,
      "grad_norm": 0.055140383541584015,
      "learning_rate": 0.00029518861729295877,
      "loss": 3.7759,
      "step": 1100
    },
    {
      "epoch": 0.08391557363591587,
      "grad_norm": 0.05693132057785988,
      "learning_rate": 0.0002949697190806275,
      "loss": 3.8138,
      "step": 1150
    },
    {
      "epoch": 0.08756407683747743,
      "grad_norm": 0.06278181076049805,
      "learning_rate": 0.0002947508208682962,
      "loss": 3.7514,
      "step": 1200
    },
    {
      "epoch": 0.09121258003903898,
      "grad_norm": 0.05038214474916458,
      "learning_rate": 0.0002945319226559649,
      "loss": 3.7932,
      "step": 1250
    },
    {
      "epoch": 0.09486108324060054,
      "grad_norm": 0.07193893194198608,
      "learning_rate": 0.0002943130244436337,
      "loss": 3.7459,
      "step": 1300
    },
    {
      "epoch": 0.09850958644216211,
      "grad_norm": 0.04489973187446594,
      "learning_rate": 0.00029409412623130244,
      "loss": 3.8312,
      "step": 1350
    },
    {
      "epoch": 0.10215808964372367,
      "grad_norm": 0.06196662783622742,
      "learning_rate": 0.00029387522801897114,
      "loss": 3.7967,
      "step": 1400
    },
    {
      "epoch": 0.10580659284528522,
      "grad_norm": 0.04057652875781059,
      "learning_rate": 0.0002936563298066399,
      "loss": 3.7156,
      "step": 1450
    },
    {
      "epoch": 0.10945509604684678,
      "grad_norm": 0.0447261743247509,
      "learning_rate": 0.00029343743159430865,
      "loss": 3.7739,
      "step": 1500
    },
    {
      "epoch": 0.11310359924840833,
      "grad_norm": 0.04624299332499504,
      "learning_rate": 0.00029321853338197735,
      "loss": 3.8316,
      "step": 1550
    },
    {
      "epoch": 0.1167521024499699,
      "grad_norm": 0.05665179342031479,
      "learning_rate": 0.0002929996351696461,
      "loss": 3.7806,
      "step": 1600
    },
    {
      "epoch": 0.12040060565153146,
      "grad_norm": 0.12215862423181534,
      "learning_rate": 0.0002927807369573148,
      "loss": 3.732,
      "step": 1650
    },
    {
      "epoch": 0.12404910885309302,
      "grad_norm": 0.037881702184677124,
      "learning_rate": 0.00029256183874498356,
      "loss": 3.7962,
      "step": 1700
    },
    {
      "epoch": 0.12769761205465457,
      "grad_norm": 0.05145750194787979,
      "learning_rate": 0.0002923429405326523,
      "loss": 3.7747,
      "step": 1750
    },
    {
      "epoch": 0.13134611525621614,
      "grad_norm": 0.04171350598335266,
      "learning_rate": 0.000292124042320321,
      "loss": 3.8249,
      "step": 1800
    },
    {
      "epoch": 0.1349946184577777,
      "grad_norm": 0.060327451676130295,
      "learning_rate": 0.0002919051441079898,
      "loss": 3.8035,
      "step": 1850
    },
    {
      "epoch": 0.13864312165933926,
      "grad_norm": 0.05733056738972664,
      "learning_rate": 0.0002916862458956585,
      "loss": 3.7642,
      "step": 1900
    },
    {
      "epoch": 0.14229162486090083,
      "grad_norm": 0.0331217423081398,
      "learning_rate": 0.00029146734768332723,
      "loss": 3.7772,
      "step": 1950
    },
    {
      "epoch": 0.14594012806246237,
      "grad_norm": 0.04495008662343025,
      "learning_rate": 0.00029124844947099593,
      "loss": 3.843,
      "step": 2000
    },
    {
      "epoch": 0.14958863126402394,
      "grad_norm": 0.03319648653268814,
      "learning_rate": 0.0002910295512586647,
      "loss": 3.7777,
      "step": 2050
    },
    {
      "epoch": 0.15323713446558548,
      "grad_norm": 0.11495686322450638,
      "learning_rate": 0.00029081065304633344,
      "loss": 3.8468,
      "step": 2100
    },
    {
      "epoch": 0.15688563766714705,
      "grad_norm": 0.03723183274269104,
      "learning_rate": 0.00029059175483400214,
      "loss": 3.7911,
      "step": 2150
    },
    {
      "epoch": 0.16053414086870862,
      "grad_norm": 0.3307992219924927,
      "learning_rate": 0.0002903728566216709,
      "loss": 3.7926,
      "step": 2200
    },
    {
      "epoch": 0.16418264407027017,
      "grad_norm": 0.04692172259092331,
      "learning_rate": 0.00029015395840933966,
      "loss": 3.7424,
      "step": 2250
    },
    {
      "epoch": 0.16783114727183174,
      "grad_norm": 0.03813797980546951,
      "learning_rate": 0.00028993506019700836,
      "loss": 3.8132,
      "step": 2300
    },
    {
      "epoch": 0.17147965047339328,
      "grad_norm": 0.08578605949878693,
      "learning_rate": 0.0002897161619846771,
      "loss": 3.7534,
      "step": 2350
    },
    {
      "epoch": 0.17512815367495485,
      "grad_norm": 0.16760919988155365,
      "learning_rate": 0.00028949726377234587,
      "loss": 3.7728,
      "step": 2400
    },
    {
      "epoch": 0.17877665687651642,
      "grad_norm": 0.0770140066742897,
      "learning_rate": 0.00028927836556001457,
      "loss": 3.7526,
      "step": 2450
    },
    {
      "epoch": 0.18242516007807796,
      "grad_norm": 0.046180251985788345,
      "learning_rate": 0.00028905946734768327,
      "loss": 3.8012,
      "step": 2500
    },
    {
      "epoch": 0.18607366327963953,
      "grad_norm": 0.03855273127555847,
      "learning_rate": 0.000288840569135352,
      "loss": 3.89,
      "step": 2550
    },
    {
      "epoch": 0.18972216648120108,
      "grad_norm": 0.033983685076236725,
      "learning_rate": 0.0002886216709230208,
      "loss": 3.7971,
      "step": 2600
    },
    {
      "epoch": 0.19337066968276265,
      "grad_norm": 0.052928801625967026,
      "learning_rate": 0.0002884027727106895,
      "loss": 3.7804,
      "step": 2650
    },
    {
      "epoch": 0.19701917288432422,
      "grad_norm": 0.04947362840175629,
      "learning_rate": 0.00028818387449835824,
      "loss": 3.7944,
      "step": 2700
    },
    {
      "epoch": 0.20066767608588576,
      "grad_norm": 0.03991773724555969,
      "learning_rate": 0.000287964976286027,
      "loss": 3.7954,
      "step": 2750
    },
    {
      "epoch": 0.20431617928744733,
      "grad_norm": 0.046088214963674545,
      "learning_rate": 0.0002877460780736957,
      "loss": 3.7915,
      "step": 2800
    },
    {
      "epoch": 0.20796468248900887,
      "grad_norm": 0.08090098947286606,
      "learning_rate": 0.00028752717986136445,
      "loss": 3.8283,
      "step": 2850
    },
    {
      "epoch": 0.21161318569057044,
      "grad_norm": 0.050669629126787186,
      "learning_rate": 0.0002873082816490332,
      "loss": 3.7318,
      "step": 2900
    },
    {
      "epoch": 0.215261688892132,
      "grad_norm": 0.04391877353191376,
      "learning_rate": 0.0002870893834367019,
      "loss": 3.7945,
      "step": 2950
    },
    {
      "epoch": 0.21891019209369356,
      "grad_norm": 0.0506870299577713,
      "learning_rate": 0.00028687048522437066,
      "loss": 3.8057,
      "step": 3000
    },
    {
      "epoch": 0.22255869529525513,
      "grad_norm": 0.13693498075008392,
      "learning_rate": 0.00028665158701203936,
      "loss": 3.7884,
      "step": 3050
    },
    {
      "epoch": 0.22620719849681667,
      "grad_norm": 0.059501904994249344,
      "learning_rate": 0.0002864326887997081,
      "loss": 3.86,
      "step": 3100
    },
    {
      "epoch": 0.22985570169837824,
      "grad_norm": 0.08309494704008102,
      "learning_rate": 0.0002862137905873768,
      "loss": 3.7692,
      "step": 3150
    },
    {
      "epoch": 0.2335042048999398,
      "grad_norm": 0.057229477912187576,
      "learning_rate": 0.0002859948923750456,
      "loss": 3.815,
      "step": 3200
    },
    {
      "epoch": 0.23715270810150135,
      "grad_norm": 0.05534157156944275,
      "learning_rate": 0.00028577599416271433,
      "loss": 3.8132,
      "step": 3250
    },
    {
      "epoch": 0.24080121130306292,
      "grad_norm": 0.05423249676823616,
      "learning_rate": 0.00028555709595038303,
      "loss": 3.8105,
      "step": 3300
    },
    {
      "epoch": 0.24444971450462447,
      "grad_norm": 0.1370498090982437,
      "learning_rate": 0.0002853381977380518,
      "loss": 3.7925,
      "step": 3350
    },
    {
      "epoch": 0.24809821770618604,
      "grad_norm": 0.050562504678964615,
      "learning_rate": 0.0002851192995257205,
      "loss": 3.7878,
      "step": 3400
    },
    {
      "epoch": 0.2517467209077476,
      "grad_norm": 0.03731625899672508,
      "learning_rate": 0.00028490040131338924,
      "loss": 3.8141,
      "step": 3450
    },
    {
      "epoch": 0.25539522410930915,
      "grad_norm": 0.057750578969717026,
      "learning_rate": 0.000284681503101058,
      "loss": 3.804,
      "step": 3500
    },
    {
      "epoch": 0.2590437273108707,
      "grad_norm": 0.08468908816576004,
      "learning_rate": 0.0002844626048887267,
      "loss": 3.804,
      "step": 3550
    },
    {
      "epoch": 0.2626922305124323,
      "grad_norm": 0.04053274542093277,
      "learning_rate": 0.00028424370667639546,
      "loss": 3.7404,
      "step": 3600
    },
    {
      "epoch": 0.26634073371399386,
      "grad_norm": 0.056923758238554,
      "learning_rate": 0.0002840248084640642,
      "loss": 3.8564,
      "step": 3650
    },
    {
      "epoch": 0.2699892369155554,
      "grad_norm": 0.061479076743125916,
      "learning_rate": 0.0002838059102517329,
      "loss": 3.7302,
      "step": 3700
    },
    {
      "epoch": 0.27363774011711695,
      "grad_norm": 0.03686347231268883,
      "learning_rate": 0.0002835870120394016,
      "loss": 3.7742,
      "step": 3750
    },
    {
      "epoch": 0.2772862433186785,
      "grad_norm": 0.09652872383594513,
      "learning_rate": 0.0002833681138270704,
      "loss": 3.7584,
      "step": 3800
    },
    {
      "epoch": 0.2809347465202401,
      "grad_norm": 0.04618903622031212,
      "learning_rate": 0.0002831492156147391,
      "loss": 3.794,
      "step": 3850
    },
    {
      "epoch": 0.28458324972180166,
      "grad_norm": 0.026881106197834015,
      "learning_rate": 0.0002829303174024078,
      "loss": 3.7537,
      "step": 3900
    },
    {
      "epoch": 0.28823175292336317,
      "grad_norm": 0.047772299498319626,
      "learning_rate": 0.0002827114191900766,
      "loss": 3.8091,
      "step": 3950
    },
    {
      "epoch": 0.29188025612492474,
      "grad_norm": 0.09770311415195465,
      "learning_rate": 0.00028249252097774534,
      "loss": 3.7222,
      "step": 4000
    },
    {
      "epoch": 0.2955287593264863,
      "grad_norm": 0.058687321841716766,
      "learning_rate": 0.00028227362276541404,
      "loss": 3.7192,
      "step": 4050
    },
    {
      "epoch": 0.2991772625280479,
      "grad_norm": 0.03348792344331741,
      "learning_rate": 0.0002820547245530828,
      "loss": 3.8284,
      "step": 4100
    },
    {
      "epoch": 0.30282576572960945,
      "grad_norm": 0.04036444425582886,
      "learning_rate": 0.00028183582634075155,
      "loss": 3.7396,
      "step": 4150
    },
    {
      "epoch": 0.30647426893117097,
      "grad_norm": 0.05977707356214523,
      "learning_rate": 0.00028161692812842025,
      "loss": 3.833,
      "step": 4200
    },
    {
      "epoch": 0.31012277213273254,
      "grad_norm": 0.05918131768703461,
      "learning_rate": 0.000281398029916089,
      "loss": 3.7626,
      "step": 4250
    },
    {
      "epoch": 0.3137712753342941,
      "grad_norm": 0.035686418414115906,
      "learning_rate": 0.00028117913170375776,
      "loss": 3.8577,
      "step": 4300
    },
    {
      "epoch": 0.3174197785358557,
      "grad_norm": 0.03383041173219681,
      "learning_rate": 0.00028096023349142646,
      "loss": 3.7575,
      "step": 4350
    },
    {
      "epoch": 0.32106828173741725,
      "grad_norm": 0.03892826288938522,
      "learning_rate": 0.0002807413352790952,
      "loss": 3.77,
      "step": 4400
    },
    {
      "epoch": 0.32471678493897876,
      "grad_norm": 0.058677367866039276,
      "learning_rate": 0.0002805224370667639,
      "loss": 3.7666,
      "step": 4450
    },
    {
      "epoch": 0.32836528814054033,
      "grad_norm": 0.043243780732154846,
      "learning_rate": 0.0002803035388544327,
      "loss": 3.8098,
      "step": 4500
    },
    {
      "epoch": 0.3320137913421019,
      "grad_norm": 0.048561178147792816,
      "learning_rate": 0.0002800846406421014,
      "loss": 3.7292,
      "step": 4550
    },
    {
      "epoch": 0.3356622945436635,
      "grad_norm": 0.050229985266923904,
      "learning_rate": 0.00027986574242977013,
      "loss": 3.7582,
      "step": 4600
    },
    {
      "epoch": 0.33931079774522505,
      "grad_norm": 0.04608413577079773,
      "learning_rate": 0.0002796468442174389,
      "loss": 3.7968,
      "step": 4650
    },
    {
      "epoch": 0.34295930094678656,
      "grad_norm": 0.03866652026772499,
      "learning_rate": 0.0002794279460051076,
      "loss": 3.7742,
      "step": 4700
    },
    {
      "epoch": 0.34660780414834813,
      "grad_norm": 0.046391092240810394,
      "learning_rate": 0.00027920904779277634,
      "loss": 3.8094,
      "step": 4750
    },
    {
      "epoch": 0.3502563073499097,
      "grad_norm": 0.05525911971926689,
      "learning_rate": 0.00027899014958044505,
      "loss": 3.7614,
      "step": 4800
    },
    {
      "epoch": 0.35390481055147127,
      "grad_norm": 0.2321861833333969,
      "learning_rate": 0.0002787712513681138,
      "loss": 3.8035,
      "step": 4850
    },
    {
      "epoch": 0.35755331375303284,
      "grad_norm": 0.032668691128492355,
      "learning_rate": 0.00027855235315578256,
      "loss": 3.7511,
      "step": 4900
    },
    {
      "epoch": 0.36120181695459436,
      "grad_norm": 0.036952581256628036,
      "learning_rate": 0.00027833345494345126,
      "loss": 3.7405,
      "step": 4950
    },
    {
      "epoch": 0.3648503201561559,
      "grad_norm": 0.03920332342386246,
      "learning_rate": 0.00027811455673112,
      "loss": 3.8023,
      "step": 5000
    },
    {
      "epoch": 0.3684988233577175,
      "grad_norm": 0.042424824088811874,
      "learning_rate": 0.00027789565851878877,
      "loss": 3.765,
      "step": 5050
    },
    {
      "epoch": 0.37214732655927907,
      "grad_norm": 0.05212566256523132,
      "learning_rate": 0.00027767676030645747,
      "loss": 3.7448,
      "step": 5100
    },
    {
      "epoch": 0.37579582976084064,
      "grad_norm": 0.03994728997349739,
      "learning_rate": 0.00027745786209412617,
      "loss": 3.8031,
      "step": 5150
    },
    {
      "epoch": 0.37944433296240215,
      "grad_norm": 0.05328117311000824,
      "learning_rate": 0.00027723896388179493,
      "loss": 3.8124,
      "step": 5200
    },
    {
      "epoch": 0.3830928361639637,
      "grad_norm": 0.03581259399652481,
      "learning_rate": 0.0002770200656694637,
      "loss": 3.7812,
      "step": 5250
    },
    {
      "epoch": 0.3867413393655253,
      "grad_norm": 0.09596124291419983,
      "learning_rate": 0.0002768011674571324,
      "loss": 3.805,
      "step": 5300
    },
    {
      "epoch": 0.39038984256708686,
      "grad_norm": 0.05078890174627304,
      "learning_rate": 0.00027658226924480114,
      "loss": 3.8448,
      "step": 5350
    },
    {
      "epoch": 0.39403834576864843,
      "grad_norm": 0.043389398604631424,
      "learning_rate": 0.0002763633710324699,
      "loss": 3.7753,
      "step": 5400
    },
    {
      "epoch": 0.39768684897020995,
      "grad_norm": 0.03501654788851738,
      "learning_rate": 0.0002761444728201386,
      "loss": 3.7624,
      "step": 5450
    },
    {
      "epoch": 0.4013353521717715,
      "grad_norm": 0.038352906703948975,
      "learning_rate": 0.00027592557460780735,
      "loss": 3.7308,
      "step": 5500
    },
    {
      "epoch": 0.4049838553733331,
      "grad_norm": 0.03946085274219513,
      "learning_rate": 0.0002757066763954761,
      "loss": 3.7477,
      "step": 5550
    },
    {
      "epoch": 0.40863235857489466,
      "grad_norm": 0.0423196405172348,
      "learning_rate": 0.0002754877781831448,
      "loss": 3.8115,
      "step": 5600
    },
    {
      "epoch": 0.41228086177645623,
      "grad_norm": 0.05357174947857857,
      "learning_rate": 0.00027526887997081356,
      "loss": 3.7472,
      "step": 5650
    },
    {
      "epoch": 0.41592936497801775,
      "grad_norm": 0.03342980891466141,
      "learning_rate": 0.0002750499817584823,
      "loss": 3.8031,
      "step": 5700
    },
    {
      "epoch": 0.4195778681795793,
      "grad_norm": 0.045062050223350525,
      "learning_rate": 0.000274831083546151,
      "loss": 3.7107,
      "step": 5750
    },
    {
      "epoch": 0.4232263713811409,
      "grad_norm": 0.04666491597890854,
      "learning_rate": 0.0002746121853338197,
      "loss": 3.7632,
      "step": 5800
    },
    {
      "epoch": 0.42687487458270246,
      "grad_norm": 0.029684990644454956,
      "learning_rate": 0.0002743932871214885,
      "loss": 3.7848,
      "step": 5850
    },
    {
      "epoch": 0.430523377784264,
      "grad_norm": 0.032057274132966995,
      "learning_rate": 0.00027417438890915723,
      "loss": 3.7159,
      "step": 5900
    },
    {
      "epoch": 0.43417188098582554,
      "grad_norm": 0.38756996393203735,
      "learning_rate": 0.00027395549069682593,
      "loss": 3.7925,
      "step": 5950
    },
    {
      "epoch": 0.4378203841873871,
      "grad_norm": 0.03629770502448082,
      "learning_rate": 0.0002737365924844947,
      "loss": 3.7831,
      "step": 6000
    },
    {
      "epoch": 0.4414688873889487,
      "grad_norm": 0.048876747488975525,
      "learning_rate": 0.00027351769427216345,
      "loss": 3.7472,
      "step": 6050
    },
    {
      "epoch": 0.44511739059051025,
      "grad_norm": 0.059854354709386826,
      "learning_rate": 0.00027329879605983215,
      "loss": 3.8124,
      "step": 6100
    },
    {
      "epoch": 0.4487658937920718,
      "grad_norm": 0.07895169407129288,
      "learning_rate": 0.0002730798978475009,
      "loss": 3.7448,
      "step": 6150
    },
    {
      "epoch": 0.45241439699363334,
      "grad_norm": 0.07532142847776413,
      "learning_rate": 0.0002728609996351696,
      "loss": 3.7715,
      "step": 6200
    },
    {
      "epoch": 0.4560629001951949,
      "grad_norm": 0.036540623754262924,
      "learning_rate": 0.00027264210142283836,
      "loss": 3.755,
      "step": 6250
    },
    {
      "epoch": 0.4597114033967565,
      "grad_norm": 0.05499234050512314,
      "learning_rate": 0.0002724232032105071,
      "loss": 3.7566,
      "step": 6300
    },
    {
      "epoch": 0.46335990659831805,
      "grad_norm": 0.20882061123847961,
      "learning_rate": 0.0002722043049981758,
      "loss": 3.7867,
      "step": 6350
    },
    {
      "epoch": 0.4670084097998796,
      "grad_norm": 0.24420703947544098,
      "learning_rate": 0.00027198540678584457,
      "loss": 3.8599,
      "step": 6400
    },
    {
      "epoch": 0.47065691300144114,
      "grad_norm": 0.08462422341108322,
      "learning_rate": 0.00027176650857351327,
      "loss": 3.7701,
      "step": 6450
    },
    {
      "epoch": 0.4743054162030027,
      "grad_norm": 0.058603543788194656,
      "learning_rate": 0.00027154761036118203,
      "loss": 3.7709,
      "step": 6500
    },
    {
      "epoch": 0.4779539194045643,
      "grad_norm": 0.036258019506931305,
      "learning_rate": 0.00027132871214885073,
      "loss": 3.7483,
      "step": 6550
    },
    {
      "epoch": 0.48160242260612585,
      "grad_norm": 0.056518469005823135,
      "learning_rate": 0.0002711098139365195,
      "loss": 3.7614,
      "step": 6600
    },
    {
      "epoch": 0.4852509258076874,
      "grad_norm": 0.04797118529677391,
      "learning_rate": 0.00027089091572418824,
      "loss": 3.8187,
      "step": 6650
    },
    {
      "epoch": 0.48889942900924893,
      "grad_norm": 0.037991564720869064,
      "learning_rate": 0.00027067201751185694,
      "loss": 3.7599,
      "step": 6700
    },
    {
      "epoch": 0.4925479322108105,
      "grad_norm": 0.15909089148044586,
      "learning_rate": 0.0002704531192995257,
      "loss": 3.734,
      "step": 6750
    },
    {
      "epoch": 0.49619643541237207,
      "grad_norm": 0.05104684457182884,
      "learning_rate": 0.00027023422108719445,
      "loss": 3.8102,
      "step": 6800
    },
    {
      "epoch": 0.49984493861393364,
      "grad_norm": 0.04384302347898483,
      "learning_rate": 0.00027001532287486315,
      "loss": 3.736,
      "step": 6850
    },
    {
      "epoch": 0.5034934418154952,
      "grad_norm": 0.03357456624507904,
      "learning_rate": 0.0002697964246625319,
      "loss": 3.7849,
      "step": 6900
    },
    {
      "epoch": 0.5071419450170568,
      "grad_norm": 0.08118319511413574,
      "learning_rate": 0.00026957752645020066,
      "loss": 3.7579,
      "step": 6950
    },
    {
      "epoch": 0.5107904482186183,
      "grad_norm": 0.05475417152047157,
      "learning_rate": 0.00026935862823786937,
      "loss": 3.7692,
      "step": 7000
    },
    {
      "epoch": 0.5144389514201799,
      "grad_norm": 0.038107018917798996,
      "learning_rate": 0.00026913973002553807,
      "loss": 3.7602,
      "step": 7050
    },
    {
      "epoch": 0.5180874546217414,
      "grad_norm": 0.05635403096675873,
      "learning_rate": 0.0002689208318132069,
      "loss": 3.7427,
      "step": 7100
    },
    {
      "epoch": 0.521735957823303,
      "grad_norm": 0.04352715238928795,
      "learning_rate": 0.0002687019336008756,
      "loss": 3.7403,
      "step": 7150
    },
    {
      "epoch": 0.5253844610248646,
      "grad_norm": 0.05898762494325638,
      "learning_rate": 0.0002684830353885443,
      "loss": 3.7781,
      "step": 7200
    },
    {
      "epoch": 0.5290329642264261,
      "grad_norm": 0.03815086930990219,
      "learning_rate": 0.00026826413717621303,
      "loss": 3.816,
      "step": 7250
    },
    {
      "epoch": 0.5326814674279877,
      "grad_norm": 0.039125699549913406,
      "learning_rate": 0.0002680452389638818,
      "loss": 3.7858,
      "step": 7300
    },
    {
      "epoch": 0.5363299706295492,
      "grad_norm": 0.1979663372039795,
      "learning_rate": 0.0002678263407515505,
      "loss": 3.7172,
      "step": 7350
    },
    {
      "epoch": 0.5399784738311108,
      "grad_norm": 0.06419413536787033,
      "learning_rate": 0.00026760744253921925,
      "loss": 3.7665,
      "step": 7400
    },
    {
      "epoch": 0.5436269770326724,
      "grad_norm": 0.044370170682668686,
      "learning_rate": 0.000267388544326888,
      "loss": 3.7519,
      "step": 7450
    },
    {
      "epoch": 0.5472754802342339,
      "grad_norm": 0.04969647526741028,
      "learning_rate": 0.0002671696461145567,
      "loss": 3.8177,
      "step": 7500
    },
    {
      "epoch": 0.5509239834357955,
      "grad_norm": 0.21754583716392517,
      "learning_rate": 0.00026695074790222546,
      "loss": 3.7718,
      "step": 7550
    },
    {
      "epoch": 0.554572486637357,
      "grad_norm": 0.0563935711979866,
      "learning_rate": 0.00026673184968989416,
      "loss": 3.7828,
      "step": 7600
    },
    {
      "epoch": 0.5582209898389185,
      "grad_norm": 0.0720403641462326,
      "learning_rate": 0.0002665129514775629,
      "loss": 3.716,
      "step": 7650
    },
    {
      "epoch": 0.5618694930404802,
      "grad_norm": 0.12610460817813873,
      "learning_rate": 0.0002662940532652316,
      "loss": 3.7034,
      "step": 7700
    },
    {
      "epoch": 0.5655179962420417,
      "grad_norm": 0.05261334776878357,
      "learning_rate": 0.00026607515505290037,
      "loss": 3.7634,
      "step": 7750
    },
    {
      "epoch": 0.5691664994436033,
      "grad_norm": 0.035278305411338806,
      "learning_rate": 0.00026585625684056913,
      "loss": 3.8057,
      "step": 7800
    },
    {
      "epoch": 0.5728150026451648,
      "grad_norm": 0.07556525617837906,
      "learning_rate": 0.00026563735862823783,
      "loss": 3.7993,
      "step": 7850
    },
    {
      "epoch": 0.5764635058467263,
      "grad_norm": 0.037905529141426086,
      "learning_rate": 0.0002654184604159066,
      "loss": 3.8026,
      "step": 7900
    },
    {
      "epoch": 0.580112009048288,
      "grad_norm": 0.04708358272910118,
      "learning_rate": 0.0002651995622035753,
      "loss": 3.725,
      "step": 7950
    },
    {
      "epoch": 0.5837605122498495,
      "grad_norm": 0.03775063157081604,
      "learning_rate": 0.00026498066399124404,
      "loss": 3.7516,
      "step": 8000
    },
    {
      "epoch": 0.5874090154514111,
      "grad_norm": 0.07850445061922073,
      "learning_rate": 0.0002647617657789128,
      "loss": 3.8048,
      "step": 8050
    },
    {
      "epoch": 0.5910575186529726,
      "grad_norm": 0.055150218307971954,
      "learning_rate": 0.0002645428675665815,
      "loss": 3.7952,
      "step": 8100
    },
    {
      "epoch": 0.5947060218545341,
      "grad_norm": 0.04658922925591469,
      "learning_rate": 0.00026432396935425025,
      "loss": 3.7206,
      "step": 8150
    },
    {
      "epoch": 0.5983545250560958,
      "grad_norm": 0.12437862902879715,
      "learning_rate": 0.000264105071141919,
      "loss": 3.7605,
      "step": 8200
    },
    {
      "epoch": 0.6020030282576573,
      "grad_norm": 0.04028536006808281,
      "learning_rate": 0.0002638861729295877,
      "loss": 3.7845,
      "step": 8250
    },
    {
      "epoch": 0.6056515314592189,
      "grad_norm": 0.04889799281954765,
      "learning_rate": 0.0002636672747172564,
      "loss": 3.7051,
      "step": 8300
    },
    {
      "epoch": 0.6093000346607804,
      "grad_norm": 0.0449523963034153,
      "learning_rate": 0.0002634483765049252,
      "loss": 3.8262,
      "step": 8350
    },
    {
      "epoch": 0.6129485378623419,
      "grad_norm": 0.03449857980012894,
      "learning_rate": 0.0002632294782925939,
      "loss": 3.7999,
      "step": 8400
    },
    {
      "epoch": 0.6165970410639036,
      "grad_norm": 0.040457263588905334,
      "learning_rate": 0.0002630105800802626,
      "loss": 3.7261,
      "step": 8450
    },
    {
      "epoch": 0.6202455442654651,
      "grad_norm": 0.03202451765537262,
      "learning_rate": 0.0002627916818679314,
      "loss": 3.7404,
      "step": 8500
    },
    {
      "epoch": 0.6238940474670267,
      "grad_norm": 0.035345982760190964,
      "learning_rate": 0.00026257278365560013,
      "loss": 3.7975,
      "step": 8550
    },
    {
      "epoch": 0.6275425506685882,
      "grad_norm": 0.07346878945827484,
      "learning_rate": 0.00026235388544326884,
      "loss": 3.7793,
      "step": 8600
    },
    {
      "epoch": 0.6311910538701497,
      "grad_norm": 0.041771192103624344,
      "learning_rate": 0.0002621349872309376,
      "loss": 3.763,
      "step": 8650
    },
    {
      "epoch": 0.6348395570717114,
      "grad_norm": 0.038838401436805725,
      "learning_rate": 0.00026191608901860635,
      "loss": 3.7834,
      "step": 8700
    },
    {
      "epoch": 0.6384880602732729,
      "grad_norm": 0.042152874171733856,
      "learning_rate": 0.00026169719080627505,
      "loss": 3.7367,
      "step": 8750
    },
    {
      "epoch": 0.6421365634748345,
      "grad_norm": 0.061544034630060196,
      "learning_rate": 0.0002614782925939438,
      "loss": 3.7704,
      "step": 8800
    },
    {
      "epoch": 0.645785066676396,
      "grad_norm": 0.07182334363460541,
      "learning_rate": 0.00026125939438161256,
      "loss": 3.8205,
      "step": 8850
    },
    {
      "epoch": 0.6494335698779575,
      "grad_norm": 0.023128552362322807,
      "learning_rate": 0.00026104049616928126,
      "loss": 3.7306,
      "step": 8900
    },
    {
      "epoch": 0.6530820730795192,
      "grad_norm": 0.1577276885509491,
      "learning_rate": 0.00026082159795695,
      "loss": 3.786,
      "step": 8950
    },
    {
      "epoch": 0.6567305762810807,
      "grad_norm": 0.03565877676010132,
      "learning_rate": 0.0002606026997446187,
      "loss": 3.7752,
      "step": 9000
    },
    {
      "epoch": 0.6603790794826423,
      "grad_norm": 0.027303671464323997,
      "learning_rate": 0.00026038380153228747,
      "loss": 3.7949,
      "step": 9050
    },
    {
      "epoch": 0.6640275826842038,
      "grad_norm": 0.054411400109529495,
      "learning_rate": 0.0002601649033199562,
      "loss": 3.7443,
      "step": 9100
    },
    {
      "epoch": 0.6676760858857653,
      "grad_norm": 0.04960021376609802,
      "learning_rate": 0.00025994600510762493,
      "loss": 3.7069,
      "step": 9150
    },
    {
      "epoch": 0.671324589087327,
      "grad_norm": 0.09200795739889145,
      "learning_rate": 0.0002597271068952937,
      "loss": 3.7692,
      "step": 9200
    },
    {
      "epoch": 0.6749730922888885,
      "grad_norm": 0.03438730165362358,
      "learning_rate": 0.0002595082086829624,
      "loss": 3.7694,
      "step": 9250
    },
    {
      "epoch": 0.6786215954904501,
      "grad_norm": 0.035373587161302567,
      "learning_rate": 0.00025928931047063114,
      "loss": 3.7894,
      "step": 9300
    },
    {
      "epoch": 0.6822700986920116,
      "grad_norm": 0.043179698288440704,
      "learning_rate": 0.00025907041225829984,
      "loss": 3.7602,
      "step": 9350
    },
    {
      "epoch": 0.6859186018935731,
      "grad_norm": 0.028434082865715027,
      "learning_rate": 0.0002588515140459686,
      "loss": 3.79,
      "step": 9400
    },
    {
      "epoch": 0.6895671050951347,
      "grad_norm": 0.023637497797608376,
      "learning_rate": 0.00025863261583363735,
      "loss": 3.6746,
      "step": 9450
    },
    {
      "epoch": 0.6932156082966963,
      "grad_norm": 0.04148092865943909,
      "learning_rate": 0.00025841371762130606,
      "loss": 3.7167,
      "step": 9500
    },
    {
      "epoch": 0.6968641114982579,
      "grad_norm": 0.037714675068855286,
      "learning_rate": 0.0002581948194089748,
      "loss": 3.8069,
      "step": 9550
    },
    {
      "epoch": 0.7005126146998194,
      "grad_norm": 0.1351778656244278,
      "learning_rate": 0.00025797592119664357,
      "loss": 3.7434,
      "step": 9600
    },
    {
      "epoch": 0.7041611179013809,
      "grad_norm": 0.04339450225234032,
      "learning_rate": 0.00025775702298431227,
      "loss": 3.7455,
      "step": 9650
    },
    {
      "epoch": 0.7078096211029425,
      "grad_norm": 0.0386631041765213,
      "learning_rate": 0.00025753812477198097,
      "loss": 3.7262,
      "step": 9700
    },
    {
      "epoch": 0.7114581243045041,
      "grad_norm": 0.029608257114887238,
      "learning_rate": 0.0002573192265596497,
      "loss": 3.7308,
      "step": 9750
    },
    {
      "epoch": 0.7151066275060657,
      "grad_norm": 0.044863514602184296,
      "learning_rate": 0.0002571003283473185,
      "loss": 3.7836,
      "step": 9800
    },
    {
      "epoch": 0.7187551307076272,
      "grad_norm": 0.03969186171889305,
      "learning_rate": 0.0002568814301349872,
      "loss": 3.7858,
      "step": 9850
    },
    {
      "epoch": 0.7224036339091887,
      "grad_norm": 0.033198028802871704,
      "learning_rate": 0.00025666253192265594,
      "loss": 3.805,
      "step": 9900
    },
    {
      "epoch": 0.7260521371107503,
      "grad_norm": 0.04711172729730606,
      "learning_rate": 0.0002564436337103247,
      "loss": 3.7364,
      "step": 9950
    },
    {
      "epoch": 0.7297006403123119,
      "grad_norm": 0.04380730539560318,
      "learning_rate": 0.0002562247354979934,
      "loss": 3.7527,
      "step": 10000
    },
    {
      "epoch": 0.7333491435138735,
      "grad_norm": 0.03492945432662964,
      "learning_rate": 0.00025600583728566215,
      "loss": 3.788,
      "step": 10050
    },
    {
      "epoch": 0.736997646715435,
      "grad_norm": 0.040144167840480804,
      "learning_rate": 0.0002557869390733309,
      "loss": 3.787,
      "step": 10100
    },
    {
      "epoch": 0.7406461499169965,
      "grad_norm": 0.3803487718105316,
      "learning_rate": 0.0002555680408609996,
      "loss": 3.7476,
      "step": 10150
    },
    {
      "epoch": 0.7442946531185581,
      "grad_norm": 0.03278876096010208,
      "learning_rate": 0.00025534914264866836,
      "loss": 3.7914,
      "step": 10200
    },
    {
      "epoch": 0.7479431563201197,
      "grad_norm": 0.050172772258520126,
      "learning_rate": 0.0002551302444363371,
      "loss": 3.8092,
      "step": 10250
    },
    {
      "epoch": 0.7515916595216813,
      "grad_norm": 0.0357128269970417,
      "learning_rate": 0.0002549113462240058,
      "loss": 3.7576,
      "step": 10300
    },
    {
      "epoch": 0.7552401627232428,
      "grad_norm": 0.02880084700882435,
      "learning_rate": 0.0002546924480116745,
      "loss": 3.7878,
      "step": 10350
    },
    {
      "epoch": 0.7588886659248043,
      "grad_norm": 0.041418302804231644,
      "learning_rate": 0.0002544735497993433,
      "loss": 3.7377,
      "step": 10400
    },
    {
      "epoch": 0.7625371691263659,
      "grad_norm": 0.05331307649612427,
      "learning_rate": 0.00025425465158701203,
      "loss": 3.7702,
      "step": 10450
    },
    {
      "epoch": 0.7661856723279274,
      "grad_norm": 0.037208449095487595,
      "learning_rate": 0.00025403575337468073,
      "loss": 3.8176,
      "step": 10500
    },
    {
      "epoch": 0.7698341755294891,
      "grad_norm": 0.1868438571691513,
      "learning_rate": 0.0002538168551623495,
      "loss": 3.7435,
      "step": 10550
    },
    {
      "epoch": 0.7734826787310506,
      "grad_norm": 0.03965349867939949,
      "learning_rate": 0.00025359795695001824,
      "loss": 3.7943,
      "step": 10600
    },
    {
      "epoch": 0.7771311819326121,
      "grad_norm": 0.04679163917899132,
      "learning_rate": 0.00025337905873768694,
      "loss": 3.7727,
      "step": 10650
    },
    {
      "epoch": 0.7807796851341737,
      "grad_norm": 0.05407506600022316,
      "learning_rate": 0.0002531601605253557,
      "loss": 3.7681,
      "step": 10700
    },
    {
      "epoch": 0.7844281883357352,
      "grad_norm": 0.052542489022016525,
      "learning_rate": 0.0002529412623130244,
      "loss": 3.6909,
      "step": 10750
    },
    {
      "epoch": 0.7880766915372969,
      "grad_norm": 0.04365282878279686,
      "learning_rate": 0.00025272236410069316,
      "loss": 3.8204,
      "step": 10800
    },
    {
      "epoch": 0.7917251947388584,
      "grad_norm": 0.03474139794707298,
      "learning_rate": 0.0002525034658883619,
      "loss": 3.7248,
      "step": 10850
    },
    {
      "epoch": 0.7953736979404199,
      "grad_norm": 0.03610363230109215,
      "learning_rate": 0.0002522845676760306,
      "loss": 3.7517,
      "step": 10900
    },
    {
      "epoch": 0.7990222011419815,
      "grad_norm": 0.039271511137485504,
      "learning_rate": 0.00025206566946369937,
      "loss": 3.7778,
      "step": 10950
    },
    {
      "epoch": 0.802670704343543,
      "grad_norm": 0.039804987609386444,
      "learning_rate": 0.00025184677125136807,
      "loss": 3.7286,
      "step": 11000
    },
    {
      "epoch": 0.8063192075451047,
      "grad_norm": 0.16726399958133698,
      "learning_rate": 0.0002516278730390368,
      "loss": 3.7227,
      "step": 11050
    },
    {
      "epoch": 0.8099677107466662,
      "grad_norm": 0.04762920364737511,
      "learning_rate": 0.0002514089748267055,
      "loss": 3.7642,
      "step": 11100
    },
    {
      "epoch": 0.8136162139482277,
      "grad_norm": 0.03606169670820236,
      "learning_rate": 0.0002511900766143743,
      "loss": 3.7091,
      "step": 11150
    },
    {
      "epoch": 0.8172647171497893,
      "grad_norm": 0.033103834837675095,
      "learning_rate": 0.00025097117840204304,
      "loss": 3.7931,
      "step": 11200
    },
    {
      "epoch": 0.8209132203513508,
      "grad_norm": 0.039384398609399796,
      "learning_rate": 0.00025075228018971174,
      "loss": 3.7181,
      "step": 11250
    },
    {
      "epoch": 0.8245617235529125,
      "grad_norm": 0.04030264914035797,
      "learning_rate": 0.0002505333819773805,
      "loss": 3.7372,
      "step": 11300
    },
    {
      "epoch": 0.828210226754474,
      "grad_norm": 0.0351419635117054,
      "learning_rate": 0.00025031448376504925,
      "loss": 3.761,
      "step": 11350
    },
    {
      "epoch": 0.8318587299560355,
      "grad_norm": 0.046962421387434006,
      "learning_rate": 0.00025009558555271795,
      "loss": 3.796,
      "step": 11400
    },
    {
      "epoch": 0.8355072331575971,
      "grad_norm": 0.042807839810848236,
      "learning_rate": 0.0002498766873403867,
      "loss": 3.7763,
      "step": 11450
    },
    {
      "epoch": 0.8391557363591586,
      "grad_norm": 0.054092634469270706,
      "learning_rate": 0.00024965778912805546,
      "loss": 3.7508,
      "step": 11500
    },
    {
      "epoch": 0.8428042395607203,
      "grad_norm": 0.03967931121587753,
      "learning_rate": 0.00024943889091572416,
      "loss": 3.7436,
      "step": 11550
    },
    {
      "epoch": 0.8464527427622818,
      "grad_norm": 0.03748660907149315,
      "learning_rate": 0.00024921999270339286,
      "loss": 3.7681,
      "step": 11600
    },
    {
      "epoch": 0.8501012459638433,
      "grad_norm": 0.06830700486898422,
      "learning_rate": 0.0002490010944910617,
      "loss": 3.7668,
      "step": 11650
    },
    {
      "epoch": 0.8537497491654049,
      "grad_norm": 0.04274776205420494,
      "learning_rate": 0.0002487821962787304,
      "loss": 3.8394,
      "step": 11700
    },
    {
      "epoch": 0.8573982523669664,
      "grad_norm": 0.03649476543068886,
      "learning_rate": 0.0002485632980663991,
      "loss": 3.7252,
      "step": 11750
    },
    {
      "epoch": 0.861046755568528,
      "grad_norm": 0.0675521269440651,
      "learning_rate": 0.00024834439985406783,
      "loss": 3.7771,
      "step": 11800
    },
    {
      "epoch": 0.8646952587700896,
      "grad_norm": 0.045794982463121414,
      "learning_rate": 0.0002481255016417366,
      "loss": 3.7628,
      "step": 11850
    },
    {
      "epoch": 0.8683437619716511,
      "grad_norm": 0.036884695291519165,
      "learning_rate": 0.0002479066034294053,
      "loss": 3.7131,
      "step": 11900
    },
    {
      "epoch": 0.8719922651732127,
      "grad_norm": 0.05138828232884407,
      "learning_rate": 0.00024768770521707404,
      "loss": 3.7466,
      "step": 11950
    },
    {
      "epoch": 0.8756407683747742,
      "grad_norm": 0.11742337793111801,
      "learning_rate": 0.0002474688070047428,
      "loss": 3.7503,
      "step": 12000
    },
    {
      "epoch": 0.8792892715763359,
      "grad_norm": 0.03942801430821419,
      "learning_rate": 0.0002472499087924115,
      "loss": 3.7665,
      "step": 12050
    },
    {
      "epoch": 0.8829377747778974,
      "grad_norm": 0.16825155913829803,
      "learning_rate": 0.00024703101058008026,
      "loss": 3.7422,
      "step": 12100
    },
    {
      "epoch": 0.8865862779794589,
      "grad_norm": 0.03790099173784256,
      "learning_rate": 0.00024681211236774896,
      "loss": 3.7001,
      "step": 12150
    },
    {
      "epoch": 0.8902347811810205,
      "grad_norm": 0.048273518681526184,
      "learning_rate": 0.0002465932141554177,
      "loss": 3.8242,
      "step": 12200
    },
    {
      "epoch": 0.893883284382582,
      "grad_norm": 0.05202070251107216,
      "learning_rate": 0.00024637431594308647,
      "loss": 3.7517,
      "step": 12250
    },
    {
      "epoch": 0.8975317875841436,
      "grad_norm": 0.0339135117828846,
      "learning_rate": 0.00024615541773075517,
      "loss": 3.7364,
      "step": 12300
    },
    {
      "epoch": 0.9011802907857052,
      "grad_norm": 0.05662667378783226,
      "learning_rate": 0.0002459365195184239,
      "loss": 3.7294,
      "step": 12350
    },
    {
      "epoch": 0.9048287939872667,
      "grad_norm": 0.04340604320168495,
      "learning_rate": 0.0002457176213060926,
      "loss": 3.7599,
      "step": 12400
    },
    {
      "epoch": 0.9084772971888283,
      "grad_norm": 0.03602934628725052,
      "learning_rate": 0.0002454987230937614,
      "loss": 3.7934,
      "step": 12450
    },
    {
      "epoch": 0.9121258003903898,
      "grad_norm": 0.051781877875328064,
      "learning_rate": 0.0002452798248814301,
      "loss": 3.757,
      "step": 12500
    },
    {
      "epoch": 0.9157743035919514,
      "grad_norm": 0.05975133180618286,
      "learning_rate": 0.00024506092666909884,
      "loss": 3.8141,
      "step": 12550
    },
    {
      "epoch": 0.919422806793513,
      "grad_norm": 0.02861228585243225,
      "learning_rate": 0.0002448420284567676,
      "loss": 3.7278,
      "step": 12600
    },
    {
      "epoch": 0.9230713099950745,
      "grad_norm": 0.03803494572639465,
      "learning_rate": 0.0002446231302444363,
      "loss": 3.7608,
      "step": 12650
    },
    {
      "epoch": 0.9267198131966361,
      "grad_norm": 0.040464699268341064,
      "learning_rate": 0.00024440423203210505,
      "loss": 3.7474,
      "step": 12700
    },
    {
      "epoch": 0.9303683163981976,
      "grad_norm": 0.035990022122859955,
      "learning_rate": 0.0002441853338197738,
      "loss": 3.7693,
      "step": 12750
    },
    {
      "epoch": 0.9340168195997592,
      "grad_norm": 0.12842823565006256,
      "learning_rate": 0.0002439664356074425,
      "loss": 3.7205,
      "step": 12800
    },
    {
      "epoch": 0.9376653228013208,
      "grad_norm": 0.04690799117088318,
      "learning_rate": 0.00024374753739511124,
      "loss": 3.7619,
      "step": 12850
    },
    {
      "epoch": 0.9413138260028823,
      "grad_norm": 0.03609974682331085,
      "learning_rate": 0.00024352863918278,
      "loss": 3.8023,
      "step": 12900
    },
    {
      "epoch": 0.9449623292044439,
      "grad_norm": 0.048785604536533356,
      "learning_rate": 0.00024330974097044872,
      "loss": 3.6927,
      "step": 12950
    },
    {
      "epoch": 0.9486108324060054,
      "grad_norm": 0.033076245337724686,
      "learning_rate": 0.00024309084275811745,
      "loss": 3.7412,
      "step": 13000
    },
    {
      "epoch": 0.952259335607567,
      "grad_norm": 0.028776904568076134,
      "learning_rate": 0.0002428719445457862,
      "loss": 3.7878,
      "step": 13050
    },
    {
      "epoch": 0.9559078388091286,
      "grad_norm": 0.02954328991472721,
      "learning_rate": 0.00024265304633345493,
      "loss": 3.7784,
      "step": 13100
    },
    {
      "epoch": 0.9595563420106901,
      "grad_norm": 0.08677668124437332,
      "learning_rate": 0.00024243414812112366,
      "loss": 3.7609,
      "step": 13150
    },
    {
      "epoch": 0.9632048452122517,
      "grad_norm": 0.03310631215572357,
      "learning_rate": 0.0002422152499087924,
      "loss": 3.7892,
      "step": 13200
    },
    {
      "epoch": 0.9668533484138132,
      "grad_norm": 0.03703955188393593,
      "learning_rate": 0.00024199635169646114,
      "loss": 3.7195,
      "step": 13250
    },
    {
      "epoch": 0.9705018516153748,
      "grad_norm": 0.0360308475792408,
      "learning_rate": 0.00024177745348412984,
      "loss": 3.7718,
      "step": 13300
    },
    {
      "epoch": 0.9741503548169363,
      "grad_norm": 0.041493192315101624,
      "learning_rate": 0.00024155855527179857,
      "loss": 3.6603,
      "step": 13350
    },
    {
      "epoch": 0.9777988580184979,
      "grad_norm": 0.07808513939380646,
      "learning_rate": 0.00024133965705946733,
      "loss": 3.6857,
      "step": 13400
    },
    {
      "epoch": 0.9814473612200595,
      "grad_norm": 0.13988623023033142,
      "learning_rate": 0.00024112075884713606,
      "loss": 3.7568,
      "step": 13450
    },
    {
      "epoch": 0.985095864421621,
      "grad_norm": 0.040506236255168915,
      "learning_rate": 0.00024090186063480479,
      "loss": 3.7456,
      "step": 13500
    },
    {
      "epoch": 0.9887443676231826,
      "grad_norm": 0.04095887765288353,
      "learning_rate": 0.00024068296242247351,
      "loss": 3.7838,
      "step": 13550
    },
    {
      "epoch": 0.9923928708247441,
      "grad_norm": 0.02780856378376484,
      "learning_rate": 0.00024046406421014227,
      "loss": 3.727,
      "step": 13600
    },
    {
      "epoch": 0.9960413740263057,
      "grad_norm": 0.03426589444279671,
      "learning_rate": 0.000240245165997811,
      "loss": 3.7816,
      "step": 13650
    },
    {
      "epoch": 0.9996898772278673,
      "grad_norm": 0.04505148530006409,
      "learning_rate": 0.00024002626778547973,
      "loss": 3.7641,
      "step": 13700
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.6197409629821777,
      "eval_runtime": 3.4942,
      "eval_samples_per_second": 28.619,
      "eval_steps_per_second": 3.72,
      "step": 13705
    },
    {
      "epoch": 1.0032836528814053,
      "grad_norm": 0.04090108722448349,
      "learning_rate": 0.00023980736957314848,
      "loss": 3.7404,
      "step": 13750
    },
    {
      "epoch": 1.006932156082967,
      "grad_norm": 0.04469461739063263,
      "learning_rate": 0.0002395884713608172,
      "loss": 3.7613,
      "step": 13800
    },
    {
      "epoch": 1.0105806592845286,
      "grad_norm": 0.19506309926509857,
      "learning_rate": 0.00023936957314848594,
      "loss": 3.7324,
      "step": 13850
    },
    {
      "epoch": 1.0142291624860902,
      "grad_norm": 0.04022873938083649,
      "learning_rate": 0.00023915067493615464,
      "loss": 3.7193,
      "step": 13900
    },
    {
      "epoch": 1.0178776656876516,
      "grad_norm": 0.04357985779643059,
      "learning_rate": 0.00023893177672382342,
      "loss": 3.6963,
      "step": 13950
    },
    {
      "epoch": 1.0215261688892132,
      "grad_norm": 0.03487132489681244,
      "learning_rate": 0.00023871287851149212,
      "loss": 3.7934,
      "step": 14000
    },
    {
      "epoch": 1.0251746720907748,
      "grad_norm": 0.036217644810676575,
      "learning_rate": 0.00023849398029916085,
      "loss": 3.7241,
      "step": 14050
    },
    {
      "epoch": 1.0288231752923362,
      "grad_norm": 0.4628518521785736,
      "learning_rate": 0.0002382750820868296,
      "loss": 3.8456,
      "step": 14100
    },
    {
      "epoch": 1.0324716784938979,
      "grad_norm": 0.05662817135453224,
      "learning_rate": 0.00023805618387449834,
      "loss": 3.7859,
      "step": 14150
    },
    {
      "epoch": 1.0361201816954595,
      "grad_norm": 0.054693058133125305,
      "learning_rate": 0.00023783728566216706,
      "loss": 3.7488,
      "step": 14200
    },
    {
      "epoch": 1.039768684897021,
      "grad_norm": 0.0636269822716713,
      "learning_rate": 0.0002376183874498358,
      "loss": 3.7195,
      "step": 14250
    },
    {
      "epoch": 1.0434171880985825,
      "grad_norm": 0.16742122173309326,
      "learning_rate": 0.00023739948923750455,
      "loss": 3.7932,
      "step": 14300
    },
    {
      "epoch": 1.0470656913001442,
      "grad_norm": 0.027793755754828453,
      "learning_rate": 0.00023718059102517328,
      "loss": 3.7855,
      "step": 14350
    },
    {
      "epoch": 1.0507141945017058,
      "grad_norm": 0.03833455964922905,
      "learning_rate": 0.000236961692812842,
      "loss": 3.7791,
      "step": 14400
    },
    {
      "epoch": 1.0543626977032672,
      "grad_norm": 0.028694558888673782,
      "learning_rate": 0.00023674279460051076,
      "loss": 3.7536,
      "step": 14450
    },
    {
      "epoch": 1.0580112009048288,
      "grad_norm": 0.05112793669104576,
      "learning_rate": 0.0002365238963881795,
      "loss": 3.7776,
      "step": 14500
    },
    {
      "epoch": 1.0616597041063904,
      "grad_norm": 0.03966891020536423,
      "learning_rate": 0.00023630499817584822,
      "loss": 3.7033,
      "step": 14550
    },
    {
      "epoch": 1.0653082073079518,
      "grad_norm": 0.034847281873226166,
      "learning_rate": 0.00023608609996351692,
      "loss": 3.7573,
      "step": 14600
    },
    {
      "epoch": 1.0689567105095135,
      "grad_norm": 0.0321110337972641,
      "learning_rate": 0.00023586720175118567,
      "loss": 3.6903,
      "step": 14650
    },
    {
      "epoch": 1.072605213711075,
      "grad_norm": 0.03850401192903519,
      "learning_rate": 0.0002356483035388544,
      "loss": 3.7537,
      "step": 14700
    },
    {
      "epoch": 1.0762537169126365,
      "grad_norm": 0.05369709059596062,
      "learning_rate": 0.00023542940532652313,
      "loss": 3.7825,
      "step": 14750
    },
    {
      "epoch": 1.0799022201141981,
      "grad_norm": 0.18880654871463776,
      "learning_rate": 0.00023521050711419189,
      "loss": 3.6863,
      "step": 14800
    },
    {
      "epoch": 1.0835507233157597,
      "grad_norm": 0.04152613505721092,
      "learning_rate": 0.00023499160890186061,
      "loss": 3.7209,
      "step": 14850
    },
    {
      "epoch": 1.0871992265173214,
      "grad_norm": 0.03712688386440277,
      "learning_rate": 0.00023477271068952934,
      "loss": 3.7094,
      "step": 14900
    },
    {
      "epoch": 1.0908477297188828,
      "grad_norm": 0.05150223150849342,
      "learning_rate": 0.00023455381247719807,
      "loss": 3.7697,
      "step": 14950
    },
    {
      "epoch": 1.0944962329204444,
      "grad_norm": 0.05034307762980461,
      "learning_rate": 0.00023433491426486683,
      "loss": 3.7455,
      "step": 15000
    },
    {
      "epoch": 1.098144736122006,
      "grad_norm": 0.048461925238370895,
      "learning_rate": 0.00023411601605253555,
      "loss": 3.7075,
      "step": 15050
    },
    {
      "epoch": 1.1017932393235674,
      "grad_norm": 0.12226790934801102,
      "learning_rate": 0.00023389711784020428,
      "loss": 3.7211,
      "step": 15100
    },
    {
      "epoch": 1.105441742525129,
      "grad_norm": 0.050078921020030975,
      "learning_rate": 0.00023367821962787304,
      "loss": 3.7438,
      "step": 15150
    },
    {
      "epoch": 1.1090902457266907,
      "grad_norm": 0.03300346061587334,
      "learning_rate": 0.00023345932141554177,
      "loss": 3.7432,
      "step": 15200
    },
    {
      "epoch": 1.112738748928252,
      "grad_norm": 0.0669669657945633,
      "learning_rate": 0.00023324042320321047,
      "loss": 3.7875,
      "step": 15250
    },
    {
      "epoch": 1.1163872521298137,
      "grad_norm": 0.037628013640642166,
      "learning_rate": 0.0002330215249908792,
      "loss": 3.7482,
      "step": 15300
    },
    {
      "epoch": 1.1200357553313753,
      "grad_norm": 0.04143832251429558,
      "learning_rate": 0.00023280262677854795,
      "loss": 3.7686,
      "step": 15350
    },
    {
      "epoch": 1.123684258532937,
      "grad_norm": 0.03838564082980156,
      "learning_rate": 0.00023258372856621668,
      "loss": 3.759,
      "step": 15400
    },
    {
      "epoch": 1.1273327617344984,
      "grad_norm": 1.517522931098938,
      "learning_rate": 0.0002323648303538854,
      "loss": 3.7963,
      "step": 15450
    },
    {
      "epoch": 1.13098126493606,
      "grad_norm": 0.060505349189043045,
      "learning_rate": 0.00023214593214155416,
      "loss": 3.7151,
      "step": 15500
    },
    {
      "epoch": 1.1346297681376216,
      "grad_norm": 0.04080701619386673,
      "learning_rate": 0.0002319270339292229,
      "loss": 3.7585,
      "step": 15550
    },
    {
      "epoch": 1.138278271339183,
      "grad_norm": 0.05273197218775749,
      "learning_rate": 0.00023170813571689162,
      "loss": 3.8283,
      "step": 15600
    },
    {
      "epoch": 1.1419267745407446,
      "grad_norm": 0.04702245444059372,
      "learning_rate": 0.00023148923750456035,
      "loss": 3.7583,
      "step": 15650
    },
    {
      "epoch": 1.1455752777423063,
      "grad_norm": 0.035292260348796844,
      "learning_rate": 0.0002312703392922291,
      "loss": 3.7589,
      "step": 15700
    },
    {
      "epoch": 1.149223780943868,
      "grad_norm": 0.15973253548145294,
      "learning_rate": 0.00023105144107989783,
      "loss": 3.7187,
      "step": 15750
    },
    {
      "epoch": 1.1528722841454293,
      "grad_norm": 0.08204863965511322,
      "learning_rate": 0.00023083254286756656,
      "loss": 3.7326,
      "step": 15800
    },
    {
      "epoch": 1.156520787346991,
      "grad_norm": 0.048146408051252365,
      "learning_rate": 0.00023061364465523532,
      "loss": 3.7682,
      "step": 15850
    },
    {
      "epoch": 1.1601692905485526,
      "grad_norm": 0.04632381349802017,
      "learning_rate": 0.00023039474644290405,
      "loss": 3.763,
      "step": 15900
    },
    {
      "epoch": 1.163817793750114,
      "grad_norm": 0.04307112470269203,
      "learning_rate": 0.00023017584823057275,
      "loss": 3.7247,
      "step": 15950
    },
    {
      "epoch": 1.1674662969516756,
      "grad_norm": 0.04215457662940025,
      "learning_rate": 0.00022995695001824148,
      "loss": 3.7508,
      "step": 16000
    },
    {
      "epoch": 1.1711148001532372,
      "grad_norm": 0.08878426998853683,
      "learning_rate": 0.00022973805180591023,
      "loss": 3.7979,
      "step": 16050
    },
    {
      "epoch": 1.1747633033547986,
      "grad_norm": 0.033042531460523605,
      "learning_rate": 0.00022951915359357896,
      "loss": 3.7261,
      "step": 16100
    },
    {
      "epoch": 1.1784118065563602,
      "grad_norm": 0.043988894671201706,
      "learning_rate": 0.0002293002553812477,
      "loss": 3.7267,
      "step": 16150
    },
    {
      "epoch": 1.1820603097579219,
      "grad_norm": 0.03956916928291321,
      "learning_rate": 0.00022908135716891644,
      "loss": 3.7577,
      "step": 16200
    },
    {
      "epoch": 1.1857088129594833,
      "grad_norm": 0.05795186385512352,
      "learning_rate": 0.00022886245895658517,
      "loss": 3.7828,
      "step": 16250
    },
    {
      "epoch": 1.189357316161045,
      "grad_norm": 0.04314567148685455,
      "learning_rate": 0.0002286435607442539,
      "loss": 3.7809,
      "step": 16300
    },
    {
      "epoch": 1.1930058193626065,
      "grad_norm": 0.04510662332177162,
      "learning_rate": 0.00022842466253192263,
      "loss": 3.74,
      "step": 16350
    },
    {
      "epoch": 1.196654322564168,
      "grad_norm": 0.03833683952689171,
      "learning_rate": 0.00022820576431959138,
      "loss": 3.7809,
      "step": 16400
    },
    {
      "epoch": 1.2003028257657296,
      "grad_norm": 0.21374206244945526,
      "learning_rate": 0.0002279868661072601,
      "loss": 3.7461,
      "step": 16450
    },
    {
      "epoch": 1.2039513289672912,
      "grad_norm": 0.03803669661283493,
      "learning_rate": 0.00022776796789492884,
      "loss": 3.7314,
      "step": 16500
    },
    {
      "epoch": 1.2075998321688528,
      "grad_norm": 0.041917238384485245,
      "learning_rate": 0.0002275490696825976,
      "loss": 3.6744,
      "step": 16550
    },
    {
      "epoch": 1.2112483353704142,
      "grad_norm": 0.03598570451140404,
      "learning_rate": 0.0002273301714702663,
      "loss": 3.7176,
      "step": 16600
    },
    {
      "epoch": 1.2148968385719758,
      "grad_norm": 0.03387374430894852,
      "learning_rate": 0.00022711127325793503,
      "loss": 3.7066,
      "step": 16650
    },
    {
      "epoch": 1.2185453417735375,
      "grad_norm": 0.08991953730583191,
      "learning_rate": 0.00022689237504560375,
      "loss": 3.6998,
      "step": 16700
    },
    {
      "epoch": 1.222193844975099,
      "grad_norm": 0.026019835844635963,
      "learning_rate": 0.0002266734768332725,
      "loss": 3.7563,
      "step": 16750
    },
    {
      "epoch": 1.2258423481766605,
      "grad_norm": 0.03544925898313522,
      "learning_rate": 0.00022645457862094124,
      "loss": 3.7542,
      "step": 16800
    },
    {
      "epoch": 1.2294908513782221,
      "grad_norm": 0.047079335898160934,
      "learning_rate": 0.00022623568040860997,
      "loss": 3.7759,
      "step": 16850
    },
    {
      "epoch": 1.2331393545797837,
      "grad_norm": 0.03889049217104912,
      "learning_rate": 0.00022601678219627872,
      "loss": 3.719,
      "step": 16900
    },
    {
      "epoch": 1.2367878577813451,
      "grad_norm": 0.06887563318014145,
      "learning_rate": 0.00022579788398394745,
      "loss": 3.8103,
      "step": 16950
    },
    {
      "epoch": 1.2404363609829068,
      "grad_norm": 0.04053463041782379,
      "learning_rate": 0.00022557898577161618,
      "loss": 3.7593,
      "step": 17000
    },
    {
      "epoch": 1.2440848641844684,
      "grad_norm": 0.051020774990320206,
      "learning_rate": 0.0002253600875592849,
      "loss": 3.7464,
      "step": 17050
    },
    {
      "epoch": 1.2477333673860298,
      "grad_norm": 0.03496795520186424,
      "learning_rate": 0.00022514118934695366,
      "loss": 3.6955,
      "step": 17100
    },
    {
      "epoch": 1.2513818705875914,
      "grad_norm": 0.10475992411375046,
      "learning_rate": 0.0002249222911346224,
      "loss": 3.7083,
      "step": 17150
    },
    {
      "epoch": 1.255030373789153,
      "grad_norm": 0.03078918531537056,
      "learning_rate": 0.0002247033929222911,
      "loss": 3.7339,
      "step": 17200
    },
    {
      "epoch": 1.2586788769907145,
      "grad_norm": 0.03111954964697361,
      "learning_rate": 0.00022448449470995987,
      "loss": 3.7282,
      "step": 17250
    },
    {
      "epoch": 1.262327380192276,
      "grad_norm": 0.034766826778650284,
      "learning_rate": 0.00022426559649762858,
      "loss": 3.724,
      "step": 17300
    },
    {
      "epoch": 1.2659758833938377,
      "grad_norm": 0.06815806031227112,
      "learning_rate": 0.0002240466982852973,
      "loss": 3.7303,
      "step": 17350
    },
    {
      "epoch": 1.2696243865953991,
      "grad_norm": 0.049190081655979156,
      "learning_rate": 0.00022382780007296603,
      "loss": 3.7126,
      "step": 17400
    },
    {
      "epoch": 1.2732728897969607,
      "grad_norm": 0.04145616292953491,
      "learning_rate": 0.0002236089018606348,
      "loss": 3.763,
      "step": 17450
    },
    {
      "epoch": 1.2769213929985224,
      "grad_norm": 0.05226116627454758,
      "learning_rate": 0.00022339000364830352,
      "loss": 3.7352,
      "step": 17500
    },
    {
      "epoch": 1.280569896200084,
      "grad_norm": 0.043208252638578415,
      "learning_rate": 0.00022317110543597224,
      "loss": 3.695,
      "step": 17550
    },
    {
      "epoch": 1.2842183994016454,
      "grad_norm": 0.04710938781499863,
      "learning_rate": 0.000222952207223641,
      "loss": 3.7899,
      "step": 17600
    },
    {
      "epoch": 1.287866902603207,
      "grad_norm": 0.04672008380293846,
      "learning_rate": 0.00022273330901130973,
      "loss": 3.7437,
      "step": 17650
    },
    {
      "epoch": 1.2915154058047686,
      "grad_norm": 0.06530438363552094,
      "learning_rate": 0.00022251441079897846,
      "loss": 3.7217,
      "step": 17700
    },
    {
      "epoch": 1.2951639090063303,
      "grad_norm": 0.0764741525053978,
      "learning_rate": 0.00022229551258664719,
      "loss": 3.6842,
      "step": 17750
    },
    {
      "epoch": 1.2988124122078917,
      "grad_norm": 0.038985252380371094,
      "learning_rate": 0.00022207661437431594,
      "loss": 3.7193,
      "step": 17800
    },
    {
      "epoch": 1.3024609154094533,
      "grad_norm": 0.05751195177435875,
      "learning_rate": 0.00022185771616198467,
      "loss": 3.6906,
      "step": 17850
    },
    {
      "epoch": 1.306109418611015,
      "grad_norm": 0.059702035039663315,
      "learning_rate": 0.00022163881794965337,
      "loss": 3.748,
      "step": 17900
    },
    {
      "epoch": 1.3097579218125763,
      "grad_norm": 0.03381015360355377,
      "learning_rate": 0.00022141991973732213,
      "loss": 3.7611,
      "step": 17950
    },
    {
      "epoch": 1.313406425014138,
      "grad_norm": 0.04454955458641052,
      "learning_rate": 0.00022120102152499085,
      "loss": 3.7557,
      "step": 18000
    },
    {
      "epoch": 1.3170549282156996,
      "grad_norm": 0.03766092658042908,
      "learning_rate": 0.00022098212331265958,
      "loss": 3.6921,
      "step": 18050
    },
    {
      "epoch": 1.320703431417261,
      "grad_norm": 0.030874619260430336,
      "learning_rate": 0.0002207632251003283,
      "loss": 3.7928,
      "step": 18100
    },
    {
      "epoch": 1.3243519346188226,
      "grad_norm": 0.034130651503801346,
      "learning_rate": 0.00022054432688799707,
      "loss": 3.722,
      "step": 18150
    },
    {
      "epoch": 1.3280004378203842,
      "grad_norm": 0.056567978113889694,
      "learning_rate": 0.0002203254286756658,
      "loss": 3.6631,
      "step": 18200
    },
    {
      "epoch": 1.3316489410219456,
      "grad_norm": 0.06647706776857376,
      "learning_rate": 0.00022010653046333452,
      "loss": 3.702,
      "step": 18250
    },
    {
      "epoch": 1.3352974442235073,
      "grad_norm": 0.065660260617733,
      "learning_rate": 0.00021988763225100328,
      "loss": 3.752,
      "step": 18300
    },
    {
      "epoch": 1.338945947425069,
      "grad_norm": 0.029660243541002274,
      "learning_rate": 0.000219668734038672,
      "loss": 3.662,
      "step": 18350
    },
    {
      "epoch": 1.3425944506266303,
      "grad_norm": 0.06708890944719315,
      "learning_rate": 0.00021944983582634074,
      "loss": 3.7586,
      "step": 18400
    },
    {
      "epoch": 1.346242953828192,
      "grad_norm": 0.05066681653261185,
      "learning_rate": 0.00021923093761400944,
      "loss": 3.7515,
      "step": 18450
    },
    {
      "epoch": 1.3498914570297535,
      "grad_norm": 0.07868752628564835,
      "learning_rate": 0.00021901203940167822,
      "loss": 3.7385,
      "step": 18500
    },
    {
      "epoch": 1.3535399602313152,
      "grad_norm": 0.1273961216211319,
      "learning_rate": 0.00021879314118934692,
      "loss": 3.713,
      "step": 18550
    },
    {
      "epoch": 1.3571884634328766,
      "grad_norm": 0.172705739736557,
      "learning_rate": 0.00021857424297701565,
      "loss": 3.7561,
      "step": 18600
    },
    {
      "epoch": 1.3608369666344382,
      "grad_norm": 0.10469924658536911,
      "learning_rate": 0.0002183553447646844,
      "loss": 3.7456,
      "step": 18650
    },
    {
      "epoch": 1.3644854698359998,
      "grad_norm": 0.3939546048641205,
      "learning_rate": 0.00021813644655235313,
      "loss": 3.7991,
      "step": 18700
    },
    {
      "epoch": 1.3681339730375615,
      "grad_norm": 0.21344639360904694,
      "learning_rate": 0.00021791754834002186,
      "loss": 3.7533,
      "step": 18750
    },
    {
      "epoch": 1.3717824762391229,
      "grad_norm": 0.03942576050758362,
      "learning_rate": 0.00021769865012769062,
      "loss": 3.7735,
      "step": 18800
    },
    {
      "epoch": 1.3754309794406845,
      "grad_norm": 0.1853078156709671,
      "learning_rate": 0.00021747975191535934,
      "loss": 3.7736,
      "step": 18850
    },
    {
      "epoch": 1.3790794826422461,
      "grad_norm": 0.04366566613316536,
      "learning_rate": 0.00021726085370302807,
      "loss": 3.772,
      "step": 18900
    },
    {
      "epoch": 1.3827279858438075,
      "grad_norm": 0.03695155307650566,
      "learning_rate": 0.0002170419554906968,
      "loss": 3.7625,
      "step": 18950
    },
    {
      "epoch": 1.3863764890453691,
      "grad_norm": 0.0532066747546196,
      "learning_rate": 0.00021682305727836556,
      "loss": 3.7601,
      "step": 19000
    },
    {
      "epoch": 1.3900249922469308,
      "grad_norm": 0.042456869035959244,
      "learning_rate": 0.00021660415906603429,
      "loss": 3.7164,
      "step": 19050
    },
    {
      "epoch": 1.3936734954484922,
      "grad_norm": 0.051930904388427734,
      "learning_rate": 0.00021638526085370301,
      "loss": 3.7482,
      "step": 19100
    },
    {
      "epoch": 1.3973219986500538,
      "grad_norm": 0.09603128582239151,
      "learning_rate": 0.00021616636264137177,
      "loss": 3.7329,
      "step": 19150
    },
    {
      "epoch": 1.4009705018516154,
      "grad_norm": 0.028102722018957138,
      "learning_rate": 0.0002159474644290405,
      "loss": 3.6888,
      "step": 19200
    },
    {
      "epoch": 1.4046190050531768,
      "grad_norm": 0.037440817803144455,
      "learning_rate": 0.0002157285662167092,
      "loss": 3.728,
      "step": 19250
    },
    {
      "epoch": 1.4082675082547385,
      "grad_norm": 0.060961104929447174,
      "learning_rate": 0.00021550966800437793,
      "loss": 3.7263,
      "step": 19300
    },
    {
      "epoch": 1.4119160114563,
      "grad_norm": 0.1901373714208603,
      "learning_rate": 0.00021529076979204668,
      "loss": 3.7841,
      "step": 19350
    },
    {
      "epoch": 1.4155645146578615,
      "grad_norm": 0.17188921570777893,
      "learning_rate": 0.0002150718715797154,
      "loss": 3.7217,
      "step": 19400
    },
    {
      "epoch": 1.419213017859423,
      "grad_norm": 0.06910090148448944,
      "learning_rate": 0.00021485297336738414,
      "loss": 3.7502,
      "step": 19450
    },
    {
      "epoch": 1.4228615210609847,
      "grad_norm": 0.3995209038257599,
      "learning_rate": 0.0002146340751550529,
      "loss": 3.737,
      "step": 19500
    },
    {
      "epoch": 1.4265100242625464,
      "grad_norm": 0.048238687217235565,
      "learning_rate": 0.00021441517694272162,
      "loss": 3.6937,
      "step": 19550
    },
    {
      "epoch": 1.430158527464108,
      "grad_norm": 0.11558366566896439,
      "learning_rate": 0.00021419627873039035,
      "loss": 3.7424,
      "step": 19600
    },
    {
      "epoch": 1.4338070306656694,
      "grad_norm": 0.03168173134326935,
      "learning_rate": 0.00021397738051805908,
      "loss": 3.6756,
      "step": 19650
    },
    {
      "epoch": 1.437455533867231,
      "grad_norm": 0.07648158073425293,
      "learning_rate": 0.00021375848230572784,
      "loss": 3.7567,
      "step": 19700
    },
    {
      "epoch": 1.4411040370687926,
      "grad_norm": 0.47223401069641113,
      "learning_rate": 0.00021353958409339656,
      "loss": 3.7227,
      "step": 19750
    },
    {
      "epoch": 1.444752540270354,
      "grad_norm": 0.044387441128492355,
      "learning_rate": 0.00021332068588106527,
      "loss": 3.7422,
      "step": 19800
    },
    {
      "epoch": 1.4484010434719157,
      "grad_norm": 0.0492052361369133,
      "learning_rate": 0.00021310178766873405,
      "loss": 3.7378,
      "step": 19850
    },
    {
      "epoch": 1.4520495466734773,
      "grad_norm": 0.029866209253668785,
      "learning_rate": 0.00021288288945640275,
      "loss": 3.7438,
      "step": 19900
    },
    {
      "epoch": 1.4556980498750387,
      "grad_norm": 0.09800336509943008,
      "learning_rate": 0.00021266399124407148,
      "loss": 3.7424,
      "step": 19950
    },
    {
      "epoch": 1.4593465530766003,
      "grad_norm": 0.041090868413448334,
      "learning_rate": 0.0002124450930317402,
      "loss": 3.6977,
      "step": 20000
    },
    {
      "epoch": 1.462995056278162,
      "grad_norm": 0.0812729001045227,
      "learning_rate": 0.00021222619481940896,
      "loss": 3.7003,
      "step": 20050
    },
    {
      "epoch": 1.4666435594797234,
      "grad_norm": 0.0362551286816597,
      "learning_rate": 0.0002120072966070777,
      "loss": 3.695,
      "step": 20100
    },
    {
      "epoch": 1.470292062681285,
      "grad_norm": 0.045014895498752594,
      "learning_rate": 0.00021178839839474642,
      "loss": 3.7217,
      "step": 20150
    },
    {
      "epoch": 1.4739405658828466,
      "grad_norm": 0.03407137095928192,
      "learning_rate": 0.00021156950018241517,
      "loss": 3.6941,
      "step": 20200
    },
    {
      "epoch": 1.477589069084408,
      "grad_norm": 0.044785305857658386,
      "learning_rate": 0.0002113506019700839,
      "loss": 3.7355,
      "step": 20250
    },
    {
      "epoch": 1.4812375722859696,
      "grad_norm": 0.04852747544646263,
      "learning_rate": 0.00021113170375775263,
      "loss": 3.7285,
      "step": 20300
    },
    {
      "epoch": 1.4848860754875313,
      "grad_norm": 0.04478832334280014,
      "learning_rate": 0.00021091280554542136,
      "loss": 3.7396,
      "step": 20350
    },
    {
      "epoch": 1.4885345786890927,
      "grad_norm": 0.05452612414956093,
      "learning_rate": 0.00021069390733309011,
      "loss": 3.6699,
      "step": 20400
    },
    {
      "epoch": 1.4921830818906543,
      "grad_norm": 0.34000536799430847,
      "learning_rate": 0.00021047500912075884,
      "loss": 3.7358,
      "step": 20450
    },
    {
      "epoch": 1.495831585092216,
      "grad_norm": 0.028732100501656532,
      "learning_rate": 0.00021025611090842754,
      "loss": 3.7201,
      "step": 20500
    },
    {
      "epoch": 1.4994800882937775,
      "grad_norm": 0.03826945647597313,
      "learning_rate": 0.00021003721269609633,
      "loss": 3.7455,
      "step": 20550
    },
    {
      "epoch": 1.5031285914953392,
      "grad_norm": 0.09516750276088715,
      "learning_rate": 0.00020981831448376503,
      "loss": 3.7119,
      "step": 20600
    },
    {
      "epoch": 1.5067770946969006,
      "grad_norm": 0.03188200294971466,
      "learning_rate": 0.00020959941627143376,
      "loss": 3.6845,
      "step": 20650
    },
    {
      "epoch": 1.5104255978984622,
      "grad_norm": 0.3965957462787628,
      "learning_rate": 0.00020938051805910248,
      "loss": 3.7461,
      "step": 20700
    },
    {
      "epoch": 1.5140741011000238,
      "grad_norm": 0.03341841697692871,
      "learning_rate": 0.00020916161984677124,
      "loss": 3.7246,
      "step": 20750
    },
    {
      "epoch": 1.5177226043015852,
      "grad_norm": 0.03954887017607689,
      "learning_rate": 0.00020894272163443997,
      "loss": 3.7148,
      "step": 20800
    },
    {
      "epoch": 1.5213711075031469,
      "grad_norm": 0.04117938503623009,
      "learning_rate": 0.0002087238234221087,
      "loss": 3.694,
      "step": 20850
    },
    {
      "epoch": 1.5250196107047085,
      "grad_norm": 0.031011167913675308,
      "learning_rate": 0.00020850492520977745,
      "loss": 3.7734,
      "step": 20900
    },
    {
      "epoch": 1.5286681139062699,
      "grad_norm": 0.03980780765414238,
      "learning_rate": 0.00020828602699744618,
      "loss": 3.6828,
      "step": 20950
    },
    {
      "epoch": 1.5323166171078315,
      "grad_norm": 0.08052249252796173,
      "learning_rate": 0.0002080671287851149,
      "loss": 3.7032,
      "step": 21000
    },
    {
      "epoch": 1.5359651203093931,
      "grad_norm": 0.04264044761657715,
      "learning_rate": 0.00020784823057278364,
      "loss": 3.7709,
      "step": 21050
    },
    {
      "epoch": 1.5396136235109545,
      "grad_norm": 0.07172401994466782,
      "learning_rate": 0.0002076293323604524,
      "loss": 3.7435,
      "step": 21100
    },
    {
      "epoch": 1.5432621267125162,
      "grad_norm": 0.05430204048752785,
      "learning_rate": 0.00020741043414812112,
      "loss": 3.7231,
      "step": 21150
    },
    {
      "epoch": 1.5469106299140778,
      "grad_norm": 0.025894206017255783,
      "learning_rate": 0.00020719153593578982,
      "loss": 3.7655,
      "step": 21200
    },
    {
      "epoch": 1.5505591331156392,
      "grad_norm": 0.05294632166624069,
      "learning_rate": 0.00020697263772345858,
      "loss": 3.7312,
      "step": 21250
    },
    {
      "epoch": 1.5542076363172008,
      "grad_norm": 0.0543757788836956,
      "learning_rate": 0.0002067537395111273,
      "loss": 3.7751,
      "step": 21300
    },
    {
      "epoch": 1.5578561395187625,
      "grad_norm": 0.19300492107868195,
      "learning_rate": 0.00020653484129879603,
      "loss": 3.6977,
      "step": 21350
    },
    {
      "epoch": 1.5615046427203239,
      "grad_norm": 0.02627233788371086,
      "learning_rate": 0.00020631594308646476,
      "loss": 3.7459,
      "step": 21400
    },
    {
      "epoch": 1.5651531459218857,
      "grad_norm": 0.038918063044548035,
      "learning_rate": 0.00020609704487413352,
      "loss": 3.6792,
      "step": 21450
    },
    {
      "epoch": 1.568801649123447,
      "grad_norm": 0.3862779140472412,
      "learning_rate": 0.00020587814666180225,
      "loss": 3.7494,
      "step": 21500
    },
    {
      "epoch": 1.5724501523250085,
      "grad_norm": 0.042077939957380295,
      "learning_rate": 0.00020565924844947097,
      "loss": 3.7232,
      "step": 21550
    },
    {
      "epoch": 1.5760986555265704,
      "grad_norm": 0.03379373624920845,
      "learning_rate": 0.00020544035023713973,
      "loss": 3.751,
      "step": 21600
    },
    {
      "epoch": 1.5797471587281318,
      "grad_norm": 0.09585865586996078,
      "learning_rate": 0.00020522145202480846,
      "loss": 3.7543,
      "step": 21650
    },
    {
      "epoch": 1.5833956619296934,
      "grad_norm": 0.03631490096449852,
      "learning_rate": 0.0002050025538124772,
      "loss": 3.749,
      "step": 21700
    },
    {
      "epoch": 1.587044165131255,
      "grad_norm": 0.04379186034202576,
      "learning_rate": 0.0002047836556001459,
      "loss": 3.6549,
      "step": 21750
    },
    {
      "epoch": 1.5906926683328164,
      "grad_norm": 0.03800224885344505,
      "learning_rate": 0.00020456475738781467,
      "loss": 3.7285,
      "step": 21800
    },
    {
      "epoch": 1.594341171534378,
      "grad_norm": 0.03520674258470535,
      "learning_rate": 0.00020434585917548337,
      "loss": 3.6773,
      "step": 21850
    },
    {
      "epoch": 1.5979896747359397,
      "grad_norm": 0.03876625746488571,
      "learning_rate": 0.0002041269609631521,
      "loss": 3.799,
      "step": 21900
    },
    {
      "epoch": 1.601638177937501,
      "grad_norm": 0.04502211883664131,
      "learning_rate": 0.00020390806275082086,
      "loss": 3.7079,
      "step": 21950
    },
    {
      "epoch": 1.6052866811390627,
      "grad_norm": 0.04586934670805931,
      "learning_rate": 0.00020368916453848958,
      "loss": 3.7554,
      "step": 22000
    },
    {
      "epoch": 1.6089351843406243,
      "grad_norm": 0.04659333825111389,
      "learning_rate": 0.0002034702663261583,
      "loss": 3.7483,
      "step": 22050
    },
    {
      "epoch": 1.6125836875421857,
      "grad_norm": 0.07218611240386963,
      "learning_rate": 0.00020325136811382704,
      "loss": 3.737,
      "step": 22100
    },
    {
      "epoch": 1.6162321907437474,
      "grad_norm": 0.04065568372607231,
      "learning_rate": 0.0002030324699014958,
      "loss": 3.6511,
      "step": 22150
    },
    {
      "epoch": 1.619880693945309,
      "grad_norm": 0.05431501567363739,
      "learning_rate": 0.00020281357168916453,
      "loss": 3.716,
      "step": 22200
    },
    {
      "epoch": 1.6235291971468704,
      "grad_norm": 0.036163125187158585,
      "learning_rate": 0.00020259467347683325,
      "loss": 3.7629,
      "step": 22250
    },
    {
      "epoch": 1.627177700348432,
      "grad_norm": 0.0995253175497055,
      "learning_rate": 0.000202375775264502,
      "loss": 3.7668,
      "step": 22300
    },
    {
      "epoch": 1.6308262035499936,
      "grad_norm": 0.024355482310056686,
      "learning_rate": 0.00020215687705217074,
      "loss": 3.7315,
      "step": 22350
    },
    {
      "epoch": 1.634474706751555,
      "grad_norm": 0.06413973122835159,
      "learning_rate": 0.00020193797883983947,
      "loss": 3.7258,
      "step": 22400
    },
    {
      "epoch": 1.6381232099531169,
      "grad_norm": 0.055735472589731216,
      "learning_rate": 0.00020171908062750817,
      "loss": 3.6792,
      "step": 22450
    },
    {
      "epoch": 1.6417717131546783,
      "grad_norm": 0.10247907787561417,
      "learning_rate": 0.00020150018241517695,
      "loss": 3.7569,
      "step": 22500
    },
    {
      "epoch": 1.6454202163562397,
      "grad_norm": 0.03374054655432701,
      "learning_rate": 0.00020128128420284565,
      "loss": 3.6881,
      "step": 22550
    },
    {
      "epoch": 1.6490687195578015,
      "grad_norm": 0.03926060348749161,
      "learning_rate": 0.00020106238599051438,
      "loss": 3.6941,
      "step": 22600
    },
    {
      "epoch": 1.652717222759363,
      "grad_norm": 0.03623991459608078,
      "learning_rate": 0.00020084348777818313,
      "loss": 3.7633,
      "step": 22650
    },
    {
      "epoch": 1.6563657259609246,
      "grad_norm": 0.041897013783454895,
      "learning_rate": 0.00020062458956585186,
      "loss": 3.7022,
      "step": 22700
    },
    {
      "epoch": 1.6600142291624862,
      "grad_norm": 0.03213215619325638,
      "learning_rate": 0.0002004056913535206,
      "loss": 3.6811,
      "step": 22750
    },
    {
      "epoch": 1.6636627323640476,
      "grad_norm": 0.03069021739065647,
      "learning_rate": 0.00020018679314118932,
      "loss": 3.7115,
      "step": 22800
    },
    {
      "epoch": 1.6673112355656092,
      "grad_norm": 0.028623683378100395,
      "learning_rate": 0.00019996789492885808,
      "loss": 3.7639,
      "step": 22850
    },
    {
      "epoch": 1.6709597387671709,
      "grad_norm": 0.03154635801911354,
      "learning_rate": 0.0001997489967165268,
      "loss": 3.7333,
      "step": 22900
    },
    {
      "epoch": 1.6746082419687323,
      "grad_norm": 0.04658370465040207,
      "learning_rate": 0.00019953009850419553,
      "loss": 3.6698,
      "step": 22950
    },
    {
      "epoch": 1.6782567451702939,
      "grad_norm": 0.0418594628572464,
      "learning_rate": 0.0001993112002918643,
      "loss": 3.7934,
      "step": 23000
    },
    {
      "epoch": 1.6819052483718555,
      "grad_norm": 0.03730812296271324,
      "learning_rate": 0.00019909230207953302,
      "loss": 3.6988,
      "step": 23050
    },
    {
      "epoch": 1.685553751573417,
      "grad_norm": 0.3426954746246338,
      "learning_rate": 0.00019887340386720172,
      "loss": 3.7112,
      "step": 23100
    },
    {
      "epoch": 1.6892022547749785,
      "grad_norm": 0.03945504128932953,
      "learning_rate": 0.00019865450565487045,
      "loss": 3.6631,
      "step": 23150
    },
    {
      "epoch": 1.6928507579765402,
      "grad_norm": 0.0314551517367363,
      "learning_rate": 0.0001984356074425392,
      "loss": 3.7211,
      "step": 23200
    },
    {
      "epoch": 1.6964992611781016,
      "grad_norm": 0.02280016429722309,
      "learning_rate": 0.00019821670923020793,
      "loss": 3.6601,
      "step": 23250
    },
    {
      "epoch": 1.7001477643796632,
      "grad_norm": 0.5990465879440308,
      "learning_rate": 0.00019799781101787666,
      "loss": 3.7243,
      "step": 23300
    },
    {
      "epoch": 1.7037962675812248,
      "grad_norm": 0.05704393610358238,
      "learning_rate": 0.0001977789128055454,
      "loss": 3.7162,
      "step": 23350
    },
    {
      "epoch": 1.7074447707827862,
      "grad_norm": 0.03614777699112892,
      "learning_rate": 0.00019756001459321414,
      "loss": 3.7398,
      "step": 23400
    },
    {
      "epoch": 1.711093273984348,
      "grad_norm": 0.14724422991275787,
      "learning_rate": 0.00019734111638088287,
      "loss": 3.8188,
      "step": 23450
    },
    {
      "epoch": 1.7147417771859095,
      "grad_norm": 0.04389943927526474,
      "learning_rate": 0.0001971222181685516,
      "loss": 3.7058,
      "step": 23500
    },
    {
      "epoch": 1.7183902803874709,
      "grad_norm": 0.03524565324187279,
      "learning_rate": 0.00019690331995622035,
      "loss": 3.7449,
      "step": 23550
    },
    {
      "epoch": 1.7220387835890327,
      "grad_norm": 0.03427056223154068,
      "learning_rate": 0.00019668442174388908,
      "loss": 3.7831,
      "step": 23600
    },
    {
      "epoch": 1.7256872867905941,
      "grad_norm": 0.0431627482175827,
      "learning_rate": 0.0001964655235315578,
      "loss": 3.6639,
      "step": 23650
    },
    {
      "epoch": 1.7293357899921558,
      "grad_norm": 0.06245267763733864,
      "learning_rate": 0.00019624662531922657,
      "loss": 3.763,
      "step": 23700
    },
    {
      "epoch": 1.7329842931937174,
      "grad_norm": 0.053598690778017044,
      "learning_rate": 0.0001960277271068953,
      "loss": 3.7407,
      "step": 23750
    },
    {
      "epoch": 1.7366327963952788,
      "grad_norm": 0.0344044454395771,
      "learning_rate": 0.000195808828894564,
      "loss": 3.7296,
      "step": 23800
    },
    {
      "epoch": 1.7402812995968404,
      "grad_norm": 0.047508012503385544,
      "learning_rate": 0.00019558993068223272,
      "loss": 3.71,
      "step": 23850
    },
    {
      "epoch": 1.743929802798402,
      "grad_norm": 0.038889892399311066,
      "learning_rate": 0.00019537103246990148,
      "loss": 3.7082,
      "step": 23900
    },
    {
      "epoch": 1.7475783059999634,
      "grad_norm": 0.039165135473012924,
      "learning_rate": 0.0001951521342575702,
      "loss": 3.7192,
      "step": 23950
    },
    {
      "epoch": 1.751226809201525,
      "grad_norm": 0.02955910749733448,
      "learning_rate": 0.00019493323604523894,
      "loss": 3.7085,
      "step": 24000
    },
    {
      "epoch": 1.7548753124030867,
      "grad_norm": 0.1282394677400589,
      "learning_rate": 0.0001947143378329077,
      "loss": 3.7096,
      "step": 24050
    },
    {
      "epoch": 1.758523815604648,
      "grad_norm": 0.256636381149292,
      "learning_rate": 0.00019449543962057642,
      "loss": 3.6639,
      "step": 24100
    },
    {
      "epoch": 1.7621723188062097,
      "grad_norm": 0.03235531598329544,
      "learning_rate": 0.00019427654140824515,
      "loss": 3.7336,
      "step": 24150
    },
    {
      "epoch": 1.7658208220077714,
      "grad_norm": 0.02946777455508709,
      "learning_rate": 0.00019405764319591388,
      "loss": 3.6831,
      "step": 24200
    },
    {
      "epoch": 1.7694693252093328,
      "grad_norm": 0.05889840051531792,
      "learning_rate": 0.00019383874498358263,
      "loss": 3.7598,
      "step": 24250
    },
    {
      "epoch": 1.7731178284108944,
      "grad_norm": 0.037389207631349564,
      "learning_rate": 0.00019361984677125136,
      "loss": 3.7918,
      "step": 24300
    },
    {
      "epoch": 1.776766331612456,
      "grad_norm": 0.03324613720178604,
      "learning_rate": 0.0001934009485589201,
      "loss": 3.7412,
      "step": 24350
    },
    {
      "epoch": 1.7804148348140174,
      "grad_norm": 0.04208570346236229,
      "learning_rate": 0.00019318205034658884,
      "loss": 3.813,
      "step": 24400
    },
    {
      "epoch": 1.7840633380155793,
      "grad_norm": 0.05217617005109787,
      "learning_rate": 0.00019296315213425755,
      "loss": 3.7057,
      "step": 24450
    },
    {
      "epoch": 1.7877118412171407,
      "grad_norm": 0.03759100288152695,
      "learning_rate": 0.00019274425392192627,
      "loss": 3.7126,
      "step": 24500
    },
    {
      "epoch": 1.791360344418702,
      "grad_norm": 0.03655564785003662,
      "learning_rate": 0.000192525355709595,
      "loss": 3.7218,
      "step": 24550
    },
    {
      "epoch": 1.795008847620264,
      "grad_norm": 0.03807259351015091,
      "learning_rate": 0.00019230645749726376,
      "loss": 3.7041,
      "step": 24600
    },
    {
      "epoch": 1.7986573508218253,
      "grad_norm": 0.07582968473434448,
      "learning_rate": 0.00019208755928493249,
      "loss": 3.6783,
      "step": 24650
    },
    {
      "epoch": 1.802305854023387,
      "grad_norm": 0.030767671763896942,
      "learning_rate": 0.00019186866107260121,
      "loss": 3.6681,
      "step": 24700
    },
    {
      "epoch": 1.8059543572249486,
      "grad_norm": 0.0707489475607872,
      "learning_rate": 0.00019164976286026997,
      "loss": 3.742,
      "step": 24750
    },
    {
      "epoch": 1.80960286042651,
      "grad_norm": 0.05240349471569061,
      "learning_rate": 0.0001914308646479387,
      "loss": 3.7894,
      "step": 24800
    },
    {
      "epoch": 1.8132513636280716,
      "grad_norm": 0.04019312188029289,
      "learning_rate": 0.00019121196643560743,
      "loss": 3.7302,
      "step": 24850
    },
    {
      "epoch": 1.8168998668296332,
      "grad_norm": 0.04476282000541687,
      "learning_rate": 0.00019099306822327616,
      "loss": 3.707,
      "step": 24900
    },
    {
      "epoch": 1.8205483700311946,
      "grad_norm": 0.04857993125915527,
      "learning_rate": 0.0001907741700109449,
      "loss": 3.7597,
      "step": 24950
    },
    {
      "epoch": 1.8241968732327563,
      "grad_norm": 0.03361140936613083,
      "learning_rate": 0.00019055527179861364,
      "loss": 3.7538,
      "step": 25000
    },
    {
      "epoch": 1.8278453764343179,
      "grad_norm": 0.09826559573411942,
      "learning_rate": 0.00019033637358628234,
      "loss": 3.6868,
      "step": 25050
    },
    {
      "epoch": 1.8314938796358793,
      "grad_norm": 0.03084491938352585,
      "learning_rate": 0.00019011747537395112,
      "loss": 3.7431,
      "step": 25100
    },
    {
      "epoch": 1.835142382837441,
      "grad_norm": 0.11798015981912613,
      "learning_rate": 0.00018989857716161982,
      "loss": 3.7929,
      "step": 25150
    },
    {
      "epoch": 1.8387908860390025,
      "grad_norm": 0.036520007997751236,
      "learning_rate": 0.00018967967894928855,
      "loss": 3.7462,
      "step": 25200
    },
    {
      "epoch": 1.842439389240564,
      "grad_norm": 0.023551637306809425,
      "learning_rate": 0.00018946078073695728,
      "loss": 3.76,
      "step": 25250
    },
    {
      "epoch": 1.8460878924421256,
      "grad_norm": 0.0640057772397995,
      "learning_rate": 0.00018924188252462604,
      "loss": 3.7177,
      "step": 25300
    },
    {
      "epoch": 1.8497363956436872,
      "grad_norm": 0.04241984337568283,
      "learning_rate": 0.00018902298431229476,
      "loss": 3.7021,
      "step": 25350
    },
    {
      "epoch": 1.8533848988452486,
      "grad_norm": 0.036146871745586395,
      "learning_rate": 0.0001888040860999635,
      "loss": 3.756,
      "step": 25400
    },
    {
      "epoch": 1.8570334020468104,
      "grad_norm": 0.03958416357636452,
      "learning_rate": 0.00018858518788763225,
      "loss": 3.7315,
      "step": 25450
    },
    {
      "epoch": 1.8606819052483718,
      "grad_norm": 0.07102249562740326,
      "learning_rate": 0.00018836628967530098,
      "loss": 3.7001,
      "step": 25500
    },
    {
      "epoch": 1.8643304084499333,
      "grad_norm": 0.04141505807638168,
      "learning_rate": 0.0001881473914629697,
      "loss": 3.7048,
      "step": 25550
    },
    {
      "epoch": 1.867978911651495,
      "grad_norm": 0.06838545948266983,
      "learning_rate": 0.00018792849325063843,
      "loss": 3.7255,
      "step": 25600
    },
    {
      "epoch": 1.8716274148530565,
      "grad_norm": 0.03120884858071804,
      "learning_rate": 0.0001877095950383072,
      "loss": 3.7672,
      "step": 25650
    },
    {
      "epoch": 1.8752759180546181,
      "grad_norm": 0.09379472583532333,
      "learning_rate": 0.00018749069682597592,
      "loss": 3.7397,
      "step": 25700
    },
    {
      "epoch": 1.8789244212561798,
      "grad_norm": 0.028936374932527542,
      "learning_rate": 0.00018727179861364462,
      "loss": 3.6687,
      "step": 25750
    },
    {
      "epoch": 1.8825729244577412,
      "grad_norm": 0.035228826105594635,
      "learning_rate": 0.0001870529004013134,
      "loss": 3.8183,
      "step": 25800
    },
    {
      "epoch": 1.8862214276593028,
      "grad_norm": 0.033734120428562164,
      "learning_rate": 0.0001868340021889821,
      "loss": 3.6821,
      "step": 25850
    },
    {
      "epoch": 1.8898699308608644,
      "grad_norm": 0.02987995371222496,
      "learning_rate": 0.00018661510397665083,
      "loss": 3.7407,
      "step": 25900
    },
    {
      "epoch": 1.8935184340624258,
      "grad_norm": 0.3574458956718445,
      "learning_rate": 0.00018639620576431956,
      "loss": 3.6965,
      "step": 25950
    },
    {
      "epoch": 1.8971669372639874,
      "grad_norm": 0.0339631587266922,
      "learning_rate": 0.00018617730755198832,
      "loss": 3.728,
      "step": 26000
    },
    {
      "epoch": 1.900815440465549,
      "grad_norm": 0.0729265958070755,
      "learning_rate": 0.00018595840933965704,
      "loss": 3.7234,
      "step": 26050
    },
    {
      "epoch": 1.9044639436671105,
      "grad_norm": 0.030001645907759666,
      "learning_rate": 0.00018573951112732577,
      "loss": 3.74,
      "step": 26100
    },
    {
      "epoch": 1.908112446868672,
      "grad_norm": 0.06935340911149979,
      "learning_rate": 0.00018552061291499453,
      "loss": 3.7572,
      "step": 26150
    },
    {
      "epoch": 1.9117609500702337,
      "grad_norm": 0.0831083431839943,
      "learning_rate": 0.00018530171470266326,
      "loss": 3.7083,
      "step": 26200
    },
    {
      "epoch": 1.9154094532717951,
      "grad_norm": 0.03957926481962204,
      "learning_rate": 0.00018508281649033198,
      "loss": 3.686,
      "step": 26250
    },
    {
      "epoch": 1.9190579564733568,
      "grad_norm": 0.028678100556135178,
      "learning_rate": 0.0001848639182780007,
      "loss": 3.7491,
      "step": 26300
    },
    {
      "epoch": 1.9227064596749184,
      "grad_norm": 0.033647097647190094,
      "learning_rate": 0.00018464502006566947,
      "loss": 3.649,
      "step": 26350
    },
    {
      "epoch": 1.9263549628764798,
      "grad_norm": 0.04425426945090294,
      "learning_rate": 0.00018442612185333817,
      "loss": 3.7429,
      "step": 26400
    },
    {
      "epoch": 1.9300034660780416,
      "grad_norm": 0.03666727989912033,
      "learning_rate": 0.0001842072236410069,
      "loss": 3.7211,
      "step": 26450
    },
    {
      "epoch": 1.933651969279603,
      "grad_norm": 0.059716686606407166,
      "learning_rate": 0.00018398832542867565,
      "loss": 3.7194,
      "step": 26500
    },
    {
      "epoch": 1.9373004724811644,
      "grad_norm": 0.036296624690294266,
      "learning_rate": 0.00018376942721634438,
      "loss": 3.6925,
      "step": 26550
    },
    {
      "epoch": 1.9409489756827263,
      "grad_norm": 0.03174708038568497,
      "learning_rate": 0.0001835505290040131,
      "loss": 3.7636,
      "step": 26600
    },
    {
      "epoch": 1.9445974788842877,
      "grad_norm": 0.038897812366485596,
      "learning_rate": 0.00018333163079168184,
      "loss": 3.7237,
      "step": 26650
    },
    {
      "epoch": 1.9482459820858493,
      "grad_norm": 0.09178526699542999,
      "learning_rate": 0.0001831127325793506,
      "loss": 3.7073,
      "step": 26700
    },
    {
      "epoch": 1.951894485287411,
      "grad_norm": 0.1691887378692627,
      "learning_rate": 0.00018289383436701932,
      "loss": 3.78,
      "step": 26750
    },
    {
      "epoch": 1.9555429884889723,
      "grad_norm": 0.04344664141535759,
      "learning_rate": 0.00018267493615468805,
      "loss": 3.7942,
      "step": 26800
    },
    {
      "epoch": 1.959191491690534,
      "grad_norm": 0.053674545139074326,
      "learning_rate": 0.0001824560379423568,
      "loss": 3.755,
      "step": 26850
    },
    {
      "epoch": 1.9628399948920956,
      "grad_norm": 0.026353230699896812,
      "learning_rate": 0.00018223713973002553,
      "loss": 3.7186,
      "step": 26900
    },
    {
      "epoch": 1.966488498093657,
      "grad_norm": 0.02703090012073517,
      "learning_rate": 0.00018201824151769426,
      "loss": 3.8022,
      "step": 26950
    },
    {
      "epoch": 1.9701370012952186,
      "grad_norm": 0.05088435858488083,
      "learning_rate": 0.00018179934330536296,
      "loss": 3.7215,
      "step": 27000
    },
    {
      "epoch": 1.9737855044967803,
      "grad_norm": 0.05746394768357277,
      "learning_rate": 0.00018158044509303175,
      "loss": 3.7214,
      "step": 27050
    },
    {
      "epoch": 1.9774340076983417,
      "grad_norm": 0.043972764164209366,
      "learning_rate": 0.00018136154688070045,
      "loss": 3.6459,
      "step": 27100
    },
    {
      "epoch": 1.9810825108999033,
      "grad_norm": 0.1445809304714203,
      "learning_rate": 0.00018114264866836918,
      "loss": 3.7044,
      "step": 27150
    },
    {
      "epoch": 1.984731014101465,
      "grad_norm": 0.06865675747394562,
      "learning_rate": 0.00018092375045603793,
      "loss": 3.7722,
      "step": 27200
    },
    {
      "epoch": 1.9883795173030263,
      "grad_norm": 0.050892237573862076,
      "learning_rate": 0.00018070485224370666,
      "loss": 3.7315,
      "step": 27250
    },
    {
      "epoch": 1.992028020504588,
      "grad_norm": 0.03812471404671669,
      "learning_rate": 0.0001804859540313754,
      "loss": 3.6713,
      "step": 27300
    },
    {
      "epoch": 1.9956765237061496,
      "grad_norm": 0.036164265125989914,
      "learning_rate": 0.00018026705581904412,
      "loss": 3.7273,
      "step": 27350
    },
    {
      "epoch": 1.999325026907711,
      "grad_norm": 0.05214216932654381,
      "learning_rate": 0.00018004815760671287,
      "loss": 3.6758,
      "step": 27400
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.5996508598327637,
      "eval_runtime": 2.9755,
      "eval_samples_per_second": 33.607,
      "eval_steps_per_second": 4.369,
      "step": 27410
    },
    {
      "epoch": 2.002918802561249,
      "grad_norm": 0.027314063161611557,
      "learning_rate": 0.0001798292593943816,
      "loss": 3.6346,
      "step": 27450
    },
    {
      "epoch": 2.0065673057628106,
      "grad_norm": 0.028155092149972916,
      "learning_rate": 0.00017961036118205033,
      "loss": 3.785,
      "step": 27500
    },
    {
      "epoch": 2.0102158089643725,
      "grad_norm": 0.12065047770738602,
      "learning_rate": 0.00017939146296971908,
      "loss": 3.7022,
      "step": 27550
    },
    {
      "epoch": 2.013864312165934,
      "grad_norm": 0.04419897124171257,
      "learning_rate": 0.0001791725647573878,
      "loss": 3.7155,
      "step": 27600
    },
    {
      "epoch": 2.0175128153674953,
      "grad_norm": 0.04260478913784027,
      "learning_rate": 0.00017895366654505654,
      "loss": 3.7157,
      "step": 27650
    },
    {
      "epoch": 2.021161318569057,
      "grad_norm": 0.06051073223352432,
      "learning_rate": 0.00017873476833272524,
      "loss": 3.7153,
      "step": 27700
    },
    {
      "epoch": 2.0248098217706185,
      "grad_norm": 0.2477870136499405,
      "learning_rate": 0.000178515870120394,
      "loss": 3.7113,
      "step": 27750
    },
    {
      "epoch": 2.0284583249721804,
      "grad_norm": 0.025440825149416924,
      "learning_rate": 0.00017829697190806273,
      "loss": 3.7141,
      "step": 27800
    },
    {
      "epoch": 2.0321068281737418,
      "grad_norm": 0.03519119694828987,
      "learning_rate": 0.00017807807369573145,
      "loss": 3.7351,
      "step": 27850
    },
    {
      "epoch": 2.035755331375303,
      "grad_norm": 0.033185578882694244,
      "learning_rate": 0.0001778591754834002,
      "loss": 3.7242,
      "step": 27900
    },
    {
      "epoch": 2.039403834576865,
      "grad_norm": 0.05087568238377571,
      "learning_rate": 0.00017764027727106894,
      "loss": 3.696,
      "step": 27950
    },
    {
      "epoch": 2.0430523377784264,
      "grad_norm": 0.033870164304971695,
      "learning_rate": 0.00017742137905873767,
      "loss": 3.6935,
      "step": 28000
    },
    {
      "epoch": 2.046700840979988,
      "grad_norm": 0.03202371671795845,
      "learning_rate": 0.0001772024808464064,
      "loss": 3.7051,
      "step": 28050
    },
    {
      "epoch": 2.0503493441815497,
      "grad_norm": 0.04216271638870239,
      "learning_rate": 0.00017698358263407515,
      "loss": 3.7096,
      "step": 28100
    },
    {
      "epoch": 2.053997847383111,
      "grad_norm": 0.04536506533622742,
      "learning_rate": 0.00017676468442174388,
      "loss": 3.7478,
      "step": 28150
    },
    {
      "epoch": 2.0576463505846725,
      "grad_norm": 0.036417827010154724,
      "learning_rate": 0.0001765457862094126,
      "loss": 3.7387,
      "step": 28200
    },
    {
      "epoch": 2.0612948537862343,
      "grad_norm": 0.041624657809734344,
      "learning_rate": 0.00017632688799708136,
      "loss": 3.6809,
      "step": 28250
    },
    {
      "epoch": 2.0649433569877957,
      "grad_norm": 0.04628399387001991,
      "learning_rate": 0.0001761079897847501,
      "loss": 3.7256,
      "step": 28300
    },
    {
      "epoch": 2.068591860189357,
      "grad_norm": 0.03277404233813286,
      "learning_rate": 0.0001758890915724188,
      "loss": 3.6959,
      "step": 28350
    },
    {
      "epoch": 2.072240363390919,
      "grad_norm": 0.03182367980480194,
      "learning_rate": 0.00017567019336008752,
      "loss": 3.7312,
      "step": 28400
    },
    {
      "epoch": 2.0758888665924804,
      "grad_norm": 0.030347885563969612,
      "learning_rate": 0.00017545129514775628,
      "loss": 3.6431,
      "step": 28450
    },
    {
      "epoch": 2.079537369794042,
      "grad_norm": 0.03233147785067558,
      "learning_rate": 0.000175232396935425,
      "loss": 3.703,
      "step": 28500
    },
    {
      "epoch": 2.0831858729956036,
      "grad_norm": 0.027962269261479378,
      "learning_rate": 0.00017501349872309373,
      "loss": 3.7209,
      "step": 28550
    },
    {
      "epoch": 2.086834376197165,
      "grad_norm": 0.033755045384168625,
      "learning_rate": 0.0001747946005107625,
      "loss": 3.7032,
      "step": 28600
    },
    {
      "epoch": 2.090482879398727,
      "grad_norm": 0.04333024471998215,
      "learning_rate": 0.00017457570229843122,
      "loss": 3.7247,
      "step": 28650
    },
    {
      "epoch": 2.0941313826002883,
      "grad_norm": 0.058910906314849854,
      "learning_rate": 0.00017435680408609995,
      "loss": 3.6625,
      "step": 28700
    },
    {
      "epoch": 2.0977798858018497,
      "grad_norm": 0.0860552191734314,
      "learning_rate": 0.00017413790587376867,
      "loss": 3.7065,
      "step": 28750
    },
    {
      "epoch": 2.1014283890034116,
      "grad_norm": 0.06849955767393112,
      "learning_rate": 0.00017391900766143743,
      "loss": 3.7741,
      "step": 28800
    },
    {
      "epoch": 2.105076892204973,
      "grad_norm": 0.03335732966661453,
      "learning_rate": 0.00017370010944910616,
      "loss": 3.6936,
      "step": 28850
    },
    {
      "epoch": 2.1087253954065344,
      "grad_norm": 0.03288894146680832,
      "learning_rate": 0.00017348121123677489,
      "loss": 3.6794,
      "step": 28900
    },
    {
      "epoch": 2.112373898608096,
      "grad_norm": 0.035127751529216766,
      "learning_rate": 0.00017326231302444364,
      "loss": 3.6136,
      "step": 28950
    },
    {
      "epoch": 2.1160224018096576,
      "grad_norm": 0.024130450561642647,
      "learning_rate": 0.00017304341481211237,
      "loss": 3.6797,
      "step": 29000
    },
    {
      "epoch": 2.119670905011219,
      "grad_norm": 0.04234929382801056,
      "learning_rate": 0.00017282451659978107,
      "loss": 3.6743,
      "step": 29050
    },
    {
      "epoch": 2.123319408212781,
      "grad_norm": 0.03158097714185715,
      "learning_rate": 0.0001726056183874498,
      "loss": 3.7482,
      "step": 29100
    },
    {
      "epoch": 2.1269679114143423,
      "grad_norm": 0.06742559373378754,
      "learning_rate": 0.00017238672017511855,
      "loss": 3.681,
      "step": 29150
    },
    {
      "epoch": 2.1306164146159037,
      "grad_norm": 0.026925407350063324,
      "learning_rate": 0.00017216782196278728,
      "loss": 3.7497,
      "step": 29200
    },
    {
      "epoch": 2.1342649178174655,
      "grad_norm": 0.10145969688892365,
      "learning_rate": 0.000171948923750456,
      "loss": 3.7677,
      "step": 29250
    },
    {
      "epoch": 2.137913421019027,
      "grad_norm": 0.031433653086423874,
      "learning_rate": 0.00017173002553812477,
      "loss": 3.6695,
      "step": 29300
    },
    {
      "epoch": 2.1415619242205883,
      "grad_norm": 0.07982908189296722,
      "learning_rate": 0.0001715111273257935,
      "loss": 3.77,
      "step": 29350
    },
    {
      "epoch": 2.14521042742215,
      "grad_norm": 0.02896960824728012,
      "learning_rate": 0.00017129222911346222,
      "loss": 3.7789,
      "step": 29400
    },
    {
      "epoch": 2.1488589306237116,
      "grad_norm": 0.052224207669496536,
      "learning_rate": 0.00017107333090113095,
      "loss": 3.6879,
      "step": 29450
    },
    {
      "epoch": 2.152507433825273,
      "grad_norm": 0.040135618299245834,
      "learning_rate": 0.0001708544326887997,
      "loss": 3.7112,
      "step": 29500
    },
    {
      "epoch": 2.156155937026835,
      "grad_norm": 0.05442206561565399,
      "learning_rate": 0.00017063553447646844,
      "loss": 3.7153,
      "step": 29550
    },
    {
      "epoch": 2.1598044402283962,
      "grad_norm": 0.03723670914769173,
      "learning_rate": 0.00017041663626413716,
      "loss": 3.6894,
      "step": 29600
    },
    {
      "epoch": 2.1634529434299576,
      "grad_norm": 0.03198996186256409,
      "learning_rate": 0.00017019773805180592,
      "loss": 3.7002,
      "step": 29650
    },
    {
      "epoch": 2.1671014466315195,
      "grad_norm": 0.10657303780317307,
      "learning_rate": 0.00016997883983947462,
      "loss": 3.7167,
      "step": 29700
    },
    {
      "epoch": 2.170749949833081,
      "grad_norm": 0.062158357352018356,
      "learning_rate": 0.00016975994162714335,
      "loss": 3.7019,
      "step": 29750
    },
    {
      "epoch": 2.1743984530346427,
      "grad_norm": 0.024099837988615036,
      "learning_rate": 0.00016954104341481208,
      "loss": 3.6816,
      "step": 29800
    },
    {
      "epoch": 2.178046956236204,
      "grad_norm": 0.03290212154388428,
      "learning_rate": 0.00016932214520248083,
      "loss": 3.7117,
      "step": 29850
    },
    {
      "epoch": 2.1816954594377655,
      "grad_norm": 1.05609929561615,
      "learning_rate": 0.00016910324699014956,
      "loss": 3.6935,
      "step": 29900
    },
    {
      "epoch": 2.1853439626393274,
      "grad_norm": 0.12853774428367615,
      "learning_rate": 0.0001688843487778183,
      "loss": 3.6755,
      "step": 29950
    },
    {
      "epoch": 2.188992465840889,
      "grad_norm": 0.025300130248069763,
      "learning_rate": 0.00016866545056548705,
      "loss": 3.7238,
      "step": 30000
    },
    {
      "epoch": 2.19264096904245,
      "grad_norm": 0.08892501145601273,
      "learning_rate": 0.00016844655235315577,
      "loss": 3.7197,
      "step": 30050
    },
    {
      "epoch": 2.196289472244012,
      "grad_norm": 0.025078188627958298,
      "learning_rate": 0.0001682276541408245,
      "loss": 3.7265,
      "step": 30100
    },
    {
      "epoch": 2.1999379754455735,
      "grad_norm": 0.03901035338640213,
      "learning_rate": 0.00016800875592849323,
      "loss": 3.7033,
      "step": 30150
    },
    {
      "epoch": 2.203586478647135,
      "grad_norm": 0.04873057082295418,
      "learning_rate": 0.00016778985771616199,
      "loss": 3.716,
      "step": 30200
    },
    {
      "epoch": 2.2072349818486967,
      "grad_norm": 0.029207907617092133,
      "learning_rate": 0.00016757095950383071,
      "loss": 3.7139,
      "step": 30250
    },
    {
      "epoch": 2.210883485050258,
      "grad_norm": 0.7134537100791931,
      "learning_rate": 0.00016735206129149942,
      "loss": 3.7207,
      "step": 30300
    },
    {
      "epoch": 2.2145319882518195,
      "grad_norm": 0.05341343209147453,
      "learning_rate": 0.0001671331630791682,
      "loss": 3.6541,
      "step": 30350
    },
    {
      "epoch": 2.2181804914533814,
      "grad_norm": 0.045096904039382935,
      "learning_rate": 0.0001669142648668369,
      "loss": 3.7413,
      "step": 30400
    },
    {
      "epoch": 2.2218289946549428,
      "grad_norm": 0.08342590183019638,
      "learning_rate": 0.00016669536665450563,
      "loss": 3.7443,
      "step": 30450
    },
    {
      "epoch": 2.225477497856504,
      "grad_norm": 0.03520524501800537,
      "learning_rate": 0.00016647646844217436,
      "loss": 3.7143,
      "step": 30500
    },
    {
      "epoch": 2.229126001058066,
      "grad_norm": 0.04705386608839035,
      "learning_rate": 0.0001662575702298431,
      "loss": 3.6951,
      "step": 30550
    },
    {
      "epoch": 2.2327745042596274,
      "grad_norm": 0.031306762248277664,
      "learning_rate": 0.00016603867201751184,
      "loss": 3.7155,
      "step": 30600
    },
    {
      "epoch": 2.2364230074611893,
      "grad_norm": 0.028585316613316536,
      "learning_rate": 0.00016581977380518057,
      "loss": 3.7064,
      "step": 30650
    },
    {
      "epoch": 2.2400715106627507,
      "grad_norm": 0.022670792415738106,
      "learning_rate": 0.00016560087559284932,
      "loss": 3.6254,
      "step": 30700
    },
    {
      "epoch": 2.243720013864312,
      "grad_norm": 0.02968808263540268,
      "learning_rate": 0.00016538197738051805,
      "loss": 3.7019,
      "step": 30750
    },
    {
      "epoch": 2.247368517065874,
      "grad_norm": 0.10011749714612961,
      "learning_rate": 0.00016516307916818678,
      "loss": 3.7211,
      "step": 30800
    },
    {
      "epoch": 2.2510170202674353,
      "grad_norm": 0.02958962507545948,
      "learning_rate": 0.0001649441809558555,
      "loss": 3.7175,
      "step": 30850
    },
    {
      "epoch": 2.2546655234689967,
      "grad_norm": 0.03714769333600998,
      "learning_rate": 0.00016472528274352426,
      "loss": 3.7218,
      "step": 30900
    },
    {
      "epoch": 2.2583140266705586,
      "grad_norm": 0.027733899652957916,
      "learning_rate": 0.000164506384531193,
      "loss": 3.6967,
      "step": 30950
    },
    {
      "epoch": 2.26196252987212,
      "grad_norm": 0.02549852803349495,
      "learning_rate": 0.0001642874863188617,
      "loss": 3.6986,
      "step": 31000
    },
    {
      "epoch": 2.2656110330736814,
      "grad_norm": 0.06321641057729721,
      "learning_rate": 0.00016406858810653045,
      "loss": 3.6933,
      "step": 31050
    },
    {
      "epoch": 2.2692595362752432,
      "grad_norm": 0.024481674656271935,
      "learning_rate": 0.00016384968989419918,
      "loss": 3.6875,
      "step": 31100
    },
    {
      "epoch": 2.2729080394768046,
      "grad_norm": 0.5210328102111816,
      "learning_rate": 0.0001636307916818679,
      "loss": 3.7193,
      "step": 31150
    },
    {
      "epoch": 2.276556542678366,
      "grad_norm": 0.05594715476036072,
      "learning_rate": 0.00016341189346953663,
      "loss": 3.6866,
      "step": 31200
    },
    {
      "epoch": 2.280205045879928,
      "grad_norm": 0.08277968317270279,
      "learning_rate": 0.0001631929952572054,
      "loss": 3.7352,
      "step": 31250
    },
    {
      "epoch": 2.2838535490814893,
      "grad_norm": 0.02978287823498249,
      "learning_rate": 0.00016297409704487412,
      "loss": 3.7138,
      "step": 31300
    },
    {
      "epoch": 2.2875020522830507,
      "grad_norm": 0.07596124708652496,
      "learning_rate": 0.00016275519883254285,
      "loss": 3.7409,
      "step": 31350
    },
    {
      "epoch": 2.2911505554846125,
      "grad_norm": 0.0394388884305954,
      "learning_rate": 0.0001625363006202116,
      "loss": 3.732,
      "step": 31400
    },
    {
      "epoch": 2.294799058686174,
      "grad_norm": 0.044909361749887466,
      "learning_rate": 0.00016231740240788033,
      "loss": 3.7348,
      "step": 31450
    },
    {
      "epoch": 2.298447561887736,
      "grad_norm": 0.05761009827256203,
      "learning_rate": 0.00016209850419554906,
      "loss": 3.7472,
      "step": 31500
    },
    {
      "epoch": 2.302096065089297,
      "grad_norm": 0.07354026287794113,
      "learning_rate": 0.0001618796059832178,
      "loss": 3.7352,
      "step": 31550
    },
    {
      "epoch": 2.3057445682908586,
      "grad_norm": 0.030753429979085922,
      "learning_rate": 0.00016166070777088654,
      "loss": 3.7451,
      "step": 31600
    },
    {
      "epoch": 2.30939307149242,
      "grad_norm": 0.035754792392253876,
      "learning_rate": 0.00016144180955855524,
      "loss": 3.7263,
      "step": 31650
    },
    {
      "epoch": 2.313041574693982,
      "grad_norm": 0.03070138208568096,
      "learning_rate": 0.00016122291134622397,
      "loss": 3.6954,
      "step": 31700
    },
    {
      "epoch": 2.3166900778955433,
      "grad_norm": 0.051676273345947266,
      "learning_rate": 0.00016100401313389273,
      "loss": 3.7134,
      "step": 31750
    },
    {
      "epoch": 2.320338581097105,
      "grad_norm": 0.038277726620435715,
      "learning_rate": 0.00016078511492156146,
      "loss": 3.7142,
      "step": 31800
    },
    {
      "epoch": 2.3239870842986665,
      "grad_norm": 0.1184808611869812,
      "learning_rate": 0.00016056621670923019,
      "loss": 3.7787,
      "step": 31850
    },
    {
      "epoch": 2.327635587500228,
      "grad_norm": 0.030626820400357246,
      "learning_rate": 0.0001603473184968989,
      "loss": 3.7066,
      "step": 31900
    },
    {
      "epoch": 2.3312840907017898,
      "grad_norm": 0.06596790254116058,
      "learning_rate": 0.00016012842028456767,
      "loss": 3.7114,
      "step": 31950
    },
    {
      "epoch": 2.334932593903351,
      "grad_norm": 0.03529735282063484,
      "learning_rate": 0.0001599095220722364,
      "loss": 3.7538,
      "step": 32000
    },
    {
      "epoch": 2.3385810971049126,
      "grad_norm": 0.03496669977903366,
      "learning_rate": 0.00015969062385990513,
      "loss": 3.6747,
      "step": 32050
    },
    {
      "epoch": 2.3422296003064744,
      "grad_norm": 0.0310862734913826,
      "learning_rate": 0.00015947172564757388,
      "loss": 3.6345,
      "step": 32100
    },
    {
      "epoch": 2.345878103508036,
      "grad_norm": 0.17403370141983032,
      "learning_rate": 0.0001592528274352426,
      "loss": 3.701,
      "step": 32150
    },
    {
      "epoch": 2.3495266067095972,
      "grad_norm": 0.023185834288597107,
      "learning_rate": 0.00015903392922291134,
      "loss": 3.7313,
      "step": 32200
    },
    {
      "epoch": 2.353175109911159,
      "grad_norm": 0.029449570924043655,
      "learning_rate": 0.00015881503101058004,
      "loss": 3.7429,
      "step": 32250
    },
    {
      "epoch": 2.3568236131127205,
      "grad_norm": 0.06947766989469528,
      "learning_rate": 0.00015859613279824882,
      "loss": 3.7321,
      "step": 32300
    },
    {
      "epoch": 2.360472116314282,
      "grad_norm": 0.03343326598405838,
      "learning_rate": 0.00015837723458591752,
      "loss": 3.721,
      "step": 32350
    },
    {
      "epoch": 2.3641206195158437,
      "grad_norm": 0.027394969016313553,
      "learning_rate": 0.00015815833637358625,
      "loss": 3.6869,
      "step": 32400
    },
    {
      "epoch": 2.367769122717405,
      "grad_norm": 0.09158371388912201,
      "learning_rate": 0.000157939438161255,
      "loss": 3.7116,
      "step": 32450
    },
    {
      "epoch": 2.3714176259189665,
      "grad_norm": 0.03545945882797241,
      "learning_rate": 0.00015772053994892374,
      "loss": 3.732,
      "step": 32500
    },
    {
      "epoch": 2.3750661291205284,
      "grad_norm": 0.12533922493457794,
      "learning_rate": 0.00015750164173659246,
      "loss": 3.7678,
      "step": 32550
    },
    {
      "epoch": 2.37871463232209,
      "grad_norm": 0.11349128186702728,
      "learning_rate": 0.0001572827435242612,
      "loss": 3.751,
      "step": 32600
    },
    {
      "epoch": 2.3823631355236516,
      "grad_norm": 0.03099140152335167,
      "learning_rate": 0.00015706384531192995,
      "loss": 3.6712,
      "step": 32650
    },
    {
      "epoch": 2.386011638725213,
      "grad_norm": 0.19637298583984375,
      "learning_rate": 0.00015684494709959868,
      "loss": 3.6612,
      "step": 32700
    },
    {
      "epoch": 2.3896601419267745,
      "grad_norm": 0.030805863440036774,
      "learning_rate": 0.0001566260488872674,
      "loss": 3.7135,
      "step": 32750
    },
    {
      "epoch": 2.393308645128336,
      "grad_norm": 0.21645528078079224,
      "learning_rate": 0.00015640715067493616,
      "loss": 3.7269,
      "step": 32800
    },
    {
      "epoch": 2.3969571483298977,
      "grad_norm": 0.06008106842637062,
      "learning_rate": 0.0001561882524626049,
      "loss": 3.6569,
      "step": 32850
    },
    {
      "epoch": 2.400605651531459,
      "grad_norm": 0.12624046206474304,
      "learning_rate": 0.00015596935425027362,
      "loss": 3.7018,
      "step": 32900
    },
    {
      "epoch": 2.404254154733021,
      "grad_norm": 0.033670298755168915,
      "learning_rate": 0.00015575045603794232,
      "loss": 3.6525,
      "step": 32950
    },
    {
      "epoch": 2.4079026579345824,
      "grad_norm": 0.03503101319074631,
      "learning_rate": 0.00015553155782561107,
      "loss": 3.6969,
      "step": 33000
    },
    {
      "epoch": 2.4115511611361438,
      "grad_norm": 0.02433626912534237,
      "learning_rate": 0.0001553126596132798,
      "loss": 3.7449,
      "step": 33050
    },
    {
      "epoch": 2.4151996643377056,
      "grad_norm": 0.048047568649053574,
      "learning_rate": 0.00015509376140094853,
      "loss": 3.8101,
      "step": 33100
    },
    {
      "epoch": 2.418848167539267,
      "grad_norm": 0.025133667513728142,
      "learning_rate": 0.00015487486318861729,
      "loss": 3.7666,
      "step": 33150
    },
    {
      "epoch": 2.4224966707408284,
      "grad_norm": 0.03243400901556015,
      "learning_rate": 0.00015465596497628601,
      "loss": 3.7284,
      "step": 33200
    },
    {
      "epoch": 2.4261451739423903,
      "grad_norm": 0.0232378039509058,
      "learning_rate": 0.00015443706676395474,
      "loss": 3.6562,
      "step": 33250
    },
    {
      "epoch": 2.4297936771439517,
      "grad_norm": 0.03665006533265114,
      "learning_rate": 0.00015421816855162347,
      "loss": 3.7077,
      "step": 33300
    },
    {
      "epoch": 2.433442180345513,
      "grad_norm": 0.096726194024086,
      "learning_rate": 0.00015399927033929223,
      "loss": 3.7382,
      "step": 33350
    },
    {
      "epoch": 2.437090683547075,
      "grad_norm": 0.0371132418513298,
      "learning_rate": 0.00015378037212696095,
      "loss": 3.7098,
      "step": 33400
    },
    {
      "epoch": 2.4407391867486363,
      "grad_norm": 0.053718384355306625,
      "learning_rate": 0.00015356147391462968,
      "loss": 3.721,
      "step": 33450
    },
    {
      "epoch": 2.444387689950198,
      "grad_norm": 0.035054031759500504,
      "learning_rate": 0.00015334257570229844,
      "loss": 3.7168,
      "step": 33500
    },
    {
      "epoch": 2.4480361931517596,
      "grad_norm": 0.02319161780178547,
      "learning_rate": 0.00015312367748996717,
      "loss": 3.727,
      "step": 33550
    },
    {
      "epoch": 2.451684696353321,
      "grad_norm": 0.032326340675354004,
      "learning_rate": 0.00015290477927763587,
      "loss": 3.6922,
      "step": 33600
    },
    {
      "epoch": 2.4553331995548824,
      "grad_norm": 0.04012110084295273,
      "learning_rate": 0.0001526858810653046,
      "loss": 3.6548,
      "step": 33650
    },
    {
      "epoch": 2.4589817027564442,
      "grad_norm": 0.030560150742530823,
      "learning_rate": 0.00015246698285297335,
      "loss": 3.6678,
      "step": 33700
    },
    {
      "epoch": 2.4626302059580056,
      "grad_norm": 0.037195052951574326,
      "learning_rate": 0.00015224808464064208,
      "loss": 3.6261,
      "step": 33750
    },
    {
      "epoch": 2.4662787091595675,
      "grad_norm": 0.02711213193833828,
      "learning_rate": 0.0001520291864283108,
      "loss": 3.6705,
      "step": 33800
    },
    {
      "epoch": 2.469927212361129,
      "grad_norm": 0.24282309412956238,
      "learning_rate": 0.00015181028821597956,
      "loss": 3.7336,
      "step": 33850
    },
    {
      "epoch": 2.4735757155626903,
      "grad_norm": 0.032279662787914276,
      "learning_rate": 0.0001515913900036483,
      "loss": 3.7485,
      "step": 33900
    },
    {
      "epoch": 2.477224218764252,
      "grad_norm": 0.05531876161694527,
      "learning_rate": 0.00015137249179131702,
      "loss": 3.7117,
      "step": 33950
    },
    {
      "epoch": 2.4808727219658135,
      "grad_norm": 0.029257871210575104,
      "learning_rate": 0.00015115359357898575,
      "loss": 3.7485,
      "step": 34000
    },
    {
      "epoch": 2.484521225167375,
      "grad_norm": 0.02871805429458618,
      "learning_rate": 0.0001509346953666545,
      "loss": 3.7185,
      "step": 34050
    },
    {
      "epoch": 2.488169728368937,
      "grad_norm": 0.01962357759475708,
      "learning_rate": 0.00015071579715432323,
      "loss": 3.6793,
      "step": 34100
    },
    {
      "epoch": 2.491818231570498,
      "grad_norm": 0.05273926258087158,
      "learning_rate": 0.00015049689894199196,
      "loss": 3.721,
      "step": 34150
    },
    {
      "epoch": 2.4954667347720596,
      "grad_norm": 0.05227784439921379,
      "learning_rate": 0.00015027800072966072,
      "loss": 3.683,
      "step": 34200
    },
    {
      "epoch": 2.4991152379736215,
      "grad_norm": 0.024138206616044044,
      "learning_rate": 0.00015005910251732945,
      "loss": 3.7151,
      "step": 34250
    },
    {
      "epoch": 2.502763741175183,
      "grad_norm": 0.02874460630118847,
      "learning_rate": 0.00014984020430499815,
      "loss": 3.7417,
      "step": 34300
    },
    {
      "epoch": 2.5064122443767447,
      "grad_norm": 0.043167468160390854,
      "learning_rate": 0.0001496213060926669,
      "loss": 3.7117,
      "step": 34350
    },
    {
      "epoch": 2.510060747578306,
      "grad_norm": 0.020832182839512825,
      "learning_rate": 0.00014940240788033563,
      "loss": 3.6937,
      "step": 34400
    },
    {
      "epoch": 2.5137092507798675,
      "grad_norm": 0.020511474460363388,
      "learning_rate": 0.00014918350966800436,
      "loss": 3.7396,
      "step": 34450
    },
    {
      "epoch": 2.517357753981429,
      "grad_norm": 0.033349089324474335,
      "learning_rate": 0.00014896461145567311,
      "loss": 3.655,
      "step": 34500
    },
    {
      "epoch": 2.5210062571829908,
      "grad_norm": 0.02685672417283058,
      "learning_rate": 0.00014874571324334184,
      "loss": 3.7246,
      "step": 34550
    },
    {
      "epoch": 2.524654760384552,
      "grad_norm": 0.038219016045331955,
      "learning_rate": 0.00014852681503101057,
      "loss": 3.7438,
      "step": 34600
    },
    {
      "epoch": 2.528303263586114,
      "grad_norm": 0.06750869005918503,
      "learning_rate": 0.0001483079168186793,
      "loss": 3.6911,
      "step": 34650
    },
    {
      "epoch": 2.5319517667876754,
      "grad_norm": 0.025622056797146797,
      "learning_rate": 0.00014808901860634803,
      "loss": 3.6971,
      "step": 34700
    },
    {
      "epoch": 2.535600269989237,
      "grad_norm": 0.054400090128183365,
      "learning_rate": 0.00014787012039401676,
      "loss": 3.7376,
      "step": 34750
    },
    {
      "epoch": 2.5392487731907982,
      "grad_norm": 0.02992349863052368,
      "learning_rate": 0.0001476512221816855,
      "loss": 3.7323,
      "step": 34800
    },
    {
      "epoch": 2.54289727639236,
      "grad_norm": 0.030408021062612534,
      "learning_rate": 0.00014743232396935424,
      "loss": 3.7275,
      "step": 34850
    },
    {
      "epoch": 2.5465457795939215,
      "grad_norm": 0.03327470272779465,
      "learning_rate": 0.00014721342575702297,
      "loss": 3.7245,
      "step": 34900
    },
    {
      "epoch": 2.5501942827954833,
      "grad_norm": 0.04301593452692032,
      "learning_rate": 0.0001469945275446917,
      "loss": 3.6384,
      "step": 34950
    },
    {
      "epoch": 2.5538427859970447,
      "grad_norm": 0.03895898908376694,
      "learning_rate": 0.00014677562933236042,
      "loss": 3.7074,
      "step": 35000
    },
    {
      "epoch": 2.557491289198606,
      "grad_norm": 0.022967999801039696,
      "learning_rate": 0.00014655673112002918,
      "loss": 3.7163,
      "step": 35050
    },
    {
      "epoch": 2.561139792400168,
      "grad_norm": 0.021564392372965813,
      "learning_rate": 0.0001463378329076979,
      "loss": 3.6671,
      "step": 35100
    },
    {
      "epoch": 2.5647882956017294,
      "grad_norm": 0.06318273395299911,
      "learning_rate": 0.00014611893469536664,
      "loss": 3.7177,
      "step": 35150
    },
    {
      "epoch": 2.568436798803291,
      "grad_norm": 0.04439930245280266,
      "learning_rate": 0.0001459000364830354,
      "loss": 3.7008,
      "step": 35200
    },
    {
      "epoch": 2.5720853020048526,
      "grad_norm": 0.04340856522321701,
      "learning_rate": 0.0001456811382707041,
      "loss": 3.6518,
      "step": 35250
    },
    {
      "epoch": 2.575733805206414,
      "grad_norm": 0.031526751816272736,
      "learning_rate": 0.00014546224005837285,
      "loss": 3.7271,
      "step": 35300
    },
    {
      "epoch": 2.5793823084079754,
      "grad_norm": 0.044760845601558685,
      "learning_rate": 0.00014524334184604158,
      "loss": 3.656,
      "step": 35350
    },
    {
      "epoch": 2.5830308116095373,
      "grad_norm": 0.034465476870536804,
      "learning_rate": 0.0001450244436337103,
      "loss": 3.6982,
      "step": 35400
    },
    {
      "epoch": 2.5866793148110987,
      "grad_norm": 0.03232419118285179,
      "learning_rate": 0.00014480554542137903,
      "loss": 3.6628,
      "step": 35450
    },
    {
      "epoch": 2.5903278180126605,
      "grad_norm": 0.06548285484313965,
      "learning_rate": 0.0001445866472090478,
      "loss": 3.7127,
      "step": 35500
    },
    {
      "epoch": 2.593976321214222,
      "grad_norm": 0.03540473431348801,
      "learning_rate": 0.00014436774899671652,
      "loss": 3.7849,
      "step": 35550
    },
    {
      "epoch": 2.5976248244157834,
      "grad_norm": 0.03141536936163902,
      "learning_rate": 0.00014414885078438525,
      "loss": 3.7376,
      "step": 35600
    },
    {
      "epoch": 2.6012733276173448,
      "grad_norm": 0.037466518580913544,
      "learning_rate": 0.00014392995257205398,
      "loss": 3.6963,
      "step": 35650
    },
    {
      "epoch": 2.6049218308189066,
      "grad_norm": 0.11315621435642242,
      "learning_rate": 0.0001437110543597227,
      "loss": 3.6983,
      "step": 35700
    },
    {
      "epoch": 2.608570334020468,
      "grad_norm": 0.025569966062903404,
      "learning_rate": 0.00014349215614739146,
      "loss": 3.754,
      "step": 35750
    },
    {
      "epoch": 2.61221883722203,
      "grad_norm": 0.038335878401994705,
      "learning_rate": 0.0001432732579350602,
      "loss": 3.7287,
      "step": 35800
    },
    {
      "epoch": 2.6158673404235913,
      "grad_norm": 0.03254277631640434,
      "learning_rate": 0.00014305435972272892,
      "loss": 3.6828,
      "step": 35850
    },
    {
      "epoch": 2.6195158436251527,
      "grad_norm": 0.04545539617538452,
      "learning_rate": 0.00014283546151039767,
      "loss": 3.6699,
      "step": 35900
    },
    {
      "epoch": 2.623164346826714,
      "grad_norm": 0.041733916848897934,
      "learning_rate": 0.00014261656329806637,
      "loss": 3.7252,
      "step": 35950
    },
    {
      "epoch": 2.626812850028276,
      "grad_norm": 0.03520975634455681,
      "learning_rate": 0.00014239766508573513,
      "loss": 3.7514,
      "step": 36000
    },
    {
      "epoch": 2.6304613532298373,
      "grad_norm": 0.029401365667581558,
      "learning_rate": 0.00014217876687340386,
      "loss": 3.7216,
      "step": 36050
    },
    {
      "epoch": 2.634109856431399,
      "grad_norm": 0.08226151019334793,
      "learning_rate": 0.00014195986866107258,
      "loss": 3.746,
      "step": 36100
    },
    {
      "epoch": 2.6377583596329606,
      "grad_norm": 0.03287854790687561,
      "learning_rate": 0.00014174097044874134,
      "loss": 3.6802,
      "step": 36150
    },
    {
      "epoch": 2.641406862834522,
      "grad_norm": 0.03870673477649689,
      "learning_rate": 0.00014152207223641007,
      "loss": 3.7233,
      "step": 36200
    },
    {
      "epoch": 2.645055366036084,
      "grad_norm": 0.027684787288308144,
      "learning_rate": 0.0001413031740240788,
      "loss": 3.7013,
      "step": 36250
    },
    {
      "epoch": 2.6487038692376452,
      "grad_norm": 0.12461115419864655,
      "learning_rate": 0.00014108427581174753,
      "loss": 3.7081,
      "step": 36300
    },
    {
      "epoch": 2.652352372439207,
      "grad_norm": 0.021466288715600967,
      "learning_rate": 0.00014086537759941625,
      "loss": 3.7286,
      "step": 36350
    },
    {
      "epoch": 2.6560008756407685,
      "grad_norm": 0.05084162577986717,
      "learning_rate": 0.00014064647938708498,
      "loss": 3.6874,
      "step": 36400
    },
    {
      "epoch": 2.65964937884233,
      "grad_norm": 0.024926528334617615,
      "learning_rate": 0.00014042758117475374,
      "loss": 3.6986,
      "step": 36450
    },
    {
      "epoch": 2.6632978820438913,
      "grad_norm": 0.034155771136283875,
      "learning_rate": 0.00014020868296242247,
      "loss": 3.6904,
      "step": 36500
    },
    {
      "epoch": 2.666946385245453,
      "grad_norm": 0.02600845694541931,
      "learning_rate": 0.0001399897847500912,
      "loss": 3.6813,
      "step": 36550
    },
    {
      "epoch": 2.6705948884470145,
      "grad_norm": 0.024390293285250664,
      "learning_rate": 0.00013977088653775992,
      "loss": 3.7846,
      "step": 36600
    },
    {
      "epoch": 2.6742433916485764,
      "grad_norm": 0.03530003875494003,
      "learning_rate": 0.00013955198832542865,
      "loss": 3.6784,
      "step": 36650
    },
    {
      "epoch": 2.677891894850138,
      "grad_norm": 0.025290310382843018,
      "learning_rate": 0.0001393330901130974,
      "loss": 3.6765,
      "step": 36700
    },
    {
      "epoch": 2.681540398051699,
      "grad_norm": 0.025722254067659378,
      "learning_rate": 0.00013911419190076613,
      "loss": 3.6798,
      "step": 36750
    },
    {
      "epoch": 2.6851889012532606,
      "grad_norm": 0.021283172070980072,
      "learning_rate": 0.00013889529368843486,
      "loss": 3.7063,
      "step": 36800
    },
    {
      "epoch": 2.6888374044548224,
      "grad_norm": 0.03129015490412712,
      "learning_rate": 0.00013867639547610362,
      "loss": 3.7694,
      "step": 36850
    },
    {
      "epoch": 2.692485907656384,
      "grad_norm": 0.03117377869784832,
      "learning_rate": 0.00013845749726377232,
      "loss": 3.6667,
      "step": 36900
    },
    {
      "epoch": 2.6961344108579457,
      "grad_norm": 0.07687089592218399,
      "learning_rate": 0.00013823859905144108,
      "loss": 3.7217,
      "step": 36950
    },
    {
      "epoch": 2.699782914059507,
      "grad_norm": 0.02712794579565525,
      "learning_rate": 0.0001380197008391098,
      "loss": 3.7702,
      "step": 37000
    },
    {
      "epoch": 2.7034314172610685,
      "grad_norm": 0.023605216294527054,
      "learning_rate": 0.00013780080262677853,
      "loss": 3.6984,
      "step": 37050
    },
    {
      "epoch": 2.7070799204626304,
      "grad_norm": 0.030429070815443993,
      "learning_rate": 0.00013758190441444726,
      "loss": 3.7178,
      "step": 37100
    },
    {
      "epoch": 2.7107284236641918,
      "grad_norm": 0.053378861397504807,
      "learning_rate": 0.00013736300620211602,
      "loss": 3.7315,
      "step": 37150
    },
    {
      "epoch": 2.714376926865753,
      "grad_norm": 0.10871393978595734,
      "learning_rate": 0.00013714410798978474,
      "loss": 3.6454,
      "step": 37200
    },
    {
      "epoch": 2.718025430067315,
      "grad_norm": 0.1560402363538742,
      "learning_rate": 0.00013692520977745347,
      "loss": 3.6966,
      "step": 37250
    },
    {
      "epoch": 2.7216739332688764,
      "grad_norm": 0.029761554673314095,
      "learning_rate": 0.0001367063115651222,
      "loss": 3.7354,
      "step": 37300
    },
    {
      "epoch": 2.725322436470438,
      "grad_norm": 0.03610110282897949,
      "learning_rate": 0.00013648741335279093,
      "loss": 3.7332,
      "step": 37350
    },
    {
      "epoch": 2.7289709396719997,
      "grad_norm": 0.02699705958366394,
      "learning_rate": 0.00013626851514045968,
      "loss": 3.7252,
      "step": 37400
    },
    {
      "epoch": 2.732619442873561,
      "grad_norm": 0.021994927898049355,
      "learning_rate": 0.0001360496169281284,
      "loss": 3.7405,
      "step": 37450
    },
    {
      "epoch": 2.736267946075123,
      "grad_norm": 0.02251945436000824,
      "learning_rate": 0.00013583071871579714,
      "loss": 3.6843,
      "step": 37500
    },
    {
      "epoch": 2.7399164492766843,
      "grad_norm": 0.021941110491752625,
      "learning_rate": 0.0001356118205034659,
      "loss": 3.645,
      "step": 37550
    },
    {
      "epoch": 2.7435649524782457,
      "grad_norm": 0.05271760746836662,
      "learning_rate": 0.0001353929222911346,
      "loss": 3.7345,
      "step": 37600
    },
    {
      "epoch": 2.747213455679807,
      "grad_norm": 0.026480451226234436,
      "learning_rate": 0.00013517402407880335,
      "loss": 3.7681,
      "step": 37650
    },
    {
      "epoch": 2.750861958881369,
      "grad_norm": 0.027411725372076035,
      "learning_rate": 0.00013495512586647208,
      "loss": 3.7698,
      "step": 37700
    },
    {
      "epoch": 2.7545104620829304,
      "grad_norm": 0.02888965792953968,
      "learning_rate": 0.0001347362276541408,
      "loss": 3.7267,
      "step": 37750
    },
    {
      "epoch": 2.7581589652844922,
      "grad_norm": 0.03401603177189827,
      "learning_rate": 0.00013451732944180954,
      "loss": 3.7382,
      "step": 37800
    },
    {
      "epoch": 2.7618074684860536,
      "grad_norm": 0.041346993297338486,
      "learning_rate": 0.0001342984312294783,
      "loss": 3.7522,
      "step": 37850
    },
    {
      "epoch": 2.765455971687615,
      "grad_norm": 0.026001131162047386,
      "learning_rate": 0.00013407953301714702,
      "loss": 3.6997,
      "step": 37900
    },
    {
      "epoch": 2.769104474889177,
      "grad_norm": 0.025585085153579712,
      "learning_rate": 0.00013386063480481575,
      "loss": 3.7486,
      "step": 37950
    },
    {
      "epoch": 2.7727529780907383,
      "grad_norm": 0.028600165620446205,
      "learning_rate": 0.00013364173659248448,
      "loss": 3.6961,
      "step": 38000
    },
    {
      "epoch": 2.7764014812922997,
      "grad_norm": 0.055807918310165405,
      "learning_rate": 0.0001334228383801532,
      "loss": 3.6824,
      "step": 38050
    },
    {
      "epoch": 2.7800499844938615,
      "grad_norm": 0.15652208030223846,
      "learning_rate": 0.00013320394016782196,
      "loss": 3.707,
      "step": 38100
    },
    {
      "epoch": 2.783698487695423,
      "grad_norm": 0.023340582847595215,
      "learning_rate": 0.0001329850419554907,
      "loss": 3.6506,
      "step": 38150
    },
    {
      "epoch": 2.7873469908969843,
      "grad_norm": 0.026678122580051422,
      "learning_rate": 0.00013276614374315942,
      "loss": 3.6953,
      "step": 38200
    },
    {
      "epoch": 2.790995494098546,
      "grad_norm": 0.05905041843652725,
      "learning_rate": 0.00013254724553082815,
      "loss": 3.7559,
      "step": 38250
    },
    {
      "epoch": 2.7946439973001076,
      "grad_norm": 0.03450913727283478,
      "learning_rate": 0.00013232834731849688,
      "loss": 3.7112,
      "step": 38300
    },
    {
      "epoch": 2.7982925005016694,
      "grad_norm": 0.05302992835640907,
      "learning_rate": 0.00013210944910616563,
      "loss": 3.7611,
      "step": 38350
    },
    {
      "epoch": 2.801941003703231,
      "grad_norm": 0.032847460359334946,
      "learning_rate": 0.00013189055089383436,
      "loss": 3.6865,
      "step": 38400
    },
    {
      "epoch": 2.8055895069047923,
      "grad_norm": 0.0754854679107666,
      "learning_rate": 0.0001316716526815031,
      "loss": 3.7481,
      "step": 38450
    },
    {
      "epoch": 2.8092380101063537,
      "grad_norm": 0.01979217492043972,
      "learning_rate": 0.00013145275446917182,
      "loss": 3.7691,
      "step": 38500
    },
    {
      "epoch": 2.8128865133079155,
      "grad_norm": 0.07206620275974274,
      "learning_rate": 0.00013123385625684055,
      "loss": 3.7562,
      "step": 38550
    },
    {
      "epoch": 2.816535016509477,
      "grad_norm": 0.049865081906318665,
      "learning_rate": 0.0001310149580445093,
      "loss": 3.7097,
      "step": 38600
    },
    {
      "epoch": 2.8201835197110388,
      "grad_norm": 0.05273963138461113,
      "learning_rate": 0.00013079605983217803,
      "loss": 3.6541,
      "step": 38650
    },
    {
      "epoch": 2.8238320229126,
      "grad_norm": 0.031025083735585213,
      "learning_rate": 0.00013057716161984676,
      "loss": 3.7536,
      "step": 38700
    },
    {
      "epoch": 2.8274805261141616,
      "grad_norm": 0.1409783810377121,
      "learning_rate": 0.0001303582634075155,
      "loss": 3.7314,
      "step": 38750
    },
    {
      "epoch": 2.831129029315723,
      "grad_norm": 0.028031740337610245,
      "learning_rate": 0.00013013936519518424,
      "loss": 3.734,
      "step": 38800
    },
    {
      "epoch": 2.834777532517285,
      "grad_norm": 0.02518472634255886,
      "learning_rate": 0.00012992046698285294,
      "loss": 3.7739,
      "step": 38850
    },
    {
      "epoch": 2.838426035718846,
      "grad_norm": 0.09872563183307648,
      "learning_rate": 0.0001297015687705217,
      "loss": 3.698,
      "step": 38900
    },
    {
      "epoch": 2.842074538920408,
      "grad_norm": 0.042347103357315063,
      "learning_rate": 0.00012948267055819043,
      "loss": 3.645,
      "step": 38950
    },
    {
      "epoch": 2.8457230421219695,
      "grad_norm": 0.03307225555181503,
      "learning_rate": 0.00012926377234585916,
      "loss": 3.6903,
      "step": 39000
    },
    {
      "epoch": 2.849371545323531,
      "grad_norm": 0.024017363786697388,
      "learning_rate": 0.0001290448741335279,
      "loss": 3.6955,
      "step": 39050
    },
    {
      "epoch": 2.8530200485250927,
      "grad_norm": 0.03932500258088112,
      "learning_rate": 0.00012882597592119664,
      "loss": 3.679,
      "step": 39100
    },
    {
      "epoch": 2.856668551726654,
      "grad_norm": 0.04016817361116409,
      "learning_rate": 0.00012860707770886537,
      "loss": 3.7343,
      "step": 39150
    },
    {
      "epoch": 2.860317054928216,
      "grad_norm": 0.07043774425983429,
      "learning_rate": 0.0001283881794965341,
      "loss": 3.7314,
      "step": 39200
    },
    {
      "epoch": 2.8639655581297774,
      "grad_norm": 0.03345401585102081,
      "learning_rate": 0.00012816928128420282,
      "loss": 3.6848,
      "step": 39250
    },
    {
      "epoch": 2.867614061331339,
      "grad_norm": 0.02726886421442032,
      "learning_rate": 0.00012795038307187158,
      "loss": 3.6872,
      "step": 39300
    },
    {
      "epoch": 2.8712625645329,
      "grad_norm": 0.03168681263923645,
      "learning_rate": 0.0001277314848595403,
      "loss": 3.7054,
      "step": 39350
    },
    {
      "epoch": 2.874911067734462,
      "grad_norm": 0.031363267451524734,
      "learning_rate": 0.00012751258664720904,
      "loss": 3.7832,
      "step": 39400
    },
    {
      "epoch": 2.8785595709360234,
      "grad_norm": 0.021511469036340714,
      "learning_rate": 0.00012729368843487776,
      "loss": 3.7486,
      "step": 39450
    },
    {
      "epoch": 2.8822080741375853,
      "grad_norm": 0.020452752709388733,
      "learning_rate": 0.00012707479022254652,
      "loss": 3.6738,
      "step": 39500
    },
    {
      "epoch": 2.8858565773391467,
      "grad_norm": 0.032279111444950104,
      "learning_rate": 0.00012685589201021522,
      "loss": 3.753,
      "step": 39550
    },
    {
      "epoch": 2.889505080540708,
      "grad_norm": 0.02837917022407055,
      "learning_rate": 0.00012663699379788398,
      "loss": 3.734,
      "step": 39600
    },
    {
      "epoch": 2.8931535837422695,
      "grad_norm": 0.032453425228595734,
      "learning_rate": 0.0001264180955855527,
      "loss": 3.7037,
      "step": 39650
    },
    {
      "epoch": 2.8968020869438313,
      "grad_norm": 0.03850550577044487,
      "learning_rate": 0.00012619919737322143,
      "loss": 3.7434,
      "step": 39700
    },
    {
      "epoch": 2.9004505901453927,
      "grad_norm": 0.12611661851406097,
      "learning_rate": 0.0001259802991608902,
      "loss": 3.7221,
      "step": 39750
    },
    {
      "epoch": 2.9040990933469546,
      "grad_norm": 0.03372563049197197,
      "learning_rate": 0.00012576140094855892,
      "loss": 3.6587,
      "step": 39800
    },
    {
      "epoch": 2.907747596548516,
      "grad_norm": 0.035110775381326675,
      "learning_rate": 0.00012554250273622765,
      "loss": 3.6911,
      "step": 39850
    },
    {
      "epoch": 2.9113960997500774,
      "grad_norm": 0.03349940478801727,
      "learning_rate": 0.00012532360452389637,
      "loss": 3.6748,
      "step": 39900
    },
    {
      "epoch": 2.9150446029516393,
      "grad_norm": 0.12479885667562485,
      "learning_rate": 0.0001251047063115651,
      "loss": 3.6445,
      "step": 39950
    },
    {
      "epoch": 2.9186931061532007,
      "grad_norm": 0.023646553978323936,
      "learning_rate": 0.00012488580809923386,
      "loss": 3.6881,
      "step": 40000
    },
    {
      "epoch": 2.922341609354762,
      "grad_norm": 0.1044984683394432,
      "learning_rate": 0.0001246669098869026,
      "loss": 3.7042,
      "step": 40050
    },
    {
      "epoch": 2.925990112556324,
      "grad_norm": 0.02609270066022873,
      "learning_rate": 0.00012444801167457132,
      "loss": 3.6732,
      "step": 40100
    },
    {
      "epoch": 2.9296386157578853,
      "grad_norm": 0.10261231660842896,
      "learning_rate": 0.00012422911346224004,
      "loss": 3.6908,
      "step": 40150
    },
    {
      "epoch": 2.9332871189594467,
      "grad_norm": 0.041669655591249466,
      "learning_rate": 0.00012401021524990877,
      "loss": 3.7803,
      "step": 40200
    },
    {
      "epoch": 2.9369356221610086,
      "grad_norm": 0.0250164233148098,
      "learning_rate": 0.0001237913170375775,
      "loss": 3.668,
      "step": 40250
    },
    {
      "epoch": 2.94058412536257,
      "grad_norm": 0.02433069236576557,
      "learning_rate": 0.00012357241882524626,
      "loss": 3.7252,
      "step": 40300
    },
    {
      "epoch": 2.944232628564132,
      "grad_norm": 0.06691361963748932,
      "learning_rate": 0.00012335352061291498,
      "loss": 3.6898,
      "step": 40350
    },
    {
      "epoch": 2.947881131765693,
      "grad_norm": 0.01993379555642605,
      "learning_rate": 0.0001231346224005837,
      "loss": 3.7432,
      "step": 40400
    },
    {
      "epoch": 2.9515296349672546,
      "grad_norm": 0.02024148590862751,
      "learning_rate": 0.00012291572418825247,
      "loss": 3.698,
      "step": 40450
    },
    {
      "epoch": 2.955178138168816,
      "grad_norm": 0.02942551113665104,
      "learning_rate": 0.00012269682597592117,
      "loss": 3.7958,
      "step": 40500
    },
    {
      "epoch": 2.958826641370378,
      "grad_norm": 0.0849180594086647,
      "learning_rate": 0.00012247792776358992,
      "loss": 3.7242,
      "step": 40550
    },
    {
      "epoch": 2.9624751445719393,
      "grad_norm": 0.032291311770677567,
      "learning_rate": 0.00012225902955125865,
      "loss": 3.7363,
      "step": 40600
    },
    {
      "epoch": 2.966123647773501,
      "grad_norm": 0.05430983379483223,
      "learning_rate": 0.0001220401313389274,
      "loss": 3.6713,
      "step": 40650
    },
    {
      "epoch": 2.9697721509750625,
      "grad_norm": 0.046220194548368454,
      "learning_rate": 0.00012182123312659612,
      "loss": 3.7509,
      "step": 40700
    },
    {
      "epoch": 2.973420654176624,
      "grad_norm": 0.055941514670848846,
      "learning_rate": 0.00012160233491426485,
      "loss": 3.6793,
      "step": 40750
    },
    {
      "epoch": 2.9770691573781853,
      "grad_norm": 0.02985675446689129,
      "learning_rate": 0.0001213834367019336,
      "loss": 3.7244,
      "step": 40800
    },
    {
      "epoch": 2.980717660579747,
      "grad_norm": 0.2887722849845886,
      "learning_rate": 0.00012116453848960232,
      "loss": 3.6864,
      "step": 40850
    },
    {
      "epoch": 2.9843661637813086,
      "grad_norm": 0.05255201458930969,
      "learning_rate": 0.00012094564027727106,
      "loss": 3.7051,
      "step": 40900
    },
    {
      "epoch": 2.9880146669828704,
      "grad_norm": 0.022182676941156387,
      "learning_rate": 0.00012072674206493978,
      "loss": 3.6758,
      "step": 40950
    },
    {
      "epoch": 2.991663170184432,
      "grad_norm": 0.02200893871486187,
      "learning_rate": 0.00012050784385260852,
      "loss": 3.6783,
      "step": 41000
    },
    {
      "epoch": 2.9953116733859932,
      "grad_norm": 0.027584685012698174,
      "learning_rate": 0.00012028894564027726,
      "loss": 3.7311,
      "step": 41050
    },
    {
      "epoch": 2.998960176587555,
      "grad_norm": 0.029547547921538353,
      "learning_rate": 0.00012007004742794599,
      "loss": 3.6855,
      "step": 41100
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.5946733951568604,
      "eval_runtime": 3.1599,
      "eval_samples_per_second": 31.647,
      "eval_steps_per_second": 4.114,
      "step": 41115
    },
    {
      "epoch": 3.002553952241093,
      "grad_norm": 0.1398307979106903,
      "learning_rate": 0.00011985114921561473,
      "loss": 3.6511,
      "step": 41150
    },
    {
      "epoch": 3.0062024554426547,
      "grad_norm": 0.017661258578300476,
      "learning_rate": 0.00011963225100328346,
      "loss": 3.6962,
      "step": 41200
    },
    {
      "epoch": 3.009850958644216,
      "grad_norm": 0.03905803710222244,
      "learning_rate": 0.0001194133527909522,
      "loss": 3.695,
      "step": 41250
    },
    {
      "epoch": 3.0134994618457775,
      "grad_norm": 0.08431414514780045,
      "learning_rate": 0.00011919445457862092,
      "loss": 3.6881,
      "step": 41300
    },
    {
      "epoch": 3.0171479650473394,
      "grad_norm": 0.024833589792251587,
      "learning_rate": 0.00011897555636628966,
      "loss": 3.7077,
      "step": 41350
    },
    {
      "epoch": 3.020796468248901,
      "grad_norm": 0.10292433202266693,
      "learning_rate": 0.0001187566581539584,
      "loss": 3.7387,
      "step": 41400
    },
    {
      "epoch": 3.0244449714504626,
      "grad_norm": 0.041727419942617416,
      "learning_rate": 0.00011853775994162713,
      "loss": 3.7382,
      "step": 41450
    },
    {
      "epoch": 3.028093474652024,
      "grad_norm": 0.028595682233572006,
      "learning_rate": 0.00011831886172929587,
      "loss": 3.6768,
      "step": 41500
    },
    {
      "epoch": 3.0317419778535855,
      "grad_norm": 0.026025822386145592,
      "learning_rate": 0.0001180999635169646,
      "loss": 3.6578,
      "step": 41550
    },
    {
      "epoch": 3.0353904810551473,
      "grad_norm": 0.027237294241786003,
      "learning_rate": 0.00011788106530463334,
      "loss": 3.7066,
      "step": 41600
    },
    {
      "epoch": 3.0390389842567087,
      "grad_norm": 0.02790343016386032,
      "learning_rate": 0.00011766216709230206,
      "loss": 3.7083,
      "step": 41650
    },
    {
      "epoch": 3.04268748745827,
      "grad_norm": 0.036800891160964966,
      "learning_rate": 0.0001174432688799708,
      "loss": 3.6508,
      "step": 41700
    },
    {
      "epoch": 3.046335990659832,
      "grad_norm": 0.12350871413946152,
      "learning_rate": 0.00011722437066763954,
      "loss": 3.7359,
      "step": 41750
    },
    {
      "epoch": 3.0499844938613934,
      "grad_norm": 0.026959339156746864,
      "learning_rate": 0.00011700547245530827,
      "loss": 3.7269,
      "step": 41800
    },
    {
      "epoch": 3.0536329970629548,
      "grad_norm": 0.04697912186384201,
      "learning_rate": 0.00011678657424297701,
      "loss": 3.6787,
      "step": 41850
    },
    {
      "epoch": 3.0572815002645166,
      "grad_norm": 0.02945401705801487,
      "learning_rate": 0.00011656767603064574,
      "loss": 3.6728,
      "step": 41900
    },
    {
      "epoch": 3.060930003466078,
      "grad_norm": 0.03150872513651848,
      "learning_rate": 0.00011634877781831448,
      "loss": 3.6949,
      "step": 41950
    },
    {
      "epoch": 3.0645785066676394,
      "grad_norm": 0.16563352942466736,
      "learning_rate": 0.0001161298796059832,
      "loss": 3.696,
      "step": 42000
    },
    {
      "epoch": 3.0682270098692013,
      "grad_norm": 0.06631025671958923,
      "learning_rate": 0.00011591098139365194,
      "loss": 3.7031,
      "step": 42050
    },
    {
      "epoch": 3.0718755130707627,
      "grad_norm": 0.019297271966934204,
      "learning_rate": 0.00011569208318132068,
      "loss": 3.7017,
      "step": 42100
    },
    {
      "epoch": 3.075524016272324,
      "grad_norm": 0.037817955017089844,
      "learning_rate": 0.00011547318496898941,
      "loss": 3.6798,
      "step": 42150
    },
    {
      "epoch": 3.079172519473886,
      "grad_norm": 0.03747722879052162,
      "learning_rate": 0.00011525428675665815,
      "loss": 3.6298,
      "step": 42200
    },
    {
      "epoch": 3.0828210226754473,
      "grad_norm": 0.021603360772132874,
      "learning_rate": 0.00011503538854432688,
      "loss": 3.6982,
      "step": 42250
    },
    {
      "epoch": 3.0864695258770087,
      "grad_norm": 0.03301504999399185,
      "learning_rate": 0.00011481649033199561,
      "loss": 3.6696,
      "step": 42300
    },
    {
      "epoch": 3.0901180290785706,
      "grad_norm": 0.04821614921092987,
      "learning_rate": 0.00011459759211966434,
      "loss": 3.6952,
      "step": 42350
    },
    {
      "epoch": 3.093766532280132,
      "grad_norm": 0.0381164588034153,
      "learning_rate": 0.00011437869390733308,
      "loss": 3.6625,
      "step": 42400
    },
    {
      "epoch": 3.097415035481694,
      "grad_norm": 0.0352042131125927,
      "learning_rate": 0.00011415979569500182,
      "loss": 3.708,
      "step": 42450
    },
    {
      "epoch": 3.1010635386832552,
      "grad_norm": 0.035634856671094894,
      "learning_rate": 0.00011394089748267055,
      "loss": 3.6981,
      "step": 42500
    },
    {
      "epoch": 3.1047120418848166,
      "grad_norm": 0.026631182059645653,
      "learning_rate": 0.00011372199927033929,
      "loss": 3.7077,
      "step": 42550
    },
    {
      "epoch": 3.1083605450863785,
      "grad_norm": 0.045946672558784485,
      "learning_rate": 0.000113503101058008,
      "loss": 3.6668,
      "step": 42600
    },
    {
      "epoch": 3.11200904828794,
      "grad_norm": 0.04426262155175209,
      "learning_rate": 0.00011328420284567675,
      "loss": 3.7465,
      "step": 42650
    },
    {
      "epoch": 3.1156575514895013,
      "grad_norm": 0.02538454532623291,
      "learning_rate": 0.00011306530463334548,
      "loss": 3.7257,
      "step": 42700
    },
    {
      "epoch": 3.119306054691063,
      "grad_norm": 0.04299815371632576,
      "learning_rate": 0.00011284640642101422,
      "loss": 3.7617,
      "step": 42750
    },
    {
      "epoch": 3.1229545578926245,
      "grad_norm": 0.038702357560396194,
      "learning_rate": 0.00011262750820868296,
      "loss": 3.7489,
      "step": 42800
    },
    {
      "epoch": 3.126603061094186,
      "grad_norm": 0.027142029255628586,
      "learning_rate": 0.00011240860999635169,
      "loss": 3.7278,
      "step": 42850
    },
    {
      "epoch": 3.130251564295748,
      "grad_norm": 0.02280390076339245,
      "learning_rate": 0.00011218971178402043,
      "loss": 3.652,
      "step": 42900
    },
    {
      "epoch": 3.133900067497309,
      "grad_norm": 0.0307513065636158,
      "learning_rate": 0.00011197081357168914,
      "loss": 3.6616,
      "step": 42950
    },
    {
      "epoch": 3.1375485706988706,
      "grad_norm": 0.02757774479687214,
      "learning_rate": 0.00011175191535935789,
      "loss": 3.7297,
      "step": 43000
    },
    {
      "epoch": 3.1411970739004325,
      "grad_norm": 0.02984207682311535,
      "learning_rate": 0.00011153301714702661,
      "loss": 3.7418,
      "step": 43050
    },
    {
      "epoch": 3.144845577101994,
      "grad_norm": 0.018845688551664352,
      "learning_rate": 0.00011131411893469536,
      "loss": 3.7405,
      "step": 43100
    },
    {
      "epoch": 3.1484940803035553,
      "grad_norm": 0.029787659645080566,
      "learning_rate": 0.0001110952207223641,
      "loss": 3.7294,
      "step": 43150
    },
    {
      "epoch": 3.152142583505117,
      "grad_norm": 0.07815461605787277,
      "learning_rate": 0.00011087632251003283,
      "loss": 3.6595,
      "step": 43200
    },
    {
      "epoch": 3.1557910867066785,
      "grad_norm": 0.03553677722811699,
      "learning_rate": 0.00011065742429770157,
      "loss": 3.6753,
      "step": 43250
    },
    {
      "epoch": 3.1594395899082404,
      "grad_norm": 0.4940904974937439,
      "learning_rate": 0.00011043852608537028,
      "loss": 3.6616,
      "step": 43300
    },
    {
      "epoch": 3.1630880931098018,
      "grad_norm": 0.03782907873392105,
      "learning_rate": 0.00011021962787303903,
      "loss": 3.6828,
      "step": 43350
    },
    {
      "epoch": 3.166736596311363,
      "grad_norm": 0.0539306104183197,
      "learning_rate": 0.00011000072966070775,
      "loss": 3.739,
      "step": 43400
    },
    {
      "epoch": 3.170385099512925,
      "grad_norm": 0.029075879603624344,
      "learning_rate": 0.0001097818314483765,
      "loss": 3.6932,
      "step": 43450
    },
    {
      "epoch": 3.1740336027144864,
      "grad_norm": 0.041627150028944016,
      "learning_rate": 0.00010956293323604524,
      "loss": 3.6881,
      "step": 43500
    },
    {
      "epoch": 3.177682105916048,
      "grad_norm": 0.0435229055583477,
      "learning_rate": 0.00010934403502371397,
      "loss": 3.7384,
      "step": 43550
    },
    {
      "epoch": 3.1813306091176097,
      "grad_norm": 0.20391543209552765,
      "learning_rate": 0.00010912513681138271,
      "loss": 3.6809,
      "step": 43600
    },
    {
      "epoch": 3.184979112319171,
      "grad_norm": 0.0384707972407341,
      "learning_rate": 0.00010890623859905142,
      "loss": 3.7085,
      "step": 43650
    },
    {
      "epoch": 3.1886276155207325,
      "grad_norm": 0.06236378848552704,
      "learning_rate": 0.00010868734038672016,
      "loss": 3.7103,
      "step": 43700
    },
    {
      "epoch": 3.1922761187222943,
      "grad_norm": 0.022503109648823738,
      "learning_rate": 0.00010846844217438889,
      "loss": 3.7221,
      "step": 43750
    },
    {
      "epoch": 3.1959246219238557,
      "grad_norm": 0.022441739216446877,
      "learning_rate": 0.00010824954396205763,
      "loss": 3.7036,
      "step": 43800
    },
    {
      "epoch": 3.199573125125417,
      "grad_norm": 0.05081986263394356,
      "learning_rate": 0.00010803064574972638,
      "loss": 3.6975,
      "step": 43850
    },
    {
      "epoch": 3.203221628326979,
      "grad_norm": 0.041120633482933044,
      "learning_rate": 0.0001078117475373951,
      "loss": 3.7634,
      "step": 43900
    },
    {
      "epoch": 3.2068701315285404,
      "grad_norm": 0.025425288826227188,
      "learning_rate": 0.00010759284932506383,
      "loss": 3.6903,
      "step": 43950
    },
    {
      "epoch": 3.210518634730102,
      "grad_norm": 0.11018933355808258,
      "learning_rate": 0.00010737395111273256,
      "loss": 3.6558,
      "step": 44000
    },
    {
      "epoch": 3.2141671379316636,
      "grad_norm": 0.041482746601104736,
      "learning_rate": 0.0001071550529004013,
      "loss": 3.6269,
      "step": 44050
    },
    {
      "epoch": 3.217815641133225,
      "grad_norm": 0.026078224182128906,
      "learning_rate": 0.00010693615468807003,
      "loss": 3.6672,
      "step": 44100
    },
    {
      "epoch": 3.2214641443347865,
      "grad_norm": 0.11935616284608841,
      "learning_rate": 0.00010671725647573877,
      "loss": 3.72,
      "step": 44150
    },
    {
      "epoch": 3.2251126475363483,
      "grad_norm": 0.04434196278452873,
      "learning_rate": 0.00010649835826340752,
      "loss": 3.7512,
      "step": 44200
    },
    {
      "epoch": 3.2287611507379097,
      "grad_norm": 0.033501286059617996,
      "learning_rate": 0.00010627946005107623,
      "loss": 3.7251,
      "step": 44250
    },
    {
      "epoch": 3.232409653939471,
      "grad_norm": 0.06466783583164215,
      "learning_rate": 0.00010606056183874497,
      "loss": 3.7245,
      "step": 44300
    },
    {
      "epoch": 3.236058157141033,
      "grad_norm": 0.038313280791044235,
      "learning_rate": 0.0001058416636264137,
      "loss": 3.7216,
      "step": 44350
    },
    {
      "epoch": 3.2397066603425944,
      "grad_norm": 0.027473224326968193,
      "learning_rate": 0.00010562276541408244,
      "loss": 3.7101,
      "step": 44400
    },
    {
      "epoch": 3.243355163544156,
      "grad_norm": 0.023412611335515976,
      "learning_rate": 0.00010540386720175117,
      "loss": 3.6746,
      "step": 44450
    },
    {
      "epoch": 3.2470036667457176,
      "grad_norm": 0.021121077239513397,
      "learning_rate": 0.00010518496898941991,
      "loss": 3.711,
      "step": 44500
    },
    {
      "epoch": 3.250652169947279,
      "grad_norm": 0.03374868631362915,
      "learning_rate": 0.00010496607077708866,
      "loss": 3.7107,
      "step": 44550
    },
    {
      "epoch": 3.254300673148841,
      "grad_norm": 0.13712862133979797,
      "learning_rate": 0.00010474717256475737,
      "loss": 3.7185,
      "step": 44600
    },
    {
      "epoch": 3.2579491763504023,
      "grad_norm": 0.022400949150323868,
      "learning_rate": 0.00010452827435242611,
      "loss": 3.7015,
      "step": 44650
    },
    {
      "epoch": 3.2615976795519637,
      "grad_norm": 0.07560990750789642,
      "learning_rate": 0.00010430937614009484,
      "loss": 3.7402,
      "step": 44700
    },
    {
      "epoch": 3.2652461827535255,
      "grad_norm": 0.028296872973442078,
      "learning_rate": 0.00010409047792776358,
      "loss": 3.6943,
      "step": 44750
    },
    {
      "epoch": 3.268894685955087,
      "grad_norm": 0.09032832086086273,
      "learning_rate": 0.00010387157971543231,
      "loss": 3.7005,
      "step": 44800
    },
    {
      "epoch": 3.2725431891566483,
      "grad_norm": 0.0687030777335167,
      "learning_rate": 0.00010365268150310105,
      "loss": 3.7236,
      "step": 44850
    },
    {
      "epoch": 3.27619169235821,
      "grad_norm": 0.061129286885261536,
      "learning_rate": 0.0001034337832907698,
      "loss": 3.7095,
      "step": 44900
    },
    {
      "epoch": 3.2798401955597716,
      "grad_norm": 0.027490217238664627,
      "learning_rate": 0.00010321488507843851,
      "loss": 3.7004,
      "step": 44950
    },
    {
      "epoch": 3.283488698761333,
      "grad_norm": 0.02590196579694748,
      "learning_rate": 0.00010299598686610725,
      "loss": 3.7399,
      "step": 45000
    },
    {
      "epoch": 3.287137201962895,
      "grad_norm": 0.04352841526269913,
      "learning_rate": 0.00010277708865377598,
      "loss": 3.7172,
      "step": 45050
    },
    {
      "epoch": 3.2907857051644562,
      "grad_norm": 0.03865779563784599,
      "learning_rate": 0.00010255819044144472,
      "loss": 3.6956,
      "step": 45100
    },
    {
      "epoch": 3.2944342083660176,
      "grad_norm": 0.03363928571343422,
      "learning_rate": 0.00010233929222911345,
      "loss": 3.7352,
      "step": 45150
    },
    {
      "epoch": 3.2980827115675795,
      "grad_norm": 0.03046591393649578,
      "learning_rate": 0.00010212039401678219,
      "loss": 3.7322,
      "step": 45200
    },
    {
      "epoch": 3.301731214769141,
      "grad_norm": 0.0312674380838871,
      "learning_rate": 0.00010190149580445093,
      "loss": 3.7443,
      "step": 45250
    },
    {
      "epoch": 3.3053797179707027,
      "grad_norm": 0.03694514185190201,
      "learning_rate": 0.00010168259759211965,
      "loss": 3.6827,
      "step": 45300
    },
    {
      "epoch": 3.309028221172264,
      "grad_norm": 0.03328113630414009,
      "learning_rate": 0.00010146369937978839,
      "loss": 3.6751,
      "step": 45350
    },
    {
      "epoch": 3.3126767243738255,
      "grad_norm": 0.016432128846645355,
      "learning_rate": 0.00010124480116745712,
      "loss": 3.7216,
      "step": 45400
    },
    {
      "epoch": 3.3163252275753874,
      "grad_norm": 0.027846502140164375,
      "learning_rate": 0.00010102590295512586,
      "loss": 3.6994,
      "step": 45450
    },
    {
      "epoch": 3.319973730776949,
      "grad_norm": 0.023841219022870064,
      "learning_rate": 0.00010080700474279459,
      "loss": 3.725,
      "step": 45500
    },
    {
      "epoch": 3.32362223397851,
      "grad_norm": 0.02561493217945099,
      "learning_rate": 0.00010058810653046333,
      "loss": 3.7359,
      "step": 45550
    },
    {
      "epoch": 3.327270737180072,
      "grad_norm": 0.03507746011018753,
      "learning_rate": 0.00010036920831813206,
      "loss": 3.6684,
      "step": 45600
    },
    {
      "epoch": 3.3309192403816335,
      "grad_norm": 0.20485739409923553,
      "learning_rate": 0.00010015031010580079,
      "loss": 3.7113,
      "step": 45650
    },
    {
      "epoch": 3.334567743583195,
      "grad_norm": 0.03801550343632698,
      "learning_rate": 9.993141189346953e-05,
      "loss": 3.6631,
      "step": 45700
    },
    {
      "epoch": 3.3382162467847567,
      "grad_norm": 0.03688914328813553,
      "learning_rate": 9.971251368113826e-05,
      "loss": 3.7332,
      "step": 45750
    },
    {
      "epoch": 3.341864749986318,
      "grad_norm": 0.026838721707463264,
      "learning_rate": 9.9493615468807e-05,
      "loss": 3.7195,
      "step": 45800
    },
    {
      "epoch": 3.3455132531878795,
      "grad_norm": 0.0339033342897892,
      "learning_rate": 9.927471725647573e-05,
      "loss": 3.7295,
      "step": 45850
    },
    {
      "epoch": 3.3491617563894414,
      "grad_norm": 0.025340687483549118,
      "learning_rate": 9.905581904414446e-05,
      "loss": 3.6998,
      "step": 45900
    },
    {
      "epoch": 3.3528102595910028,
      "grad_norm": 0.025006979703903198,
      "learning_rate": 9.88369208318132e-05,
      "loss": 3.7303,
      "step": 45950
    },
    {
      "epoch": 3.356458762792564,
      "grad_norm": 0.03996865078806877,
      "learning_rate": 9.861802261948193e-05,
      "loss": 3.7271,
      "step": 46000
    },
    {
      "epoch": 3.360107265994126,
      "grad_norm": 0.04868055507540703,
      "learning_rate": 9.839912440715067e-05,
      "loss": 3.6449,
      "step": 46050
    },
    {
      "epoch": 3.3637557691956874,
      "grad_norm": 0.03104194439947605,
      "learning_rate": 9.81802261948194e-05,
      "loss": 3.6776,
      "step": 46100
    },
    {
      "epoch": 3.3674042723972493,
      "grad_norm": 0.019142840057611465,
      "learning_rate": 9.796132798248814e-05,
      "loss": 3.624,
      "step": 46150
    },
    {
      "epoch": 3.3710527755988107,
      "grad_norm": 0.025647064670920372,
      "learning_rate": 9.774242977015685e-05,
      "loss": 3.6913,
      "step": 46200
    },
    {
      "epoch": 3.374701278800372,
      "grad_norm": 0.029737964272499084,
      "learning_rate": 9.75235315578256e-05,
      "loss": 3.7066,
      "step": 46250
    },
    {
      "epoch": 3.3783497820019335,
      "grad_norm": 0.02848462387919426,
      "learning_rate": 9.730463334549434e-05,
      "loss": 3.6572,
      "step": 46300
    },
    {
      "epoch": 3.3819982852034953,
      "grad_norm": 0.5196526646614075,
      "learning_rate": 9.708573513316307e-05,
      "loss": 3.6848,
      "step": 46350
    },
    {
      "epoch": 3.3856467884050567,
      "grad_norm": 0.0281276423484087,
      "learning_rate": 9.686683692083181e-05,
      "loss": 3.701,
      "step": 46400
    },
    {
      "epoch": 3.3892952916066186,
      "grad_norm": 0.02584659680724144,
      "learning_rate": 9.664793870850054e-05,
      "loss": 3.7233,
      "step": 46450
    },
    {
      "epoch": 3.39294379480818,
      "grad_norm": 0.022898655384778976,
      "learning_rate": 9.642904049616928e-05,
      "loss": 3.7189,
      "step": 46500
    },
    {
      "epoch": 3.3965922980097414,
      "grad_norm": 0.08871437609195709,
      "learning_rate": 9.6210142283838e-05,
      "loss": 3.7144,
      "step": 46550
    },
    {
      "epoch": 3.4002408012113032,
      "grad_norm": 0.02510860562324524,
      "learning_rate": 9.599124407150674e-05,
      "loss": 3.7233,
      "step": 46600
    },
    {
      "epoch": 3.4038893044128646,
      "grad_norm": 0.22198431193828583,
      "learning_rate": 9.577234585917548e-05,
      "loss": 3.6894,
      "step": 46650
    },
    {
      "epoch": 3.407537807614426,
      "grad_norm": 0.02964935451745987,
      "learning_rate": 9.55534476468442e-05,
      "loss": 3.7155,
      "step": 46700
    },
    {
      "epoch": 3.411186310815988,
      "grad_norm": 0.05090668424963951,
      "learning_rate": 9.533454943451295e-05,
      "loss": 3.7324,
      "step": 46750
    },
    {
      "epoch": 3.4148348140175493,
      "grad_norm": 0.026686036959290504,
      "learning_rate": 9.511565122218168e-05,
      "loss": 3.6856,
      "step": 46800
    },
    {
      "epoch": 3.4184833172191107,
      "grad_norm": 0.02076634019613266,
      "learning_rate": 9.489675300985042e-05,
      "loss": 3.6839,
      "step": 46850
    },
    {
      "epoch": 3.4221318204206725,
      "grad_norm": 0.023595841601490974,
      "learning_rate": 9.467785479751913e-05,
      "loss": 3.6778,
      "step": 46900
    },
    {
      "epoch": 3.425780323622234,
      "grad_norm": 0.027150431647896767,
      "learning_rate": 9.445895658518787e-05,
      "loss": 3.7107,
      "step": 46950
    },
    {
      "epoch": 3.4294288268237954,
      "grad_norm": 0.040717028081417084,
      "learning_rate": 9.424005837285662e-05,
      "loss": 3.7229,
      "step": 47000
    },
    {
      "epoch": 3.433077330025357,
      "grad_norm": 0.023700419813394547,
      "learning_rate": 9.402116016052534e-05,
      "loss": 3.7529,
      "step": 47050
    },
    {
      "epoch": 3.4367258332269186,
      "grad_norm": 0.032426100224256516,
      "learning_rate": 9.380226194819409e-05,
      "loss": 3.6904,
      "step": 47100
    },
    {
      "epoch": 3.44037433642848,
      "grad_norm": 0.20116232335567474,
      "learning_rate": 9.358336373586282e-05,
      "loss": 3.7108,
      "step": 47150
    },
    {
      "epoch": 3.444022839630042,
      "grad_norm": 0.03484417125582695,
      "learning_rate": 9.336446552353156e-05,
      "loss": 3.6485,
      "step": 47200
    },
    {
      "epoch": 3.4476713428316033,
      "grad_norm": 0.03141117841005325,
      "learning_rate": 9.314556731120027e-05,
      "loss": 3.7012,
      "step": 47250
    },
    {
      "epoch": 3.451319846033165,
      "grad_norm": 0.022899538278579712,
      "learning_rate": 9.292666909886901e-05,
      "loss": 3.7119,
      "step": 47300
    },
    {
      "epoch": 3.4549683492347265,
      "grad_norm": 0.02678048238158226,
      "learning_rate": 9.270777088653776e-05,
      "loss": 3.7162,
      "step": 47350
    },
    {
      "epoch": 3.458616852436288,
      "grad_norm": 0.11422864347696304,
      "learning_rate": 9.248887267420648e-05,
      "loss": 3.7556,
      "step": 47400
    },
    {
      "epoch": 3.4622653556378498,
      "grad_norm": 0.1772831380367279,
      "learning_rate": 9.226997446187523e-05,
      "loss": 3.7319,
      "step": 47450
    },
    {
      "epoch": 3.465913858839411,
      "grad_norm": 0.025153912603855133,
      "learning_rate": 9.205107624954395e-05,
      "loss": 3.6744,
      "step": 47500
    },
    {
      "epoch": 3.4695623620409726,
      "grad_norm": 0.28335705399513245,
      "learning_rate": 9.183217803721268e-05,
      "loss": 3.6658,
      "step": 47550
    },
    {
      "epoch": 3.4732108652425344,
      "grad_norm": 0.05297570303082466,
      "learning_rate": 9.161327982488142e-05,
      "loss": 3.7441,
      "step": 47600
    },
    {
      "epoch": 3.476859368444096,
      "grad_norm": 0.02710406854748726,
      "learning_rate": 9.139438161255015e-05,
      "loss": 3.7284,
      "step": 47650
    },
    {
      "epoch": 3.4805078716456572,
      "grad_norm": 0.03446696326136589,
      "learning_rate": 9.11754834002189e-05,
      "loss": 3.7443,
      "step": 47700
    },
    {
      "epoch": 3.484156374847219,
      "grad_norm": 1.1016525030136108,
      "learning_rate": 9.095658518788762e-05,
      "loss": 3.6547,
      "step": 47750
    },
    {
      "epoch": 3.4878048780487805,
      "grad_norm": 0.04340174049139023,
      "learning_rate": 9.073768697555637e-05,
      "loss": 3.6545,
      "step": 47800
    },
    {
      "epoch": 3.491453381250342,
      "grad_norm": 0.03383385390043259,
      "learning_rate": 9.051878876322508e-05,
      "loss": 3.7234,
      "step": 47850
    },
    {
      "epoch": 3.4951018844519037,
      "grad_norm": 0.03018622286617756,
      "learning_rate": 9.029989055089382e-05,
      "loss": 3.677,
      "step": 47900
    },
    {
      "epoch": 3.498750387653465,
      "grad_norm": 0.024420050904154778,
      "learning_rate": 9.008099233856256e-05,
      "loss": 3.6394,
      "step": 47950
    },
    {
      "epoch": 3.5023988908550265,
      "grad_norm": 0.028807258233428,
      "learning_rate": 8.986209412623129e-05,
      "loss": 3.6607,
      "step": 48000
    },
    {
      "epoch": 3.5060473940565884,
      "grad_norm": 0.027551693841814995,
      "learning_rate": 8.964319591390003e-05,
      "loss": 3.6744,
      "step": 48050
    },
    {
      "epoch": 3.50969589725815,
      "grad_norm": 0.024356231093406677,
      "learning_rate": 8.942429770156876e-05,
      "loss": 3.6632,
      "step": 48100
    },
    {
      "epoch": 3.5133444004597116,
      "grad_norm": 0.023064976558089256,
      "learning_rate": 8.92053994892375e-05,
      "loss": 3.666,
      "step": 48150
    },
    {
      "epoch": 3.516992903661273,
      "grad_norm": 0.02324194647371769,
      "learning_rate": 8.898650127690622e-05,
      "loss": 3.7296,
      "step": 48200
    },
    {
      "epoch": 3.5206414068628344,
      "grad_norm": 0.04592389613389969,
      "learning_rate": 8.876760306457496e-05,
      "loss": 3.6702,
      "step": 48250
    },
    {
      "epoch": 3.524289910064396,
      "grad_norm": 0.025325313210487366,
      "learning_rate": 8.85487048522437e-05,
      "loss": 3.7011,
      "step": 48300
    },
    {
      "epoch": 3.5279384132659577,
      "grad_norm": 0.03852515295147896,
      "learning_rate": 8.832980663991243e-05,
      "loss": 3.6849,
      "step": 48350
    },
    {
      "epoch": 3.531586916467519,
      "grad_norm": 0.031379107385873795,
      "learning_rate": 8.811090842758117e-05,
      "loss": 3.7111,
      "step": 48400
    },
    {
      "epoch": 3.535235419669081,
      "grad_norm": 0.04855893552303314,
      "learning_rate": 8.78920102152499e-05,
      "loss": 3.7188,
      "step": 48450
    },
    {
      "epoch": 3.5388839228706424,
      "grad_norm": 0.07157552987337112,
      "learning_rate": 8.767311200291864e-05,
      "loss": 3.6694,
      "step": 48500
    },
    {
      "epoch": 3.5425324260722038,
      "grad_norm": 0.03843335434794426,
      "learning_rate": 8.745421379058736e-05,
      "loss": 3.7236,
      "step": 48550
    },
    {
      "epoch": 3.5461809292737656,
      "grad_norm": 0.19420544803142548,
      "learning_rate": 8.72353155782561e-05,
      "loss": 3.7147,
      "step": 48600
    },
    {
      "epoch": 3.549829432475327,
      "grad_norm": 0.025806473568081856,
      "learning_rate": 8.701641736592484e-05,
      "loss": 3.7158,
      "step": 48650
    },
    {
      "epoch": 3.5534779356768884,
      "grad_norm": 0.026413461193442345,
      "learning_rate": 8.679751915359357e-05,
      "loss": 3.7664,
      "step": 48700
    },
    {
      "epoch": 3.5571264388784503,
      "grad_norm": 0.022722482681274414,
      "learning_rate": 8.657862094126231e-05,
      "loss": 3.6303,
      "step": 48750
    },
    {
      "epoch": 3.5607749420800117,
      "grad_norm": 0.0215457770973444,
      "learning_rate": 8.635972272893104e-05,
      "loss": 3.7173,
      "step": 48800
    },
    {
      "epoch": 3.564423445281573,
      "grad_norm": 0.08398742228746414,
      "learning_rate": 8.614082451659978e-05,
      "loss": 3.7568,
      "step": 48850
    },
    {
      "epoch": 3.568071948483135,
      "grad_norm": 0.031901754438877106,
      "learning_rate": 8.59219263042685e-05,
      "loss": 3.6792,
      "step": 48900
    },
    {
      "epoch": 3.5717204516846963,
      "grad_norm": 0.061828721314668655,
      "learning_rate": 8.570302809193724e-05,
      "loss": 3.7131,
      "step": 48950
    },
    {
      "epoch": 3.575368954886258,
      "grad_norm": 0.022541409358382225,
      "learning_rate": 8.548412987960598e-05,
      "loss": 3.7502,
      "step": 49000
    },
    {
      "epoch": 3.5790174580878196,
      "grad_norm": 0.03168310225009918,
      "learning_rate": 8.526523166727471e-05,
      "loss": 3.7592,
      "step": 49050
    },
    {
      "epoch": 3.582665961289381,
      "grad_norm": 0.0248660147190094,
      "learning_rate": 8.504633345494345e-05,
      "loss": 3.6876,
      "step": 49100
    },
    {
      "epoch": 3.5863144644909424,
      "grad_norm": 0.0270144734531641,
      "learning_rate": 8.482743524261218e-05,
      "loss": 3.6938,
      "step": 49150
    },
    {
      "epoch": 3.5899629676925042,
      "grad_norm": 0.031041784211993217,
      "learning_rate": 8.460853703028091e-05,
      "loss": 3.6814,
      "step": 49200
    },
    {
      "epoch": 3.5936114708940656,
      "grad_norm": 0.032651063054800034,
      "learning_rate": 8.438963881794964e-05,
      "loss": 3.7169,
      "step": 49250
    },
    {
      "epoch": 3.5972599740956275,
      "grad_norm": 0.033376991748809814,
      "learning_rate": 8.417074060561838e-05,
      "loss": 3.6973,
      "step": 49300
    },
    {
      "epoch": 3.600908477297189,
      "grad_norm": 0.023224055767059326,
      "learning_rate": 8.395184239328712e-05,
      "loss": 3.7399,
      "step": 49350
    },
    {
      "epoch": 3.6045569804987503,
      "grad_norm": 0.023909494280815125,
      "learning_rate": 8.373294418095585e-05,
      "loss": 3.7651,
      "step": 49400
    },
    {
      "epoch": 3.6082054837003117,
      "grad_norm": 0.02739042229950428,
      "learning_rate": 8.351404596862459e-05,
      "loss": 3.7122,
      "step": 49450
    },
    {
      "epoch": 3.6118539869018735,
      "grad_norm": 0.024958444759249687,
      "learning_rate": 8.32951477562933e-05,
      "loss": 3.7089,
      "step": 49500
    },
    {
      "epoch": 3.615502490103435,
      "grad_norm": 0.03579426929354668,
      "learning_rate": 8.307624954396205e-05,
      "loss": 3.7026,
      "step": 49550
    },
    {
      "epoch": 3.619150993304997,
      "grad_norm": 0.27642250061035156,
      "learning_rate": 8.285735133163078e-05,
      "loss": 3.7413,
      "step": 49600
    },
    {
      "epoch": 3.622799496506558,
      "grad_norm": 0.02882041595876217,
      "learning_rate": 8.263845311929952e-05,
      "loss": 3.7198,
      "step": 49650
    },
    {
      "epoch": 3.6264479997081196,
      "grad_norm": 0.02665911242365837,
      "learning_rate": 8.241955490696826e-05,
      "loss": 3.6572,
      "step": 49700
    },
    {
      "epoch": 3.6300965029096814,
      "grad_norm": 0.045889467000961304,
      "learning_rate": 8.220065669463699e-05,
      "loss": 3.7342,
      "step": 49750
    },
    {
      "epoch": 3.633745006111243,
      "grad_norm": 0.047949474304914474,
      "learning_rate": 8.198175848230573e-05,
      "loss": 3.7236,
      "step": 49800
    },
    {
      "epoch": 3.6373935093128047,
      "grad_norm": 0.051668912172317505,
      "learning_rate": 8.176286026997445e-05,
      "loss": 3.7268,
      "step": 49850
    },
    {
      "epoch": 3.641042012514366,
      "grad_norm": 0.032801415771245956,
      "learning_rate": 8.154396205764319e-05,
      "loss": 3.748,
      "step": 49900
    },
    {
      "epoch": 3.6446905157159275,
      "grad_norm": 0.025623632594943047,
      "learning_rate": 8.132506384531192e-05,
      "loss": 3.6491,
      "step": 49950
    },
    {
      "epoch": 3.648339018917489,
      "grad_norm": 0.028906235471367836,
      "learning_rate": 8.110616563298066e-05,
      "loss": 3.691,
      "step": 50000
    },
    {
      "epoch": 3.6519875221190508,
      "grad_norm": 0.32500630617141724,
      "learning_rate": 8.08872674206494e-05,
      "loss": 3.6735,
      "step": 50050
    },
    {
      "epoch": 3.655636025320612,
      "grad_norm": 0.03761867806315422,
      "learning_rate": 8.066836920831813e-05,
      "loss": 3.731,
      "step": 50100
    },
    {
      "epoch": 3.659284528522174,
      "grad_norm": 0.02556339092552662,
      "learning_rate": 8.044947099598687e-05,
      "loss": 3.693,
      "step": 50150
    },
    {
      "epoch": 3.6629330317237354,
      "grad_norm": 0.051074083894491196,
      "learning_rate": 8.023057278365558e-05,
      "loss": 3.659,
      "step": 50200
    },
    {
      "epoch": 3.666581534925297,
      "grad_norm": 0.02689560502767563,
      "learning_rate": 8.001167457132433e-05,
      "loss": 3.7542,
      "step": 50250
    },
    {
      "epoch": 3.670230038126858,
      "grad_norm": 0.023187560960650444,
      "learning_rate": 7.979277635899305e-05,
      "loss": 3.761,
      "step": 50300
    },
    {
      "epoch": 3.67387854132842,
      "grad_norm": 0.06560627371072769,
      "learning_rate": 7.95738781466618e-05,
      "loss": 3.6942,
      "step": 50350
    },
    {
      "epoch": 3.6775270445299815,
      "grad_norm": 0.07906327396631241,
      "learning_rate": 7.935497993433054e-05,
      "loss": 3.6856,
      "step": 50400
    },
    {
      "epoch": 3.6811755477315433,
      "grad_norm": 0.025206981226801872,
      "learning_rate": 7.913608172199927e-05,
      "loss": 3.6824,
      "step": 50450
    },
    {
      "epoch": 3.6848240509331047,
      "grad_norm": 0.027369875460863113,
      "learning_rate": 7.891718350966801e-05,
      "loss": 3.6685,
      "step": 50500
    },
    {
      "epoch": 3.688472554134666,
      "grad_norm": 0.02703397534787655,
      "learning_rate": 7.869828529733672e-05,
      "loss": 3.7366,
      "step": 50550
    },
    {
      "epoch": 3.692121057336228,
      "grad_norm": 0.0264059379696846,
      "learning_rate": 7.847938708500547e-05,
      "loss": 3.717,
      "step": 50600
    },
    {
      "epoch": 3.6957695605377894,
      "grad_norm": 0.018987664952874184,
      "learning_rate": 7.82604888726742e-05,
      "loss": 3.6637,
      "step": 50650
    },
    {
      "epoch": 3.699418063739351,
      "grad_norm": 0.03389415517449379,
      "learning_rate": 7.804159066034294e-05,
      "loss": 3.7299,
      "step": 50700
    },
    {
      "epoch": 3.7030665669409126,
      "grad_norm": 0.03803165629506111,
      "learning_rate": 7.782269244801168e-05,
      "loss": 3.6925,
      "step": 50750
    },
    {
      "epoch": 3.706715070142474,
      "grad_norm": 0.20754849910736084,
      "learning_rate": 7.76037942356804e-05,
      "loss": 3.7101,
      "step": 50800
    },
    {
      "epoch": 3.7103635733440354,
      "grad_norm": 0.025824204087257385,
      "learning_rate": 7.738489602334913e-05,
      "loss": 3.6263,
      "step": 50850
    },
    {
      "epoch": 3.7140120765455973,
      "grad_norm": 0.024063143879175186,
      "learning_rate": 7.716599781101786e-05,
      "loss": 3.6851,
      "step": 50900
    },
    {
      "epoch": 3.7176605797471587,
      "grad_norm": 0.026667242869734764,
      "learning_rate": 7.69470995986866e-05,
      "loss": 3.7039,
      "step": 50950
    },
    {
      "epoch": 3.7213090829487205,
      "grad_norm": 0.03621957078576088,
      "learning_rate": 7.672820138635533e-05,
      "loss": 3.7523,
      "step": 51000
    },
    {
      "epoch": 3.724957586150282,
      "grad_norm": 0.02640089951455593,
      "learning_rate": 7.650930317402408e-05,
      "loss": 3.6515,
      "step": 51050
    },
    {
      "epoch": 3.7286060893518433,
      "grad_norm": 0.06223282590508461,
      "learning_rate": 7.629040496169282e-05,
      "loss": 3.7224,
      "step": 51100
    },
    {
      "epoch": 3.7322545925534047,
      "grad_norm": 0.048359788954257965,
      "learning_rate": 7.607150674936153e-05,
      "loss": 3.6937,
      "step": 51150
    },
    {
      "epoch": 3.7359030957549666,
      "grad_norm": 0.07677100598812103,
      "learning_rate": 7.585260853703027e-05,
      "loss": 3.7081,
      "step": 51200
    },
    {
      "epoch": 3.739551598956528,
      "grad_norm": 0.12085344642400742,
      "learning_rate": 7.5633710324699e-05,
      "loss": 3.6761,
      "step": 51250
    },
    {
      "epoch": 3.74320010215809,
      "grad_norm": 0.04075991362333298,
      "learning_rate": 7.541481211236774e-05,
      "loss": 3.725,
      "step": 51300
    },
    {
      "epoch": 3.7468486053596513,
      "grad_norm": 0.02179764397442341,
      "learning_rate": 7.519591390003647e-05,
      "loss": 3.7411,
      "step": 51350
    },
    {
      "epoch": 3.7504971085612127,
      "grad_norm": 0.05827103555202484,
      "learning_rate": 7.497701568770521e-05,
      "loss": 3.6855,
      "step": 51400
    },
    {
      "epoch": 3.754145611762774,
      "grad_norm": 0.025835184380412102,
      "learning_rate": 7.475811747537394e-05,
      "loss": 3.6644,
      "step": 51450
    },
    {
      "epoch": 3.757794114964336,
      "grad_norm": 0.04280194640159607,
      "learning_rate": 7.453921926304267e-05,
      "loss": 3.7454,
      "step": 51500
    },
    {
      "epoch": 3.7614426181658973,
      "grad_norm": 0.03185984492301941,
      "learning_rate": 7.432032105071141e-05,
      "loss": 3.6737,
      "step": 51550
    },
    {
      "epoch": 3.765091121367459,
      "grad_norm": 0.02343672513961792,
      "learning_rate": 7.410142283838016e-05,
      "loss": 3.7448,
      "step": 51600
    },
    {
      "epoch": 3.7687396245690206,
      "grad_norm": 0.01937006786465645,
      "learning_rate": 7.388252462604888e-05,
      "loss": 3.6732,
      "step": 51650
    },
    {
      "epoch": 3.772388127770582,
      "grad_norm": 0.033466968685388565,
      "learning_rate": 7.366362641371761e-05,
      "loss": 3.6881,
      "step": 51700
    },
    {
      "epoch": 3.776036630972144,
      "grad_norm": 0.036218613386154175,
      "learning_rate": 7.344472820138635e-05,
      "loss": 3.6899,
      "step": 51750
    },
    {
      "epoch": 3.779685134173705,
      "grad_norm": 0.03302498534321785,
      "learning_rate": 7.322582998905508e-05,
      "loss": 3.6468,
      "step": 51800
    },
    {
      "epoch": 3.783333637375267,
      "grad_norm": 0.06089348718523979,
      "learning_rate": 7.300693177672381e-05,
      "loss": 3.7367,
      "step": 51850
    },
    {
      "epoch": 3.7869821405768285,
      "grad_norm": 0.0297012347728014,
      "learning_rate": 7.278803356439255e-05,
      "loss": 3.6626,
      "step": 51900
    },
    {
      "epoch": 3.79063064377839,
      "grad_norm": 0.027736729010939598,
      "learning_rate": 7.25691353520613e-05,
      "loss": 3.62,
      "step": 51950
    },
    {
      "epoch": 3.7942791469799513,
      "grad_norm": 0.03151692450046539,
      "learning_rate": 7.235023713973002e-05,
      "loss": 3.7139,
      "step": 52000
    },
    {
      "epoch": 3.797927650181513,
      "grad_norm": 0.026544535532593727,
      "learning_rate": 7.213133892739875e-05,
      "loss": 3.7043,
      "step": 52050
    },
    {
      "epoch": 3.8015761533830745,
      "grad_norm": 0.02434658817946911,
      "learning_rate": 7.191244071506749e-05,
      "loss": 3.6614,
      "step": 52100
    },
    {
      "epoch": 3.8052246565846364,
      "grad_norm": 0.0334189347922802,
      "learning_rate": 7.169354250273622e-05,
      "loss": 3.7225,
      "step": 52150
    },
    {
      "epoch": 3.808873159786198,
      "grad_norm": 0.03978713974356651,
      "learning_rate": 7.147464429040495e-05,
      "loss": 3.7182,
      "step": 52200
    },
    {
      "epoch": 3.812521662987759,
      "grad_norm": 0.022354768589138985,
      "learning_rate": 7.125574607807369e-05,
      "loss": 3.7013,
      "step": 52250
    },
    {
      "epoch": 3.8161701661893206,
      "grad_norm": 0.03902352973818779,
      "learning_rate": 7.103684786574243e-05,
      "loss": 3.7792,
      "step": 52300
    },
    {
      "epoch": 3.8198186693908824,
      "grad_norm": 0.18111282587051392,
      "learning_rate": 7.081794965341116e-05,
      "loss": 3.7061,
      "step": 52350
    },
    {
      "epoch": 3.823467172592444,
      "grad_norm": 0.05573084577918053,
      "learning_rate": 7.059905144107989e-05,
      "loss": 3.7134,
      "step": 52400
    },
    {
      "epoch": 3.8271156757940057,
      "grad_norm": 0.036387961357831955,
      "learning_rate": 7.038015322874863e-05,
      "loss": 3.6656,
      "step": 52450
    },
    {
      "epoch": 3.830764178995567,
      "grad_norm": 0.06480877101421356,
      "learning_rate": 7.016125501641736e-05,
      "loss": 3.7254,
      "step": 52500
    },
    {
      "epoch": 3.8344126821971285,
      "grad_norm": 0.02233797125518322,
      "learning_rate": 6.994235680408609e-05,
      "loss": 3.7582,
      "step": 52550
    },
    {
      "epoch": 3.8380611853986903,
      "grad_norm": 0.07576066255569458,
      "learning_rate": 6.972345859175483e-05,
      "loss": 3.6204,
      "step": 52600
    },
    {
      "epoch": 3.8417096886002517,
      "grad_norm": 0.03233421966433525,
      "learning_rate": 6.950456037942356e-05,
      "loss": 3.6617,
      "step": 52650
    },
    {
      "epoch": 3.845358191801813,
      "grad_norm": 0.01999225653707981,
      "learning_rate": 6.92856621670923e-05,
      "loss": 3.7179,
      "step": 52700
    },
    {
      "epoch": 3.849006695003375,
      "grad_norm": 0.5443800091743469,
      "learning_rate": 6.906676395476103e-05,
      "loss": 3.7137,
      "step": 52750
    },
    {
      "epoch": 3.8526551982049364,
      "grad_norm": 0.0464460663497448,
      "learning_rate": 6.884786574242976e-05,
      "loss": 3.7236,
      "step": 52800
    },
    {
      "epoch": 3.856303701406498,
      "grad_norm": 0.033920131623744965,
      "learning_rate": 6.86289675300985e-05,
      "loss": 3.7429,
      "step": 52850
    },
    {
      "epoch": 3.8599522046080597,
      "grad_norm": 0.028847433626651764,
      "learning_rate": 6.841006931776723e-05,
      "loss": 3.6918,
      "step": 52900
    },
    {
      "epoch": 3.863600707809621,
      "grad_norm": 0.04203077778220177,
      "learning_rate": 6.819117110543596e-05,
      "loss": 3.6512,
      "step": 52950
    },
    {
      "epoch": 3.867249211011183,
      "grad_norm": 0.03779658302664757,
      "learning_rate": 6.79722728931047e-05,
      "loss": 3.7652,
      "step": 53000
    },
    {
      "epoch": 3.8708977142127443,
      "grad_norm": 0.1069314181804657,
      "learning_rate": 6.775337468077344e-05,
      "loss": 3.7518,
      "step": 53050
    },
    {
      "epoch": 3.8745462174143057,
      "grad_norm": 0.10182321816682816,
      "learning_rate": 6.753447646844217e-05,
      "loss": 3.6821,
      "step": 53100
    },
    {
      "epoch": 3.878194720615867,
      "grad_norm": 0.0395430363714695,
      "learning_rate": 6.73155782561109e-05,
      "loss": 3.6901,
      "step": 53150
    },
    {
      "epoch": 3.881843223817429,
      "grad_norm": 0.027267754077911377,
      "learning_rate": 6.709668004377964e-05,
      "loss": 3.6555,
      "step": 53200
    },
    {
      "epoch": 3.8854917270189904,
      "grad_norm": 0.018678506836295128,
      "learning_rate": 6.687778183144837e-05,
      "loss": 3.661,
      "step": 53250
    },
    {
      "epoch": 3.889140230220552,
      "grad_norm": 0.036771032959222794,
      "learning_rate": 6.66588836191171e-05,
      "loss": 3.6871,
      "step": 53300
    },
    {
      "epoch": 3.8927887334221136,
      "grad_norm": 0.024868883192539215,
      "learning_rate": 6.643998540678584e-05,
      "loss": 3.7339,
      "step": 53350
    },
    {
      "epoch": 3.896437236623675,
      "grad_norm": 0.022358380258083344,
      "learning_rate": 6.622108719445458e-05,
      "loss": 3.6988,
      "step": 53400
    },
    {
      "epoch": 3.9000857398252364,
      "grad_norm": 0.04486281797289848,
      "learning_rate": 6.600218898212331e-05,
      "loss": 3.717,
      "step": 53450
    },
    {
      "epoch": 3.9037342430267983,
      "grad_norm": 0.05174582824110985,
      "learning_rate": 6.578329076979204e-05,
      "loss": 3.6908,
      "step": 53500
    },
    {
      "epoch": 3.9073827462283597,
      "grad_norm": 0.02900690771639347,
      "learning_rate": 6.556439255746078e-05,
      "loss": 3.6694,
      "step": 53550
    },
    {
      "epoch": 3.9110312494299215,
      "grad_norm": 0.08459445834159851,
      "learning_rate": 6.534549434512951e-05,
      "loss": 3.7027,
      "step": 53600
    },
    {
      "epoch": 3.914679752631483,
      "grad_norm": 0.20508383214473724,
      "learning_rate": 6.512659613279824e-05,
      "loss": 3.7163,
      "step": 53650
    },
    {
      "epoch": 3.9183282558330443,
      "grad_norm": 0.096585214138031,
      "learning_rate": 6.490769792046698e-05,
      "loss": 3.7627,
      "step": 53700
    },
    {
      "epoch": 3.921976759034606,
      "grad_norm": 0.059955570846796036,
      "learning_rate": 6.468879970813572e-05,
      "loss": 3.6854,
      "step": 53750
    },
    {
      "epoch": 3.9256252622361676,
      "grad_norm": 0.4987045228481293,
      "learning_rate": 6.446990149580445e-05,
      "loss": 3.6242,
      "step": 53800
    },
    {
      "epoch": 3.9292737654377294,
      "grad_norm": 0.020589953288435936,
      "learning_rate": 6.425100328347318e-05,
      "loss": 3.7436,
      "step": 53850
    },
    {
      "epoch": 3.932922268639291,
      "grad_norm": 0.03170733526349068,
      "learning_rate": 6.403210507114192e-05,
      "loss": 3.767,
      "step": 53900
    },
    {
      "epoch": 3.9365707718408522,
      "grad_norm": 0.026174485683441162,
      "learning_rate": 6.381320685881065e-05,
      "loss": 3.7398,
      "step": 53950
    },
    {
      "epoch": 3.9402192750424136,
      "grad_norm": 0.022941017523407936,
      "learning_rate": 6.359430864647937e-05,
      "loss": 3.719,
      "step": 54000
    },
    {
      "epoch": 3.9438677782439755,
      "grad_norm": 0.024798249825835228,
      "learning_rate": 6.337541043414812e-05,
      "loss": 3.6699,
      "step": 54050
    },
    {
      "epoch": 3.947516281445537,
      "grad_norm": 0.02989622764289379,
      "learning_rate": 6.315651222181686e-05,
      "loss": 3.6855,
      "step": 54100
    },
    {
      "epoch": 3.9511647846470987,
      "grad_norm": 0.026451246812939644,
      "learning_rate": 6.293761400948559e-05,
      "loss": 3.699,
      "step": 54150
    },
    {
      "epoch": 3.95481328784866,
      "grad_norm": 0.022206678986549377,
      "learning_rate": 6.271871579715432e-05,
      "loss": 3.7153,
      "step": 54200
    },
    {
      "epoch": 3.9584617910502216,
      "grad_norm": 0.4826337695121765,
      "learning_rate": 6.249981758482306e-05,
      "loss": 3.7118,
      "step": 54250
    },
    {
      "epoch": 3.962110294251783,
      "grad_norm": 1.3752484321594238,
      "learning_rate": 6.228091937249179e-05,
      "loss": 3.7415,
      "step": 54300
    },
    {
      "epoch": 3.965758797453345,
      "grad_norm": 0.04081374779343605,
      "learning_rate": 6.206202116016051e-05,
      "loss": 3.6717,
      "step": 54350
    },
    {
      "epoch": 3.969407300654906,
      "grad_norm": 0.046261630952358246,
      "learning_rate": 6.184312294782926e-05,
      "loss": 3.6919,
      "step": 54400
    },
    {
      "epoch": 3.973055803856468,
      "grad_norm": 0.10361625254154205,
      "learning_rate": 6.162422473549798e-05,
      "loss": 3.7045,
      "step": 54450
    },
    {
      "epoch": 3.9767043070580295,
      "grad_norm": 0.07889515161514282,
      "learning_rate": 6.140532652316673e-05,
      "loss": 3.641,
      "step": 54500
    },
    {
      "epoch": 3.980352810259591,
      "grad_norm": 0.03357668220996857,
      "learning_rate": 6.118642831083545e-05,
      "loss": 3.7684,
      "step": 54550
    },
    {
      "epoch": 3.9840013134611527,
      "grad_norm": 0.02642183005809784,
      "learning_rate": 6.096753009850419e-05,
      "loss": 3.6592,
      "step": 54600
    },
    {
      "epoch": 3.987649816662714,
      "grad_norm": 0.1080952137708664,
      "learning_rate": 6.0748631886172925e-05,
      "loss": 3.6614,
      "step": 54650
    },
    {
      "epoch": 3.9912983198642755,
      "grad_norm": 0.022387316450476646,
      "learning_rate": 6.052973367384165e-05,
      "loss": 3.6908,
      "step": 54700
    },
    {
      "epoch": 3.9949468230658374,
      "grad_norm": 0.08159025013446808,
      "learning_rate": 6.0310835461510395e-05,
      "loss": 3.6925,
      "step": 54750
    },
    {
      "epoch": 3.9985953262673988,
      "grad_norm": 0.02150088921189308,
      "learning_rate": 6.009193724917913e-05,
      "loss": 3.7274,
      "step": 54800
    },
    {
      "epoch": 4.0,
      "eval_loss": 2.591811418533325,
      "eval_runtime": 3.8173,
      "eval_samples_per_second": 26.196,
      "eval_steps_per_second": 3.406,
      "step": 54820
    },
    {
      "epoch": 4.002189101920937,
      "grad_norm": 0.10264535248279572,
      "learning_rate": 5.987303903684786e-05,
      "loss": 3.5903,
      "step": 54850
    },
    {
      "epoch": 4.005837605122498,
      "grad_norm": 0.022963320836424828,
      "learning_rate": 5.9654140824516594e-05,
      "loss": 3.6407,
      "step": 54900
    },
    {
      "epoch": 4.00948610832406,
      "grad_norm": 0.08184850215911865,
      "learning_rate": 5.943524261218533e-05,
      "loss": 3.6769,
      "step": 54950
    },
    {
      "epoch": 4.013134611525621,
      "grad_norm": 0.02758314274251461,
      "learning_rate": 5.921634439985406e-05,
      "loss": 3.7094,
      "step": 55000
    },
    {
      "epoch": 4.0167831147271835,
      "grad_norm": 0.07641196995973587,
      "learning_rate": 5.899744618752279e-05,
      "loss": 3.7072,
      "step": 55050
    },
    {
      "epoch": 4.020431617928745,
      "grad_norm": 0.026334349066019058,
      "learning_rate": 5.8778547975191534e-05,
      "loss": 3.6828,
      "step": 55100
    },
    {
      "epoch": 4.024080121130306,
      "grad_norm": 0.06642650067806244,
      "learning_rate": 5.855964976286027e-05,
      "loss": 3.7844,
      "step": 55150
    },
    {
      "epoch": 4.027728624331868,
      "grad_norm": 0.032549068331718445,
      "learning_rate": 5.8340751550529e-05,
      "loss": 3.7368,
      "step": 55200
    },
    {
      "epoch": 4.031377127533429,
      "grad_norm": 0.045709140598773956,
      "learning_rate": 5.812185333819773e-05,
      "loss": 3.6695,
      "step": 55250
    },
    {
      "epoch": 4.0350256307349905,
      "grad_norm": 0.02224395051598549,
      "learning_rate": 5.790295512586647e-05,
      "loss": 3.7474,
      "step": 55300
    },
    {
      "epoch": 4.038674133936553,
      "grad_norm": 0.02310848981142044,
      "learning_rate": 5.7684056913535196e-05,
      "loss": 3.6227,
      "step": 55350
    },
    {
      "epoch": 4.042322637138114,
      "grad_norm": 0.022055065259337425,
      "learning_rate": 5.746515870120393e-05,
      "loss": 3.7236,
      "step": 55400
    },
    {
      "epoch": 4.045971140339676,
      "grad_norm": 0.02525239810347557,
      "learning_rate": 5.7246260488872674e-05,
      "loss": 3.673,
      "step": 55450
    },
    {
      "epoch": 4.049619643541237,
      "grad_norm": 0.02180914580821991,
      "learning_rate": 5.70273622765414e-05,
      "loss": 3.616,
      "step": 55500
    },
    {
      "epoch": 4.0532681467427985,
      "grad_norm": 0.03009149432182312,
      "learning_rate": 5.680846406421014e-05,
      "loss": 3.72,
      "step": 55550
    },
    {
      "epoch": 4.056916649944361,
      "grad_norm": 0.018333762884140015,
      "learning_rate": 5.658956585187887e-05,
      "loss": 3.7434,
      "step": 55600
    },
    {
      "epoch": 4.060565153145922,
      "grad_norm": 0.09164226800203323,
      "learning_rate": 5.63706676395476e-05,
      "loss": 3.7073,
      "step": 55650
    },
    {
      "epoch": 4.0642136563474835,
      "grad_norm": 0.016271669417619705,
      "learning_rate": 5.6151769427216336e-05,
      "loss": 3.7015,
      "step": 55700
    },
    {
      "epoch": 4.067862159549045,
      "grad_norm": 0.05617239326238632,
      "learning_rate": 5.593287121488507e-05,
      "loss": 3.7054,
      "step": 55750
    },
    {
      "epoch": 4.071510662750606,
      "grad_norm": 0.030466414988040924,
      "learning_rate": 5.571397300255381e-05,
      "loss": 3.7145,
      "step": 55800
    },
    {
      "epoch": 4.075159165952168,
      "grad_norm": 0.04179730638861656,
      "learning_rate": 5.549507479022254e-05,
      "loss": 3.6456,
      "step": 55850
    },
    {
      "epoch": 4.07880766915373,
      "grad_norm": 0.03741709887981415,
      "learning_rate": 5.5276176577891276e-05,
      "loss": 3.6369,
      "step": 55900
    },
    {
      "epoch": 4.0824561723552915,
      "grad_norm": 0.04206126928329468,
      "learning_rate": 5.505727836556001e-05,
      "loss": 3.7026,
      "step": 55950
    },
    {
      "epoch": 4.086104675556853,
      "grad_norm": 0.02295754663646221,
      "learning_rate": 5.483838015322874e-05,
      "loss": 3.6914,
      "step": 56000
    },
    {
      "epoch": 4.089753178758414,
      "grad_norm": 0.026962196454405785,
      "learning_rate": 5.4619481940897475e-05,
      "loss": 3.7425,
      "step": 56050
    },
    {
      "epoch": 4.093401681959976,
      "grad_norm": 0.05862797051668167,
      "learning_rate": 5.440058372856622e-05,
      "loss": 3.7269,
      "step": 56100
    },
    {
      "epoch": 4.097050185161537,
      "grad_norm": 0.13265500962734222,
      "learning_rate": 5.418168551623495e-05,
      "loss": 3.6992,
      "step": 56150
    },
    {
      "epoch": 4.100698688363099,
      "grad_norm": 0.027348196133971214,
      "learning_rate": 5.396278730390368e-05,
      "loss": 3.686,
      "step": 56200
    },
    {
      "epoch": 4.104347191564661,
      "grad_norm": 0.04283318668603897,
      "learning_rate": 5.3743889091572416e-05,
      "loss": 3.7009,
      "step": 56250
    },
    {
      "epoch": 4.107995694766222,
      "grad_norm": 0.053576599806547165,
      "learning_rate": 5.352499087924115e-05,
      "loss": 3.6626,
      "step": 56300
    },
    {
      "epoch": 4.111644197967784,
      "grad_norm": 0.028583848848938942,
      "learning_rate": 5.330609266690988e-05,
      "loss": 3.6614,
      "step": 56350
    },
    {
      "epoch": 4.115292701169345,
      "grad_norm": 0.028031347319483757,
      "learning_rate": 5.3087194454578614e-05,
      "loss": 3.7165,
      "step": 56400
    },
    {
      "epoch": 4.118941204370907,
      "grad_norm": 0.036655474454164505,
      "learning_rate": 5.2868296242247356e-05,
      "loss": 3.7559,
      "step": 56450
    },
    {
      "epoch": 4.122589707572469,
      "grad_norm": 0.03681725636124611,
      "learning_rate": 5.2649398029916085e-05,
      "loss": 3.7213,
      "step": 56500
    },
    {
      "epoch": 4.12623821077403,
      "grad_norm": 0.023002587258815765,
      "learning_rate": 5.243049981758482e-05,
      "loss": 3.7694,
      "step": 56550
    },
    {
      "epoch": 4.1298867139755915,
      "grad_norm": 0.020523270592093468,
      "learning_rate": 5.2211601605253555e-05,
      "loss": 3.7174,
      "step": 56600
    },
    {
      "epoch": 4.133535217177153,
      "grad_norm": 0.02267417125403881,
      "learning_rate": 5.199270339292228e-05,
      "loss": 3.7458,
      "step": 56650
    },
    {
      "epoch": 4.137183720378714,
      "grad_norm": 0.04131496325135231,
      "learning_rate": 5.177380518059102e-05,
      "loss": 3.7033,
      "step": 56700
    },
    {
      "epoch": 4.140832223580277,
      "grad_norm": 0.031377047300338745,
      "learning_rate": 5.1554906968259754e-05,
      "loss": 3.6744,
      "step": 56750
    },
    {
      "epoch": 4.144480726781838,
      "grad_norm": 0.0340505950152874,
      "learning_rate": 5.1336008755928495e-05,
      "loss": 3.7375,
      "step": 56800
    },
    {
      "epoch": 4.148129229983399,
      "grad_norm": 0.07724637538194656,
      "learning_rate": 5.1117110543597224e-05,
      "loss": 3.7531,
      "step": 56850
    },
    {
      "epoch": 4.151777733184961,
      "grad_norm": 0.03368106484413147,
      "learning_rate": 5.089821233126596e-05,
      "loss": 3.6852,
      "step": 56900
    },
    {
      "epoch": 4.155426236386522,
      "grad_norm": 0.06943830847740173,
      "learning_rate": 5.0679314118934694e-05,
      "loss": 3.6618,
      "step": 56950
    },
    {
      "epoch": 4.159074739588084,
      "grad_norm": 0.0391969159245491,
      "learning_rate": 5.046041590660342e-05,
      "loss": 3.6699,
      "step": 57000
    },
    {
      "epoch": 4.162723242789646,
      "grad_norm": 0.024997597560286522,
      "learning_rate": 5.024151769427216e-05,
      "loss": 3.6935,
      "step": 57050
    },
    {
      "epoch": 4.166371745991207,
      "grad_norm": 0.08965325355529785,
      "learning_rate": 5.002261948194089e-05,
      "loss": 3.6767,
      "step": 57100
    },
    {
      "epoch": 4.170020249192769,
      "grad_norm": 0.023571666330099106,
      "learning_rate": 4.980372126960963e-05,
      "loss": 3.716,
      "step": 57150
    },
    {
      "epoch": 4.17366875239433,
      "grad_norm": 0.024406779557466507,
      "learning_rate": 4.958482305727836e-05,
      "loss": 3.6996,
      "step": 57200
    },
    {
      "epoch": 4.1773172555958915,
      "grad_norm": 0.2508426308631897,
      "learning_rate": 4.93659248449471e-05,
      "loss": 3.7928,
      "step": 57250
    },
    {
      "epoch": 4.180965758797454,
      "grad_norm": 0.019226716831326485,
      "learning_rate": 4.9147026632615827e-05,
      "loss": 3.6903,
      "step": 57300
    },
    {
      "epoch": 4.184614261999015,
      "grad_norm": 0.036625947803258896,
      "learning_rate": 4.892812842028456e-05,
      "loss": 3.6983,
      "step": 57350
    },
    {
      "epoch": 4.188262765200577,
      "grad_norm": 0.022604933008551598,
      "learning_rate": 4.87092302079533e-05,
      "loss": 3.7054,
      "step": 57400
    },
    {
      "epoch": 4.191911268402138,
      "grad_norm": 0.020913265645503998,
      "learning_rate": 4.8490331995622025e-05,
      "loss": 3.7244,
      "step": 57450
    },
    {
      "epoch": 4.195559771603699,
      "grad_norm": 0.023453978821635246,
      "learning_rate": 4.827143378329077e-05,
      "loss": 3.6177,
      "step": 57500
    },
    {
      "epoch": 4.199208274805261,
      "grad_norm": 0.024368731305003166,
      "learning_rate": 4.80525355709595e-05,
      "loss": 3.6912,
      "step": 57550
    },
    {
      "epoch": 4.202856778006823,
      "grad_norm": 0.0595172680914402,
      "learning_rate": 4.783363735862824e-05,
      "loss": 3.6502,
      "step": 57600
    },
    {
      "epoch": 4.2065052812083845,
      "grad_norm": 0.023567387834191322,
      "learning_rate": 4.7614739146296966e-05,
      "loss": 3.6889,
      "step": 57650
    },
    {
      "epoch": 4.210153784409946,
      "grad_norm": 0.022792119532823563,
      "learning_rate": 4.73958409339657e-05,
      "loss": 3.7186,
      "step": 57700
    },
    {
      "epoch": 4.213802287611507,
      "grad_norm": 0.016627078875899315,
      "learning_rate": 4.7176942721634436e-05,
      "loss": 3.6951,
      "step": 57750
    },
    {
      "epoch": 4.217450790813069,
      "grad_norm": 0.10541704297065735,
      "learning_rate": 4.6958044509303165e-05,
      "loss": 3.6618,
      "step": 57800
    },
    {
      "epoch": 4.22109929401463,
      "grad_norm": 0.027636796236038208,
      "learning_rate": 4.6739146296971906e-05,
      "loss": 3.6788,
      "step": 57850
    },
    {
      "epoch": 4.224747797216192,
      "grad_norm": 0.029783083125948906,
      "learning_rate": 4.652024808464064e-05,
      "loss": 3.6341,
      "step": 57900
    },
    {
      "epoch": 4.228396300417754,
      "grad_norm": 0.02768663503229618,
      "learning_rate": 4.630134987230938e-05,
      "loss": 3.7144,
      "step": 57950
    },
    {
      "epoch": 4.232044803619315,
      "grad_norm": 0.029652394354343414,
      "learning_rate": 4.6082451659978105e-05,
      "loss": 3.7266,
      "step": 58000
    },
    {
      "epoch": 4.235693306820877,
      "grad_norm": 0.06218293309211731,
      "learning_rate": 4.586355344764684e-05,
      "loss": 3.7036,
      "step": 58050
    },
    {
      "epoch": 4.239341810022438,
      "grad_norm": 0.04568127170205116,
      "learning_rate": 4.564465523531557e-05,
      "loss": 3.7165,
      "step": 58100
    },
    {
      "epoch": 4.242990313223999,
      "grad_norm": 0.058928053826093674,
      "learning_rate": 4.5425757022984304e-05,
      "loss": 3.7122,
      "step": 58150
    },
    {
      "epoch": 4.246638816425562,
      "grad_norm": 0.18802978098392487,
      "learning_rate": 4.5206858810653046e-05,
      "loss": 3.678,
      "step": 58200
    },
    {
      "epoch": 4.250287319627123,
      "grad_norm": 0.035708583891391754,
      "learning_rate": 4.498796059832178e-05,
      "loss": 3.6564,
      "step": 58250
    },
    {
      "epoch": 4.2539358228286845,
      "grad_norm": 0.07468853145837784,
      "learning_rate": 4.476906238599051e-05,
      "loss": 3.6418,
      "step": 58300
    },
    {
      "epoch": 4.257584326030246,
      "grad_norm": 0.020724426954984665,
      "learning_rate": 4.4550164173659244e-05,
      "loss": 3.6566,
      "step": 58350
    },
    {
      "epoch": 4.261232829231807,
      "grad_norm": 0.025430578738451004,
      "learning_rate": 4.433126596132798e-05,
      "loss": 3.6857,
      "step": 58400
    },
    {
      "epoch": 4.264881332433369,
      "grad_norm": 0.04586755111813545,
      "learning_rate": 4.411236774899671e-05,
      "loss": 3.6863,
      "step": 58450
    },
    {
      "epoch": 4.268529835634931,
      "grad_norm": 0.024916507303714752,
      "learning_rate": 4.389346953666544e-05,
      "loss": 3.7312,
      "step": 58500
    },
    {
      "epoch": 4.2721783388364925,
      "grad_norm": 0.15644574165344238,
      "learning_rate": 4.3674571324334185e-05,
      "loss": 3.7101,
      "step": 58550
    },
    {
      "epoch": 4.275826842038054,
      "grad_norm": 0.11896438896656036,
      "learning_rate": 4.345567311200292e-05,
      "loss": 3.7001,
      "step": 58600
    },
    {
      "epoch": 4.279475345239615,
      "grad_norm": 0.018852347508072853,
      "learning_rate": 4.323677489967165e-05,
      "loss": 3.7324,
      "step": 58650
    },
    {
      "epoch": 4.283123848441177,
      "grad_norm": 0.06384654343128204,
      "learning_rate": 4.3017876687340384e-05,
      "loss": 3.7098,
      "step": 58700
    },
    {
      "epoch": 4.286772351642739,
      "grad_norm": 0.030338287353515625,
      "learning_rate": 4.279897847500912e-05,
      "loss": 3.6764,
      "step": 58750
    },
    {
      "epoch": 4.2904208548443,
      "grad_norm": 0.03825277090072632,
      "learning_rate": 4.258008026267785e-05,
      "loss": 3.6689,
      "step": 58800
    },
    {
      "epoch": 4.294069358045862,
      "grad_norm": 0.02107740007340908,
      "learning_rate": 4.236118205034658e-05,
      "loss": 3.6804,
      "step": 58850
    },
    {
      "epoch": 4.297717861247423,
      "grad_norm": 0.051946286112070084,
      "learning_rate": 4.2142283838015324e-05,
      "loss": 3.6996,
      "step": 58900
    },
    {
      "epoch": 4.301366364448985,
      "grad_norm": 0.03574051707983017,
      "learning_rate": 4.192338562568405e-05,
      "loss": 3.7362,
      "step": 58950
    },
    {
      "epoch": 4.305014867650546,
      "grad_norm": 0.04734985902905464,
      "learning_rate": 4.170448741335279e-05,
      "loss": 3.6695,
      "step": 59000
    },
    {
      "epoch": 4.308663370852108,
      "grad_norm": 0.02642720751464367,
      "learning_rate": 4.148558920102152e-05,
      "loss": 3.6681,
      "step": 59050
    },
    {
      "epoch": 4.31231187405367,
      "grad_norm": 0.031000038608908653,
      "learning_rate": 4.126669098869025e-05,
      "loss": 3.6889,
      "step": 59100
    },
    {
      "epoch": 4.315960377255231,
      "grad_norm": 0.030153516680002213,
      "learning_rate": 4.1047792776358986e-05,
      "loss": 3.7339,
      "step": 59150
    },
    {
      "epoch": 4.3196088804567925,
      "grad_norm": 0.02109927125275135,
      "learning_rate": 4.082889456402772e-05,
      "loss": 3.6947,
      "step": 59200
    },
    {
      "epoch": 4.323257383658354,
      "grad_norm": 0.020023949444293976,
      "learning_rate": 4.0609996351696463e-05,
      "loss": 3.7363,
      "step": 59250
    },
    {
      "epoch": 4.326905886859915,
      "grad_norm": 0.0192734207957983,
      "learning_rate": 4.039109813936519e-05,
      "loss": 3.6704,
      "step": 59300
    },
    {
      "epoch": 4.330554390061478,
      "grad_norm": 0.04993768408894539,
      "learning_rate": 4.017219992703393e-05,
      "loss": 3.7293,
      "step": 59350
    },
    {
      "epoch": 4.334202893263039,
      "grad_norm": 0.023640919476747513,
      "learning_rate": 3.995330171470266e-05,
      "loss": 3.6783,
      "step": 59400
    },
    {
      "epoch": 4.3378513964646,
      "grad_norm": 0.029056614264845848,
      "learning_rate": 3.973440350237139e-05,
      "loss": 3.7128,
      "step": 59450
    },
    {
      "epoch": 4.341499899666162,
      "grad_norm": 0.01834844797849655,
      "learning_rate": 3.9515505290040126e-05,
      "loss": 3.7355,
      "step": 59500
    },
    {
      "epoch": 4.345148402867723,
      "grad_norm": 0.24250845611095428,
      "learning_rate": 3.929660707770886e-05,
      "loss": 3.6711,
      "step": 59550
    },
    {
      "epoch": 4.3487969060692855,
      "grad_norm": 0.03885532170534134,
      "learning_rate": 3.90777088653776e-05,
      "loss": 3.7285,
      "step": 59600
    },
    {
      "epoch": 4.352445409270847,
      "grad_norm": 0.038724400103092194,
      "learning_rate": 3.885881065304633e-05,
      "loss": 3.715,
      "step": 59650
    },
    {
      "epoch": 4.356093912472408,
      "grad_norm": 0.08397529274225235,
      "learning_rate": 3.8639912440715066e-05,
      "loss": 3.6321,
      "step": 59700
    },
    {
      "epoch": 4.35974241567397,
      "grad_norm": 0.026302030310034752,
      "learning_rate": 3.8421014228383795e-05,
      "loss": 3.6284,
      "step": 59750
    },
    {
      "epoch": 4.363390918875531,
      "grad_norm": 0.02433183416724205,
      "learning_rate": 3.820211601605253e-05,
      "loss": 3.6247,
      "step": 59800
    },
    {
      "epoch": 4.3670394220770925,
      "grad_norm": 0.020955665037035942,
      "learning_rate": 3.7983217803721265e-05,
      "loss": 3.6962,
      "step": 59850
    },
    {
      "epoch": 4.370687925278655,
      "grad_norm": 0.022786220535635948,
      "learning_rate": 3.776431959138999e-05,
      "loss": 3.7047,
      "step": 59900
    },
    {
      "epoch": 4.374336428480216,
      "grad_norm": 0.029012326151132584,
      "learning_rate": 3.7545421379058735e-05,
      "loss": 3.6034,
      "step": 59950
    },
    {
      "epoch": 4.377984931681778,
      "grad_norm": 0.045467961579561234,
      "learning_rate": 3.7326523166727464e-05,
      "loss": 3.7588,
      "step": 60000
    },
    {
      "epoch": 4.381633434883339,
      "grad_norm": 0.03298920765519142,
      "learning_rate": 3.7107624954396206e-05,
      "loss": 3.6039,
      "step": 60050
    },
    {
      "epoch": 4.3852819380849,
      "grad_norm": 0.022655164822936058,
      "learning_rate": 3.6888726742064934e-05,
      "loss": 3.6474,
      "step": 60100
    },
    {
      "epoch": 4.388930441286462,
      "grad_norm": 0.018875258043408394,
      "learning_rate": 3.666982852973367e-05,
      "loss": 3.732,
      "step": 60150
    },
    {
      "epoch": 4.392578944488024,
      "grad_norm": 0.050718728452920914,
      "learning_rate": 3.6450930317402404e-05,
      "loss": 3.6773,
      "step": 60200
    },
    {
      "epoch": 4.3962274476895855,
      "grad_norm": 0.022176530212163925,
      "learning_rate": 3.623203210507114e-05,
      "loss": 3.6842,
      "step": 60250
    },
    {
      "epoch": 4.399875950891147,
      "grad_norm": 0.026228446513414383,
      "learning_rate": 3.6013133892739874e-05,
      "loss": 3.667,
      "step": 60300
    },
    {
      "epoch": 4.403524454092708,
      "grad_norm": 0.02213992178440094,
      "learning_rate": 3.579423568040861e-05,
      "loss": 3.735,
      "step": 60350
    },
    {
      "epoch": 4.40717295729427,
      "grad_norm": 0.0573698915541172,
      "learning_rate": 3.5575337468077345e-05,
      "loss": 3.6853,
      "step": 60400
    },
    {
      "epoch": 4.410821460495832,
      "grad_norm": 0.09131903946399689,
      "learning_rate": 3.535643925574607e-05,
      "loss": 3.6946,
      "step": 60450
    },
    {
      "epoch": 4.414469963697393,
      "grad_norm": 0.0225211251527071,
      "learning_rate": 3.513754104341481e-05,
      "loss": 3.7258,
      "step": 60500
    },
    {
      "epoch": 4.418118466898955,
      "grad_norm": 0.09074199199676514,
      "learning_rate": 3.4918642831083543e-05,
      "loss": 3.7078,
      "step": 60550
    },
    {
      "epoch": 4.421766970100516,
      "grad_norm": 0.026534073054790497,
      "learning_rate": 3.469974461875228e-05,
      "loss": 3.7202,
      "step": 60600
    },
    {
      "epoch": 4.425415473302078,
      "grad_norm": 0.02453201450407505,
      "learning_rate": 3.448084640642101e-05,
      "loss": 3.759,
      "step": 60650
    },
    {
      "epoch": 4.429063976503639,
      "grad_norm": 0.021384146064519882,
      "learning_rate": 3.426194819408975e-05,
      "loss": 3.7408,
      "step": 60700
    },
    {
      "epoch": 4.432712479705201,
      "grad_norm": 0.04237829148769379,
      "learning_rate": 3.404304998175848e-05,
      "loss": 3.6792,
      "step": 60750
    },
    {
      "epoch": 4.436360982906763,
      "grad_norm": 0.11035598814487457,
      "learning_rate": 3.382415176942721e-05,
      "loss": 3.6497,
      "step": 60800
    },
    {
      "epoch": 4.440009486108324,
      "grad_norm": 0.032632265239953995,
      "learning_rate": 3.360525355709595e-05,
      "loss": 3.6927,
      "step": 60850
    },
    {
      "epoch": 4.4436579893098855,
      "grad_norm": 0.3297070860862732,
      "learning_rate": 3.338635534476468e-05,
      "loss": 3.7087,
      "step": 60900
    },
    {
      "epoch": 4.447306492511447,
      "grad_norm": 0.030161630362272263,
      "learning_rate": 3.316745713243342e-05,
      "loss": 3.6914,
      "step": 60950
    },
    {
      "epoch": 4.450954995713008,
      "grad_norm": 0.028340183198451996,
      "learning_rate": 3.2948558920102146e-05,
      "loss": 3.716,
      "step": 61000
    },
    {
      "epoch": 4.454603498914571,
      "grad_norm": 0.021463021636009216,
      "learning_rate": 3.272966070777089e-05,
      "loss": 3.677,
      "step": 61050
    },
    {
      "epoch": 4.458252002116132,
      "grad_norm": 0.026514945551753044,
      "learning_rate": 3.2510762495439617e-05,
      "loss": 3.6909,
      "step": 61100
    },
    {
      "epoch": 4.461900505317693,
      "grad_norm": 0.08136646449565887,
      "learning_rate": 3.229186428310835e-05,
      "loss": 3.7455,
      "step": 61150
    },
    {
      "epoch": 4.465549008519255,
      "grad_norm": 0.019411694258451462,
      "learning_rate": 3.207296607077709e-05,
      "loss": 3.7221,
      "step": 61200
    },
    {
      "epoch": 4.469197511720816,
      "grad_norm": 0.18109166622161865,
      "learning_rate": 3.185406785844582e-05,
      "loss": 3.6809,
      "step": 61250
    },
    {
      "epoch": 4.4728460149223785,
      "grad_norm": 0.026506783440709114,
      "learning_rate": 3.163516964611456e-05,
      "loss": 3.7377,
      "step": 61300
    },
    {
      "epoch": 4.47649451812394,
      "grad_norm": 0.023931974545121193,
      "learning_rate": 3.1416271433783286e-05,
      "loss": 3.7262,
      "step": 61350
    },
    {
      "epoch": 4.480143021325501,
      "grad_norm": 0.02546057105064392,
      "learning_rate": 3.119737322145202e-05,
      "loss": 3.7863,
      "step": 61400
    },
    {
      "epoch": 4.483791524527063,
      "grad_norm": 0.03597092628479004,
      "learning_rate": 3.0978475009120756e-05,
      "loss": 3.6772,
      "step": 61450
    },
    {
      "epoch": 4.487440027728624,
      "grad_norm": 0.0329107902944088,
      "learning_rate": 3.075957679678949e-05,
      "loss": 3.7123,
      "step": 61500
    },
    {
      "epoch": 4.491088530930186,
      "grad_norm": 0.04999540001153946,
      "learning_rate": 3.054067858445822e-05,
      "loss": 3.6377,
      "step": 61550
    },
    {
      "epoch": 4.494737034131748,
      "grad_norm": 0.028073539957404137,
      "learning_rate": 3.0321780372126958e-05,
      "loss": 3.7216,
      "step": 61600
    },
    {
      "epoch": 4.498385537333309,
      "grad_norm": 0.02040129154920578,
      "learning_rate": 3.0102882159795693e-05,
      "loss": 3.6157,
      "step": 61650
    },
    {
      "epoch": 4.502034040534871,
      "grad_norm": 0.03347676247358322,
      "learning_rate": 2.9883983947464425e-05,
      "loss": 3.6886,
      "step": 61700
    },
    {
      "epoch": 4.505682543736432,
      "grad_norm": 0.03929407149553299,
      "learning_rate": 2.9665085735133163e-05,
      "loss": 3.7265,
      "step": 61750
    },
    {
      "epoch": 4.5093310469379935,
      "grad_norm": 0.022351099178195,
      "learning_rate": 2.9446187522801895e-05,
      "loss": 3.6973,
      "step": 61800
    },
    {
      "epoch": 4.512979550139555,
      "grad_norm": 0.06270003318786621,
      "learning_rate": 2.9227289310470627e-05,
      "loss": 3.7454,
      "step": 61850
    },
    {
      "epoch": 4.516628053341117,
      "grad_norm": 0.019845658913254738,
      "learning_rate": 2.9008391098139362e-05,
      "loss": 3.6839,
      "step": 61900
    },
    {
      "epoch": 4.520276556542679,
      "grad_norm": 0.1311502307653427,
      "learning_rate": 2.8789492885808097e-05,
      "loss": 3.6859,
      "step": 61950
    },
    {
      "epoch": 4.52392505974424,
      "grad_norm": 0.04563647881150246,
      "learning_rate": 2.8570594673476832e-05,
      "loss": 3.7079,
      "step": 62000
    },
    {
      "epoch": 4.527573562945801,
      "grad_norm": 0.043951474130153656,
      "learning_rate": 2.8351696461145564e-05,
      "loss": 3.7139,
      "step": 62050
    },
    {
      "epoch": 4.531222066147363,
      "grad_norm": 0.1764812022447586,
      "learning_rate": 2.81327982488143e-05,
      "loss": 3.7396,
      "step": 62100
    },
    {
      "epoch": 4.534870569348925,
      "grad_norm": 0.11981423199176788,
      "learning_rate": 2.7913900036483034e-05,
      "loss": 3.667,
      "step": 62150
    },
    {
      "epoch": 4.5385190725504865,
      "grad_norm": 0.030080609023571014,
      "learning_rate": 2.7695001824151766e-05,
      "loss": 3.7127,
      "step": 62200
    },
    {
      "epoch": 4.542167575752048,
      "grad_norm": 0.04411797225475311,
      "learning_rate": 2.7476103611820498e-05,
      "loss": 3.7449,
      "step": 62250
    },
    {
      "epoch": 4.545816078953609,
      "grad_norm": 0.025222845375537872,
      "learning_rate": 2.7257205399489236e-05,
      "loss": 3.7161,
      "step": 62300
    },
    {
      "epoch": 4.549464582155171,
      "grad_norm": 0.05448203533887863,
      "learning_rate": 2.7038307187157968e-05,
      "loss": 3.72,
      "step": 62350
    },
    {
      "epoch": 4.553113085356732,
      "grad_norm": 0.062200866639614105,
      "learning_rate": 2.6819408974826703e-05,
      "loss": 3.6665,
      "step": 62400
    },
    {
      "epoch": 4.5567615885582935,
      "grad_norm": 0.019175751134753227,
      "learning_rate": 2.660051076249544e-05,
      "loss": 3.6674,
      "step": 62450
    },
    {
      "epoch": 4.560410091759856,
      "grad_norm": 0.06428829580545425,
      "learning_rate": 2.638161255016417e-05,
      "loss": 3.7351,
      "step": 62500
    },
    {
      "epoch": 4.564058594961417,
      "grad_norm": 0.038248367607593536,
      "learning_rate": 2.6162714337832905e-05,
      "loss": 3.7114,
      "step": 62550
    },
    {
      "epoch": 4.567707098162979,
      "grad_norm": 0.04983869940042496,
      "learning_rate": 2.5943816125501637e-05,
      "loss": 3.7353,
      "step": 62600
    },
    {
      "epoch": 4.57135560136454,
      "grad_norm": 0.02361631952226162,
      "learning_rate": 2.5724917913170376e-05,
      "loss": 3.7137,
      "step": 62650
    },
    {
      "epoch": 4.575004104566101,
      "grad_norm": 0.02379588782787323,
      "learning_rate": 2.5506019700839107e-05,
      "loss": 3.6906,
      "step": 62700
    },
    {
      "epoch": 4.578652607767664,
      "grad_norm": 0.028481654822826385,
      "learning_rate": 2.528712148850784e-05,
      "loss": 3.6982,
      "step": 62750
    },
    {
      "epoch": 4.582301110969225,
      "grad_norm": 0.041033025830984116,
      "learning_rate": 2.5068223276176578e-05,
      "loss": 3.7352,
      "step": 62800
    },
    {
      "epoch": 4.5859496141707865,
      "grad_norm": 0.0196295827627182,
      "learning_rate": 2.484932506384531e-05,
      "loss": 3.7143,
      "step": 62850
    },
    {
      "epoch": 4.589598117372348,
      "grad_norm": 0.03883425146341324,
      "learning_rate": 2.4630426851514045e-05,
      "loss": 3.7293,
      "step": 62900
    },
    {
      "epoch": 4.593246620573909,
      "grad_norm": 0.043980665504932404,
      "learning_rate": 2.4411528639182776e-05,
      "loss": 3.6809,
      "step": 62950
    },
    {
      "epoch": 4.596895123775472,
      "grad_norm": 0.1454288214445114,
      "learning_rate": 2.419263042685151e-05,
      "loss": 3.7687,
      "step": 63000
    },
    {
      "epoch": 4.600543626977033,
      "grad_norm": 0.026399483904242516,
      "learning_rate": 2.3973732214520247e-05,
      "loss": 3.7173,
      "step": 63050
    },
    {
      "epoch": 4.604192130178594,
      "grad_norm": 0.036732520908117294,
      "learning_rate": 2.375483400218898e-05,
      "loss": 3.713,
      "step": 63100
    },
    {
      "epoch": 4.607840633380156,
      "grad_norm": 0.037746675312519073,
      "learning_rate": 2.3535935789857717e-05,
      "loss": 3.7844,
      "step": 63150
    },
    {
      "epoch": 4.611489136581717,
      "grad_norm": 0.05859414115548134,
      "learning_rate": 2.331703757752645e-05,
      "loss": 3.6151,
      "step": 63200
    },
    {
      "epoch": 4.615137639783279,
      "grad_norm": 0.028160501271486282,
      "learning_rate": 2.309813936519518e-05,
      "loss": 3.6685,
      "step": 63250
    },
    {
      "epoch": 4.61878614298484,
      "grad_norm": 0.13212068378925323,
      "learning_rate": 2.287924115286392e-05,
      "loss": 3.68,
      "step": 63300
    },
    {
      "epoch": 4.622434646186402,
      "grad_norm": 0.025250380858778954,
      "learning_rate": 2.266034294053265e-05,
      "loss": 3.7164,
      "step": 63350
    },
    {
      "epoch": 4.626083149387964,
      "grad_norm": 0.046993229538202286,
      "learning_rate": 2.2441444728201383e-05,
      "loss": 3.7032,
      "step": 63400
    },
    {
      "epoch": 4.629731652589525,
      "grad_norm": 0.048346880823373795,
      "learning_rate": 2.2222546515870118e-05,
      "loss": 3.635,
      "step": 63450
    },
    {
      "epoch": 4.6333801557910865,
      "grad_norm": 0.02190404199063778,
      "learning_rate": 2.2003648303538853e-05,
      "loss": 3.7186,
      "step": 63500
    },
    {
      "epoch": 4.637028658992648,
      "grad_norm": 0.05000240355730057,
      "learning_rate": 2.1784750091207588e-05,
      "loss": 3.7167,
      "step": 63550
    },
    {
      "epoch": 4.64067716219421,
      "grad_norm": 0.04810859635472298,
      "learning_rate": 2.156585187887632e-05,
      "loss": 3.7007,
      "step": 63600
    },
    {
      "epoch": 4.644325665395772,
      "grad_norm": 0.10802659392356873,
      "learning_rate": 2.1346953666545058e-05,
      "loss": 3.6953,
      "step": 63650
    },
    {
      "epoch": 4.647974168597333,
      "grad_norm": 0.06221919506788254,
      "learning_rate": 2.112805545421379e-05,
      "loss": 3.6463,
      "step": 63700
    },
    {
      "epoch": 4.651622671798894,
      "grad_norm": 0.022882381454110146,
      "learning_rate": 2.0909157241882522e-05,
      "loss": 3.7215,
      "step": 63750
    },
    {
      "epoch": 4.655271175000456,
      "grad_norm": 0.050304021686315536,
      "learning_rate": 2.0690259029551254e-05,
      "loss": 3.7804,
      "step": 63800
    },
    {
      "epoch": 4.658919678202017,
      "grad_norm": 0.03562585636973381,
      "learning_rate": 2.0471360817219992e-05,
      "loss": 3.6472,
      "step": 63850
    },
    {
      "epoch": 4.6625681814035795,
      "grad_norm": 0.10849177837371826,
      "learning_rate": 2.0252462604888724e-05,
      "loss": 3.6681,
      "step": 63900
    },
    {
      "epoch": 4.666216684605141,
      "grad_norm": 0.06903482228517532,
      "learning_rate": 2.003356439255746e-05,
      "loss": 3.6607,
      "step": 63950
    },
    {
      "epoch": 4.669865187806702,
      "grad_norm": 0.052400749176740646,
      "learning_rate": 1.9814666180226194e-05,
      "loss": 3.6732,
      "step": 64000
    },
    {
      "epoch": 4.673513691008264,
      "grad_norm": 0.07068764418363571,
      "learning_rate": 1.959576796789493e-05,
      "loss": 3.6715,
      "step": 64050
    },
    {
      "epoch": 4.677162194209825,
      "grad_norm": 0.051948703825473785,
      "learning_rate": 1.937686975556366e-05,
      "loss": 3.6524,
      "step": 64100
    },
    {
      "epoch": 4.680810697411387,
      "grad_norm": 0.030437704175710678,
      "learning_rate": 1.9157971543232393e-05,
      "loss": 3.7069,
      "step": 64150
    },
    {
      "epoch": 4.684459200612949,
      "grad_norm": 0.04446980729699135,
      "learning_rate": 1.893907333090113e-05,
      "loss": 3.6957,
      "step": 64200
    },
    {
      "epoch": 4.68810770381451,
      "grad_norm": 0.025096645578742027,
      "learning_rate": 1.8720175118569863e-05,
      "loss": 3.7183,
      "step": 64250
    },
    {
      "epoch": 4.691756207016072,
      "grad_norm": 0.026724863797426224,
      "learning_rate": 1.8501276906238598e-05,
      "loss": 3.673,
      "step": 64300
    },
    {
      "epoch": 4.695404710217633,
      "grad_norm": 0.02492697536945343,
      "learning_rate": 1.828237869390733e-05,
      "loss": 3.6836,
      "step": 64350
    },
    {
      "epoch": 4.6990532134191945,
      "grad_norm": 0.09753409773111343,
      "learning_rate": 1.8063480481576065e-05,
      "loss": 3.6739,
      "step": 64400
    },
    {
      "epoch": 4.702701716620757,
      "grad_norm": 0.054673388600349426,
      "learning_rate": 1.78445822692448e-05,
      "loss": 3.6834,
      "step": 64450
    },
    {
      "epoch": 4.706350219822318,
      "grad_norm": 0.04277265816926956,
      "learning_rate": 1.7625684056913535e-05,
      "loss": 3.6893,
      "step": 64500
    },
    {
      "epoch": 4.70999872302388,
      "grad_norm": 0.04648813605308533,
      "learning_rate": 1.7406785844582267e-05,
      "loss": 3.666,
      "step": 64550
    },
    {
      "epoch": 4.713647226225441,
      "grad_norm": 0.020838871598243713,
      "learning_rate": 1.7187887632251002e-05,
      "loss": 3.6796,
      "step": 64600
    },
    {
      "epoch": 4.717295729427002,
      "grad_norm": 0.018690578639507294,
      "learning_rate": 1.6968989419919738e-05,
      "loss": 3.6958,
      "step": 64650
    },
    {
      "epoch": 4.720944232628564,
      "grad_norm": 0.035325147211551666,
      "learning_rate": 1.675009120758847e-05,
      "loss": 3.7269,
      "step": 64700
    },
    {
      "epoch": 4.724592735830126,
      "grad_norm": 0.028290677815675735,
      "learning_rate": 1.6531192995257204e-05,
      "loss": 3.6376,
      "step": 64750
    },
    {
      "epoch": 4.7282412390316875,
      "grad_norm": 0.04694119840860367,
      "learning_rate": 1.6312294782925936e-05,
      "loss": 3.652,
      "step": 64800
    },
    {
      "epoch": 4.731889742233249,
      "grad_norm": 0.030871674418449402,
      "learning_rate": 1.609339657059467e-05,
      "loss": 3.6502,
      "step": 64850
    },
    {
      "epoch": 4.73553824543481,
      "grad_norm": 0.0283335093408823,
      "learning_rate": 1.5874498358263406e-05,
      "loss": 3.7154,
      "step": 64900
    },
    {
      "epoch": 4.739186748636372,
      "grad_norm": 0.10929796099662781,
      "learning_rate": 1.565560014593214e-05,
      "loss": 3.708,
      "step": 64950
    },
    {
      "epoch": 4.742835251837933,
      "grad_norm": 0.13903497159481049,
      "learning_rate": 1.5436701933600873e-05,
      "loss": 3.6881,
      "step": 65000
    },
    {
      "epoch": 4.746483755039495,
      "grad_norm": 0.036523111164569855,
      "learning_rate": 1.5217803721269609e-05,
      "loss": 3.6819,
      "step": 65050
    },
    {
      "epoch": 4.750132258241057,
      "grad_norm": 0.7705785036087036,
      "learning_rate": 1.4998905508938342e-05,
      "loss": 3.6849,
      "step": 65100
    },
    {
      "epoch": 4.753780761442618,
      "grad_norm": 0.05047965049743652,
      "learning_rate": 1.4780007296607075e-05,
      "loss": 3.656,
      "step": 65150
    },
    {
      "epoch": 4.75742926464418,
      "grad_norm": 0.0400843508541584,
      "learning_rate": 1.456110908427581e-05,
      "loss": 3.6831,
      "step": 65200
    },
    {
      "epoch": 4.761077767845741,
      "grad_norm": 0.04186180606484413,
      "learning_rate": 1.4342210871944546e-05,
      "loss": 3.7081,
      "step": 65250
    },
    {
      "epoch": 4.764726271047303,
      "grad_norm": 0.02314355969429016,
      "learning_rate": 1.412331265961328e-05,
      "loss": 3.7764,
      "step": 65300
    },
    {
      "epoch": 4.768374774248865,
      "grad_norm": 0.05657408758997917,
      "learning_rate": 1.3904414447282013e-05,
      "loss": 3.6828,
      "step": 65350
    },
    {
      "epoch": 4.772023277450426,
      "grad_norm": 0.13235147297382355,
      "learning_rate": 1.3685516234950746e-05,
      "loss": 3.7233,
      "step": 65400
    },
    {
      "epoch": 4.7756717806519875,
      "grad_norm": 0.10892411321401596,
      "learning_rate": 1.3466618022619481e-05,
      "loss": 3.6409,
      "step": 65450
    },
    {
      "epoch": 4.779320283853549,
      "grad_norm": 0.023167984560132027,
      "learning_rate": 1.3247719810288215e-05,
      "loss": 3.7223,
      "step": 65500
    },
    {
      "epoch": 4.78296878705511,
      "grad_norm": 0.03353969007730484,
      "learning_rate": 1.3028821597956948e-05,
      "loss": 3.6883,
      "step": 65550
    },
    {
      "epoch": 4.786617290256672,
      "grad_norm": 0.01657767780125141,
      "learning_rate": 1.2809923385625683e-05,
      "loss": 3.7315,
      "step": 65600
    },
    {
      "epoch": 4.790265793458234,
      "grad_norm": 0.030619189143180847,
      "learning_rate": 1.2591025173294417e-05,
      "loss": 3.718,
      "step": 65650
    },
    {
      "epoch": 4.793914296659795,
      "grad_norm": 0.023548007011413574,
      "learning_rate": 1.2372126960963152e-05,
      "loss": 3.6628,
      "step": 65700
    },
    {
      "epoch": 4.797562799861357,
      "grad_norm": 0.03075944259762764,
      "learning_rate": 1.2153228748631885e-05,
      "loss": 3.7209,
      "step": 65750
    },
    {
      "epoch": 4.801211303062918,
      "grad_norm": 0.02777542732656002,
      "learning_rate": 1.1934330536300619e-05,
      "loss": 3.6693,
      "step": 65800
    },
    {
      "epoch": 4.80485980626448,
      "grad_norm": 0.022932296618819237,
      "learning_rate": 1.1715432323969352e-05,
      "loss": 3.7246,
      "step": 65850
    },
    {
      "epoch": 4.808508309466042,
      "grad_norm": 0.03489326313138008,
      "learning_rate": 1.1496534111638087e-05,
      "loss": 3.7124,
      "step": 65900
    },
    {
      "epoch": 4.812156812667603,
      "grad_norm": 0.0192548967897892,
      "learning_rate": 1.1277635899306823e-05,
      "loss": 3.7067,
      "step": 65950
    },
    {
      "epoch": 4.815805315869165,
      "grad_norm": 0.02219950594007969,
      "learning_rate": 1.1058737686975554e-05,
      "loss": 3.6298,
      "step": 66000
    },
    {
      "epoch": 4.819453819070726,
      "grad_norm": 0.04289160668849945,
      "learning_rate": 1.083983947464429e-05,
      "loss": 3.729,
      "step": 66050
    },
    {
      "epoch": 4.8231023222722875,
      "grad_norm": 0.04572484642267227,
      "learning_rate": 1.0620941262313023e-05,
      "loss": 3.7746,
      "step": 66100
    },
    {
      "epoch": 4.82675082547385,
      "grad_norm": 0.05190808326005936,
      "learning_rate": 1.0402043049981758e-05,
      "loss": 3.7357,
      "step": 66150
    },
    {
      "epoch": 4.830399328675411,
      "grad_norm": 0.061481498181819916,
      "learning_rate": 1.018314483765049e-05,
      "loss": 3.7085,
      "step": 66200
    },
    {
      "epoch": 4.834047831876973,
      "grad_norm": 0.026734787970781326,
      "learning_rate": 9.964246625319225e-06,
      "loss": 3.7369,
      "step": 66250
    },
    {
      "epoch": 4.837696335078534,
      "grad_norm": 0.025188012048602104,
      "learning_rate": 9.74534841298796e-06,
      "loss": 3.7148,
      "step": 66300
    },
    {
      "epoch": 4.841344838280095,
      "grad_norm": 0.02207004278898239,
      "learning_rate": 9.526450200656694e-06,
      "loss": 3.7413,
      "step": 66350
    },
    {
      "epoch": 4.844993341481657,
      "grad_norm": 0.024741975590586662,
      "learning_rate": 9.307551988325429e-06,
      "loss": 3.7115,
      "step": 66400
    },
    {
      "epoch": 4.848641844683218,
      "grad_norm": 0.036104559898376465,
      "learning_rate": 9.088653775994162e-06,
      "loss": 3.7508,
      "step": 66450
    },
    {
      "epoch": 4.8522903478847805,
      "grad_norm": 0.043982237577438354,
      "learning_rate": 8.869755563662896e-06,
      "loss": 3.7268,
      "step": 66500
    },
    {
      "epoch": 4.855938851086342,
      "grad_norm": 0.03116571344435215,
      "learning_rate": 8.65085735133163e-06,
      "loss": 3.6861,
      "step": 66550
    },
    {
      "epoch": 4.859587354287903,
      "grad_norm": 0.044998496770858765,
      "learning_rate": 8.431959139000364e-06,
      "loss": 3.6953,
      "step": 66600
    },
    {
      "epoch": 4.863235857489465,
      "grad_norm": 0.025058098137378693,
      "learning_rate": 8.213060926669098e-06,
      "loss": 3.7322,
      "step": 66650
    },
    {
      "epoch": 4.866884360691026,
      "grad_norm": 0.026713861152529716,
      "learning_rate": 7.994162714337831e-06,
      "loss": 3.7096,
      "step": 66700
    },
    {
      "epoch": 4.870532863892588,
      "grad_norm": 0.07266681641340256,
      "learning_rate": 7.775264502006566e-06,
      "loss": 3.7663,
      "step": 66750
    },
    {
      "epoch": 4.87418136709415,
      "grad_norm": 0.019846929237246513,
      "learning_rate": 7.556366289675301e-06,
      "loss": 3.6831,
      "step": 66800
    },
    {
      "epoch": 4.877829870295711,
      "grad_norm": 0.029427863657474518,
      "learning_rate": 7.337468077344034e-06,
      "loss": 3.6586,
      "step": 66850
    },
    {
      "epoch": 4.881478373497273,
      "grad_norm": 0.05342842638492584,
      "learning_rate": 7.118569865012768e-06,
      "loss": 3.7008,
      "step": 66900
    },
    {
      "epoch": 4.885126876698834,
      "grad_norm": 0.019325541332364082,
      "learning_rate": 6.899671652681503e-06,
      "loss": 3.5897,
      "step": 66950
    },
    {
      "epoch": 4.888775379900396,
      "grad_norm": 0.022042298689484596,
      "learning_rate": 6.680773440350236e-06,
      "loss": 3.6981,
      "step": 67000
    },
    {
      "epoch": 4.892423883101958,
      "grad_norm": 0.026675665751099586,
      "learning_rate": 6.461875228018971e-06,
      "loss": 3.6963,
      "step": 67050
    },
    {
      "epoch": 4.896072386303519,
      "grad_norm": 0.05746390298008919,
      "learning_rate": 6.242977015687705e-06,
      "loss": 3.6595,
      "step": 67100
    },
    {
      "epoch": 4.899720889505081,
      "grad_norm": 0.04467334225773811,
      "learning_rate": 6.024078803356439e-06,
      "loss": 3.6694,
      "step": 67150
    },
    {
      "epoch": 4.903369392706642,
      "grad_norm": 0.025417355820536613,
      "learning_rate": 5.8051805910251725e-06,
      "loss": 3.73,
      "step": 67200
    },
    {
      "epoch": 4.907017895908203,
      "grad_norm": 0.02846456877887249,
      "learning_rate": 5.586282378693907e-06,
      "loss": 3.6713,
      "step": 67250
    },
    {
      "epoch": 4.910666399109765,
      "grad_norm": 0.02115139365196228,
      "learning_rate": 5.36738416636264e-06,
      "loss": 3.7149,
      "step": 67300
    },
    {
      "epoch": 4.914314902311327,
      "grad_norm": 0.06798581033945084,
      "learning_rate": 5.1484859540313745e-06,
      "loss": 3.7712,
      "step": 67350
    },
    {
      "epoch": 4.9179634055128885,
      "grad_norm": 0.08041733503341675,
      "learning_rate": 4.92958774170011e-06,
      "loss": 3.7355,
      "step": 67400
    },
    {
      "epoch": 4.92161190871445,
      "grad_norm": 0.0242527537047863,
      "learning_rate": 4.710689529368843e-06,
      "loss": 3.6242,
      "step": 67450
    },
    {
      "epoch": 4.925260411916011,
      "grad_norm": 0.03717684745788574,
      "learning_rate": 4.4917913170375774e-06,
      "loss": 3.6332,
      "step": 67500
    },
    {
      "epoch": 4.928908915117573,
      "grad_norm": 0.023197585716843605,
      "learning_rate": 4.272893104706311e-06,
      "loss": 3.6579,
      "step": 67550
    },
    {
      "epoch": 4.932557418319135,
      "grad_norm": 0.023513909429311752,
      "learning_rate": 4.053994892375045e-06,
      "loss": 3.7731,
      "step": 67600
    },
    {
      "epoch": 4.936205921520696,
      "grad_norm": 0.02821934223175049,
      "learning_rate": 3.8350966800437795e-06,
      "loss": 3.685,
      "step": 67650
    },
    {
      "epoch": 4.939854424722258,
      "grad_norm": 0.05237768962979317,
      "learning_rate": 3.6161984677125134e-06,
      "loss": 3.7294,
      "step": 67700
    },
    {
      "epoch": 4.943502927923819,
      "grad_norm": 0.030654076486825943,
      "learning_rate": 3.3973002553812472e-06,
      "loss": 3.693,
      "step": 67750
    },
    {
      "epoch": 4.947151431125381,
      "grad_norm": 0.11478497087955475,
      "learning_rate": 3.1784020430499815e-06,
      "loss": 3.7214,
      "step": 67800
    },
    {
      "epoch": 4.950799934326942,
      "grad_norm": 0.10607828199863434,
      "learning_rate": 2.9595038307187154e-06,
      "loss": 3.7716,
      "step": 67850
    },
    {
      "epoch": 4.954448437528504,
      "grad_norm": 0.021325359120965004,
      "learning_rate": 2.7406056183874493e-06,
      "loss": 3.7128,
      "step": 67900
    },
    {
      "epoch": 4.958096940730066,
      "grad_norm": 0.1155528724193573,
      "learning_rate": 2.521707406056184e-06,
      "loss": 3.6737,
      "step": 67950
    },
    {
      "epoch": 4.961745443931627,
      "grad_norm": 0.02690926007926464,
      "learning_rate": 2.302809193724918e-06,
      "loss": 3.6854,
      "step": 68000
    },
    {
      "epoch": 4.9653939471331885,
      "grad_norm": 0.6003014445304871,
      "learning_rate": 2.0839109813936518e-06,
      "loss": 3.6753,
      "step": 68050
    },
    {
      "epoch": 4.96904245033475,
      "grad_norm": 0.05434327945113182,
      "learning_rate": 1.8650127690623859e-06,
      "loss": 3.6899,
      "step": 68100
    },
    {
      "epoch": 4.972690953536311,
      "grad_norm": 0.02219642698764801,
      "learning_rate": 1.64611455673112e-06,
      "loss": 3.7113,
      "step": 68150
    },
    {
      "epoch": 4.976339456737874,
      "grad_norm": 0.022873660549521446,
      "learning_rate": 1.4272163443998538e-06,
      "loss": 3.7447,
      "step": 68200
    },
    {
      "epoch": 4.979987959939435,
      "grad_norm": 0.023851651698350906,
      "learning_rate": 1.2083181320685881e-06,
      "loss": 3.746,
      "step": 68250
    },
    {
      "epoch": 4.983636463140996,
      "grad_norm": 0.03838573023676872,
      "learning_rate": 9.89419919737322e-07,
      "loss": 3.7193,
      "step": 68300
    },
    {
      "epoch": 4.987284966342558,
      "grad_norm": 0.03614107146859169,
      "learning_rate": 7.705217074060561e-07,
      "loss": 3.6526,
      "step": 68350
    },
    {
      "epoch": 4.990933469544119,
      "grad_norm": 0.03051835298538208,
      "learning_rate": 5.516234950747902e-07,
      "loss": 3.7061,
      "step": 68400
    },
    {
      "epoch": 4.9945819727456815,
      "grad_norm": 0.018544740974903107,
      "learning_rate": 3.327252827435242e-07,
      "loss": 3.7052,
      "step": 68450
    },
    {
      "epoch": 4.998230475947243,
      "grad_norm": 0.514867901802063,
      "learning_rate": 1.1382707041225829e-07,
      "loss": 3.718,
      "step": 68500
    },
    {
      "epoch": 5.0,
      "eval_loss": 2.5912630558013916,
      "eval_runtime": 4.5919,
      "eval_samples_per_second": 21.777,
      "eval_steps_per_second": 2.831,
      "step": 68525
    }
  ],
  "logging_steps": 50,
  "max_steps": 68525,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.094979898179584e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
