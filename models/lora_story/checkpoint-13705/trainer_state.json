{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 13705,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0036485032015615595,
      "grad_norm": 0.06168973445892334,
      "learning_rate": 0.00029978547975191534,
      "loss": 3.8209,
      "step": 50
    },
    {
      "epoch": 0.007297006403123119,
      "grad_norm": 0.06945208460092545,
      "learning_rate": 0.0002995665815395841,
      "loss": 3.807,
      "step": 100
    },
    {
      "epoch": 0.010945509604684678,
      "grad_norm": 0.08085992187261581,
      "learning_rate": 0.0002993476833272528,
      "loss": 3.8061,
      "step": 150
    },
    {
      "epoch": 0.014594012806246238,
      "grad_norm": 0.0659402385354042,
      "learning_rate": 0.00029912878511492155,
      "loss": 3.8285,
      "step": 200
    },
    {
      "epoch": 0.018242516007807796,
      "grad_norm": 0.05315134674310684,
      "learning_rate": 0.00029890988690259025,
      "loss": 3.8489,
      "step": 250
    },
    {
      "epoch": 0.021891019209369356,
      "grad_norm": 0.044151876121759415,
      "learning_rate": 0.000298690988690259,
      "loss": 3.8163,
      "step": 300
    },
    {
      "epoch": 0.025539522410930916,
      "grad_norm": 0.04935237020254135,
      "learning_rate": 0.00029847209047792776,
      "loss": 3.776,
      "step": 350
    },
    {
      "epoch": 0.029188025612492476,
      "grad_norm": 0.13402259349822998,
      "learning_rate": 0.00029825319226559646,
      "loss": 3.7775,
      "step": 400
    },
    {
      "epoch": 0.032836528814054036,
      "grad_norm": 0.03836560994386673,
      "learning_rate": 0.0002980342940532652,
      "loss": 3.7656,
      "step": 450
    },
    {
      "epoch": 0.03648503201561559,
      "grad_norm": 0.06438777595758438,
      "learning_rate": 0.00029781539584093397,
      "loss": 3.7798,
      "step": 500
    },
    {
      "epoch": 0.040133535217177156,
      "grad_norm": 0.06501829624176025,
      "learning_rate": 0.0002975964976286027,
      "loss": 3.7823,
      "step": 550
    },
    {
      "epoch": 0.04378203841873871,
      "grad_norm": 0.06458926200866699,
      "learning_rate": 0.0002973775994162714,
      "loss": 3.7709,
      "step": 600
    },
    {
      "epoch": 0.04743054162030027,
      "grad_norm": 0.1818145364522934,
      "learning_rate": 0.00029715870120394013,
      "loss": 3.7925,
      "step": 650
    },
    {
      "epoch": 0.05107904482186183,
      "grad_norm": 0.07774771749973297,
      "learning_rate": 0.0002969398029916089,
      "loss": 3.8046,
      "step": 700
    },
    {
      "epoch": 0.05472754802342339,
      "grad_norm": 0.044946275651454926,
      "learning_rate": 0.0002967209047792776,
      "loss": 3.7508,
      "step": 750
    },
    {
      "epoch": 0.05837605122498495,
      "grad_norm": 0.06420096009969711,
      "learning_rate": 0.00029650200656694634,
      "loss": 3.7801,
      "step": 800
    },
    {
      "epoch": 0.06202455442654651,
      "grad_norm": 0.05023753270506859,
      "learning_rate": 0.0002962831083546151,
      "loss": 3.7852,
      "step": 850
    },
    {
      "epoch": 0.06567305762810807,
      "grad_norm": 0.05120198428630829,
      "learning_rate": 0.0002960642101422838,
      "loss": 3.8047,
      "step": 900
    },
    {
      "epoch": 0.06932156082966963,
      "grad_norm": 0.0664324089884758,
      "learning_rate": 0.00029584531192995255,
      "loss": 3.745,
      "step": 950
    },
    {
      "epoch": 0.07297006403123119,
      "grad_norm": 0.053615815937519073,
      "learning_rate": 0.0002956264137176213,
      "loss": 3.7325,
      "step": 1000
    },
    {
      "epoch": 0.07661856723279274,
      "grad_norm": 0.055131010711193085,
      "learning_rate": 0.00029540751550529,
      "loss": 3.7752,
      "step": 1050
    },
    {
      "epoch": 0.08026707043435431,
      "grad_norm": 0.055140383541584015,
      "learning_rate": 0.00029518861729295877,
      "loss": 3.7759,
      "step": 1100
    },
    {
      "epoch": 0.08391557363591587,
      "grad_norm": 0.05693132057785988,
      "learning_rate": 0.0002949697190806275,
      "loss": 3.8138,
      "step": 1150
    },
    {
      "epoch": 0.08756407683747743,
      "grad_norm": 0.06278181076049805,
      "learning_rate": 0.0002947508208682962,
      "loss": 3.7514,
      "step": 1200
    },
    {
      "epoch": 0.09121258003903898,
      "grad_norm": 0.05038214474916458,
      "learning_rate": 0.0002945319226559649,
      "loss": 3.7932,
      "step": 1250
    },
    {
      "epoch": 0.09486108324060054,
      "grad_norm": 0.07193893194198608,
      "learning_rate": 0.0002943130244436337,
      "loss": 3.7459,
      "step": 1300
    },
    {
      "epoch": 0.09850958644216211,
      "grad_norm": 0.04489973187446594,
      "learning_rate": 0.00029409412623130244,
      "loss": 3.8312,
      "step": 1350
    },
    {
      "epoch": 0.10215808964372367,
      "grad_norm": 0.06196662783622742,
      "learning_rate": 0.00029387522801897114,
      "loss": 3.7967,
      "step": 1400
    },
    {
      "epoch": 0.10580659284528522,
      "grad_norm": 0.04057652875781059,
      "learning_rate": 0.0002936563298066399,
      "loss": 3.7156,
      "step": 1450
    },
    {
      "epoch": 0.10945509604684678,
      "grad_norm": 0.0447261743247509,
      "learning_rate": 0.00029343743159430865,
      "loss": 3.7739,
      "step": 1500
    },
    {
      "epoch": 0.11310359924840833,
      "grad_norm": 0.04624299332499504,
      "learning_rate": 0.00029321853338197735,
      "loss": 3.8316,
      "step": 1550
    },
    {
      "epoch": 0.1167521024499699,
      "grad_norm": 0.05665179342031479,
      "learning_rate": 0.0002929996351696461,
      "loss": 3.7806,
      "step": 1600
    },
    {
      "epoch": 0.12040060565153146,
      "grad_norm": 0.12215862423181534,
      "learning_rate": 0.0002927807369573148,
      "loss": 3.732,
      "step": 1650
    },
    {
      "epoch": 0.12404910885309302,
      "grad_norm": 0.037881702184677124,
      "learning_rate": 0.00029256183874498356,
      "loss": 3.7962,
      "step": 1700
    },
    {
      "epoch": 0.12769761205465457,
      "grad_norm": 0.05145750194787979,
      "learning_rate": 0.0002923429405326523,
      "loss": 3.7747,
      "step": 1750
    },
    {
      "epoch": 0.13134611525621614,
      "grad_norm": 0.04171350598335266,
      "learning_rate": 0.000292124042320321,
      "loss": 3.8249,
      "step": 1800
    },
    {
      "epoch": 0.1349946184577777,
      "grad_norm": 0.060327451676130295,
      "learning_rate": 0.0002919051441079898,
      "loss": 3.8035,
      "step": 1850
    },
    {
      "epoch": 0.13864312165933926,
      "grad_norm": 0.05733056738972664,
      "learning_rate": 0.0002916862458956585,
      "loss": 3.7642,
      "step": 1900
    },
    {
      "epoch": 0.14229162486090083,
      "grad_norm": 0.0331217423081398,
      "learning_rate": 0.00029146734768332723,
      "loss": 3.7772,
      "step": 1950
    },
    {
      "epoch": 0.14594012806246237,
      "grad_norm": 0.04495008662343025,
      "learning_rate": 0.00029124844947099593,
      "loss": 3.843,
      "step": 2000
    },
    {
      "epoch": 0.14958863126402394,
      "grad_norm": 0.03319648653268814,
      "learning_rate": 0.0002910295512586647,
      "loss": 3.7777,
      "step": 2050
    },
    {
      "epoch": 0.15323713446558548,
      "grad_norm": 0.11495686322450638,
      "learning_rate": 0.00029081065304633344,
      "loss": 3.8468,
      "step": 2100
    },
    {
      "epoch": 0.15688563766714705,
      "grad_norm": 0.03723183274269104,
      "learning_rate": 0.00029059175483400214,
      "loss": 3.7911,
      "step": 2150
    },
    {
      "epoch": 0.16053414086870862,
      "grad_norm": 0.3307992219924927,
      "learning_rate": 0.0002903728566216709,
      "loss": 3.7926,
      "step": 2200
    },
    {
      "epoch": 0.16418264407027017,
      "grad_norm": 0.04692172259092331,
      "learning_rate": 0.00029015395840933966,
      "loss": 3.7424,
      "step": 2250
    },
    {
      "epoch": 0.16783114727183174,
      "grad_norm": 0.03813797980546951,
      "learning_rate": 0.00028993506019700836,
      "loss": 3.8132,
      "step": 2300
    },
    {
      "epoch": 0.17147965047339328,
      "grad_norm": 0.08578605949878693,
      "learning_rate": 0.0002897161619846771,
      "loss": 3.7534,
      "step": 2350
    },
    {
      "epoch": 0.17512815367495485,
      "grad_norm": 0.16760919988155365,
      "learning_rate": 0.00028949726377234587,
      "loss": 3.7728,
      "step": 2400
    },
    {
      "epoch": 0.17877665687651642,
      "grad_norm": 0.0770140066742897,
      "learning_rate": 0.00028927836556001457,
      "loss": 3.7526,
      "step": 2450
    },
    {
      "epoch": 0.18242516007807796,
      "grad_norm": 0.046180251985788345,
      "learning_rate": 0.00028905946734768327,
      "loss": 3.8012,
      "step": 2500
    },
    {
      "epoch": 0.18607366327963953,
      "grad_norm": 0.03855273127555847,
      "learning_rate": 0.000288840569135352,
      "loss": 3.89,
      "step": 2550
    },
    {
      "epoch": 0.18972216648120108,
      "grad_norm": 0.033983685076236725,
      "learning_rate": 0.0002886216709230208,
      "loss": 3.7971,
      "step": 2600
    },
    {
      "epoch": 0.19337066968276265,
      "grad_norm": 0.052928801625967026,
      "learning_rate": 0.0002884027727106895,
      "loss": 3.7804,
      "step": 2650
    },
    {
      "epoch": 0.19701917288432422,
      "grad_norm": 0.04947362840175629,
      "learning_rate": 0.00028818387449835824,
      "loss": 3.7944,
      "step": 2700
    },
    {
      "epoch": 0.20066767608588576,
      "grad_norm": 0.03991773724555969,
      "learning_rate": 0.000287964976286027,
      "loss": 3.7954,
      "step": 2750
    },
    {
      "epoch": 0.20431617928744733,
      "grad_norm": 0.046088214963674545,
      "learning_rate": 0.0002877460780736957,
      "loss": 3.7915,
      "step": 2800
    },
    {
      "epoch": 0.20796468248900887,
      "grad_norm": 0.08090098947286606,
      "learning_rate": 0.00028752717986136445,
      "loss": 3.8283,
      "step": 2850
    },
    {
      "epoch": 0.21161318569057044,
      "grad_norm": 0.050669629126787186,
      "learning_rate": 0.0002873082816490332,
      "loss": 3.7318,
      "step": 2900
    },
    {
      "epoch": 0.215261688892132,
      "grad_norm": 0.04391877353191376,
      "learning_rate": 0.0002870893834367019,
      "loss": 3.7945,
      "step": 2950
    },
    {
      "epoch": 0.21891019209369356,
      "grad_norm": 0.0506870299577713,
      "learning_rate": 0.00028687048522437066,
      "loss": 3.8057,
      "step": 3000
    },
    {
      "epoch": 0.22255869529525513,
      "grad_norm": 0.13693498075008392,
      "learning_rate": 0.00028665158701203936,
      "loss": 3.7884,
      "step": 3050
    },
    {
      "epoch": 0.22620719849681667,
      "grad_norm": 0.059501904994249344,
      "learning_rate": 0.0002864326887997081,
      "loss": 3.86,
      "step": 3100
    },
    {
      "epoch": 0.22985570169837824,
      "grad_norm": 0.08309494704008102,
      "learning_rate": 0.0002862137905873768,
      "loss": 3.7692,
      "step": 3150
    },
    {
      "epoch": 0.2335042048999398,
      "grad_norm": 0.057229477912187576,
      "learning_rate": 0.0002859948923750456,
      "loss": 3.815,
      "step": 3200
    },
    {
      "epoch": 0.23715270810150135,
      "grad_norm": 0.05534157156944275,
      "learning_rate": 0.00028577599416271433,
      "loss": 3.8132,
      "step": 3250
    },
    {
      "epoch": 0.24080121130306292,
      "grad_norm": 0.05423249676823616,
      "learning_rate": 0.00028555709595038303,
      "loss": 3.8105,
      "step": 3300
    },
    {
      "epoch": 0.24444971450462447,
      "grad_norm": 0.1370498090982437,
      "learning_rate": 0.0002853381977380518,
      "loss": 3.7925,
      "step": 3350
    },
    {
      "epoch": 0.24809821770618604,
      "grad_norm": 0.050562504678964615,
      "learning_rate": 0.0002851192995257205,
      "loss": 3.7878,
      "step": 3400
    },
    {
      "epoch": 0.2517467209077476,
      "grad_norm": 0.03731625899672508,
      "learning_rate": 0.00028490040131338924,
      "loss": 3.8141,
      "step": 3450
    },
    {
      "epoch": 0.25539522410930915,
      "grad_norm": 0.057750578969717026,
      "learning_rate": 0.000284681503101058,
      "loss": 3.804,
      "step": 3500
    },
    {
      "epoch": 0.2590437273108707,
      "grad_norm": 0.08468908816576004,
      "learning_rate": 0.0002844626048887267,
      "loss": 3.804,
      "step": 3550
    },
    {
      "epoch": 0.2626922305124323,
      "grad_norm": 0.04053274542093277,
      "learning_rate": 0.00028424370667639546,
      "loss": 3.7404,
      "step": 3600
    },
    {
      "epoch": 0.26634073371399386,
      "grad_norm": 0.056923758238554,
      "learning_rate": 0.0002840248084640642,
      "loss": 3.8564,
      "step": 3650
    },
    {
      "epoch": 0.2699892369155554,
      "grad_norm": 0.061479076743125916,
      "learning_rate": 0.0002838059102517329,
      "loss": 3.7302,
      "step": 3700
    },
    {
      "epoch": 0.27363774011711695,
      "grad_norm": 0.03686347231268883,
      "learning_rate": 0.0002835870120394016,
      "loss": 3.7742,
      "step": 3750
    },
    {
      "epoch": 0.2772862433186785,
      "grad_norm": 0.09652872383594513,
      "learning_rate": 0.0002833681138270704,
      "loss": 3.7584,
      "step": 3800
    },
    {
      "epoch": 0.2809347465202401,
      "grad_norm": 0.04618903622031212,
      "learning_rate": 0.0002831492156147391,
      "loss": 3.794,
      "step": 3850
    },
    {
      "epoch": 0.28458324972180166,
      "grad_norm": 0.026881106197834015,
      "learning_rate": 0.0002829303174024078,
      "loss": 3.7537,
      "step": 3900
    },
    {
      "epoch": 0.28823175292336317,
      "grad_norm": 0.047772299498319626,
      "learning_rate": 0.0002827114191900766,
      "loss": 3.8091,
      "step": 3950
    },
    {
      "epoch": 0.29188025612492474,
      "grad_norm": 0.09770311415195465,
      "learning_rate": 0.00028249252097774534,
      "loss": 3.7222,
      "step": 4000
    },
    {
      "epoch": 0.2955287593264863,
      "grad_norm": 0.058687321841716766,
      "learning_rate": 0.00028227362276541404,
      "loss": 3.7192,
      "step": 4050
    },
    {
      "epoch": 0.2991772625280479,
      "grad_norm": 0.03348792344331741,
      "learning_rate": 0.0002820547245530828,
      "loss": 3.8284,
      "step": 4100
    },
    {
      "epoch": 0.30282576572960945,
      "grad_norm": 0.04036444425582886,
      "learning_rate": 0.00028183582634075155,
      "loss": 3.7396,
      "step": 4150
    },
    {
      "epoch": 0.30647426893117097,
      "grad_norm": 0.05977707356214523,
      "learning_rate": 0.00028161692812842025,
      "loss": 3.833,
      "step": 4200
    },
    {
      "epoch": 0.31012277213273254,
      "grad_norm": 0.05918131768703461,
      "learning_rate": 0.000281398029916089,
      "loss": 3.7626,
      "step": 4250
    },
    {
      "epoch": 0.3137712753342941,
      "grad_norm": 0.035686418414115906,
      "learning_rate": 0.00028117913170375776,
      "loss": 3.8577,
      "step": 4300
    },
    {
      "epoch": 0.3174197785358557,
      "grad_norm": 0.03383041173219681,
      "learning_rate": 0.00028096023349142646,
      "loss": 3.7575,
      "step": 4350
    },
    {
      "epoch": 0.32106828173741725,
      "grad_norm": 0.03892826288938522,
      "learning_rate": 0.0002807413352790952,
      "loss": 3.77,
      "step": 4400
    },
    {
      "epoch": 0.32471678493897876,
      "grad_norm": 0.058677367866039276,
      "learning_rate": 0.0002805224370667639,
      "loss": 3.7666,
      "step": 4450
    },
    {
      "epoch": 0.32836528814054033,
      "grad_norm": 0.043243780732154846,
      "learning_rate": 0.0002803035388544327,
      "loss": 3.8098,
      "step": 4500
    },
    {
      "epoch": 0.3320137913421019,
      "grad_norm": 0.048561178147792816,
      "learning_rate": 0.0002800846406421014,
      "loss": 3.7292,
      "step": 4550
    },
    {
      "epoch": 0.3356622945436635,
      "grad_norm": 0.050229985266923904,
      "learning_rate": 0.00027986574242977013,
      "loss": 3.7582,
      "step": 4600
    },
    {
      "epoch": 0.33931079774522505,
      "grad_norm": 0.04608413577079773,
      "learning_rate": 0.0002796468442174389,
      "loss": 3.7968,
      "step": 4650
    },
    {
      "epoch": 0.34295930094678656,
      "grad_norm": 0.03866652026772499,
      "learning_rate": 0.0002794279460051076,
      "loss": 3.7742,
      "step": 4700
    },
    {
      "epoch": 0.34660780414834813,
      "grad_norm": 0.046391092240810394,
      "learning_rate": 0.00027920904779277634,
      "loss": 3.8094,
      "step": 4750
    },
    {
      "epoch": 0.3502563073499097,
      "grad_norm": 0.05525911971926689,
      "learning_rate": 0.00027899014958044505,
      "loss": 3.7614,
      "step": 4800
    },
    {
      "epoch": 0.35390481055147127,
      "grad_norm": 0.2321861833333969,
      "learning_rate": 0.0002787712513681138,
      "loss": 3.8035,
      "step": 4850
    },
    {
      "epoch": 0.35755331375303284,
      "grad_norm": 0.032668691128492355,
      "learning_rate": 0.00027855235315578256,
      "loss": 3.7511,
      "step": 4900
    },
    {
      "epoch": 0.36120181695459436,
      "grad_norm": 0.036952581256628036,
      "learning_rate": 0.00027833345494345126,
      "loss": 3.7405,
      "step": 4950
    },
    {
      "epoch": 0.3648503201561559,
      "grad_norm": 0.03920332342386246,
      "learning_rate": 0.00027811455673112,
      "loss": 3.8023,
      "step": 5000
    },
    {
      "epoch": 0.3684988233577175,
      "grad_norm": 0.042424824088811874,
      "learning_rate": 0.00027789565851878877,
      "loss": 3.765,
      "step": 5050
    },
    {
      "epoch": 0.37214732655927907,
      "grad_norm": 0.05212566256523132,
      "learning_rate": 0.00027767676030645747,
      "loss": 3.7448,
      "step": 5100
    },
    {
      "epoch": 0.37579582976084064,
      "grad_norm": 0.03994728997349739,
      "learning_rate": 0.00027745786209412617,
      "loss": 3.8031,
      "step": 5150
    },
    {
      "epoch": 0.37944433296240215,
      "grad_norm": 0.05328117311000824,
      "learning_rate": 0.00027723896388179493,
      "loss": 3.8124,
      "step": 5200
    },
    {
      "epoch": 0.3830928361639637,
      "grad_norm": 0.03581259399652481,
      "learning_rate": 0.0002770200656694637,
      "loss": 3.7812,
      "step": 5250
    },
    {
      "epoch": 0.3867413393655253,
      "grad_norm": 0.09596124291419983,
      "learning_rate": 0.0002768011674571324,
      "loss": 3.805,
      "step": 5300
    },
    {
      "epoch": 0.39038984256708686,
      "grad_norm": 0.05078890174627304,
      "learning_rate": 0.00027658226924480114,
      "loss": 3.8448,
      "step": 5350
    },
    {
      "epoch": 0.39403834576864843,
      "grad_norm": 0.043389398604631424,
      "learning_rate": 0.0002763633710324699,
      "loss": 3.7753,
      "step": 5400
    },
    {
      "epoch": 0.39768684897020995,
      "grad_norm": 0.03501654788851738,
      "learning_rate": 0.0002761444728201386,
      "loss": 3.7624,
      "step": 5450
    },
    {
      "epoch": 0.4013353521717715,
      "grad_norm": 0.038352906703948975,
      "learning_rate": 0.00027592557460780735,
      "loss": 3.7308,
      "step": 5500
    },
    {
      "epoch": 0.4049838553733331,
      "grad_norm": 0.03946085274219513,
      "learning_rate": 0.0002757066763954761,
      "loss": 3.7477,
      "step": 5550
    },
    {
      "epoch": 0.40863235857489466,
      "grad_norm": 0.0423196405172348,
      "learning_rate": 0.0002754877781831448,
      "loss": 3.8115,
      "step": 5600
    },
    {
      "epoch": 0.41228086177645623,
      "grad_norm": 0.05357174947857857,
      "learning_rate": 0.00027526887997081356,
      "loss": 3.7472,
      "step": 5650
    },
    {
      "epoch": 0.41592936497801775,
      "grad_norm": 0.03342980891466141,
      "learning_rate": 0.0002750499817584823,
      "loss": 3.8031,
      "step": 5700
    },
    {
      "epoch": 0.4195778681795793,
      "grad_norm": 0.045062050223350525,
      "learning_rate": 0.000274831083546151,
      "loss": 3.7107,
      "step": 5750
    },
    {
      "epoch": 0.4232263713811409,
      "grad_norm": 0.04666491597890854,
      "learning_rate": 0.0002746121853338197,
      "loss": 3.7632,
      "step": 5800
    },
    {
      "epoch": 0.42687487458270246,
      "grad_norm": 0.029684990644454956,
      "learning_rate": 0.0002743932871214885,
      "loss": 3.7848,
      "step": 5850
    },
    {
      "epoch": 0.430523377784264,
      "grad_norm": 0.032057274132966995,
      "learning_rate": 0.00027417438890915723,
      "loss": 3.7159,
      "step": 5900
    },
    {
      "epoch": 0.43417188098582554,
      "grad_norm": 0.38756996393203735,
      "learning_rate": 0.00027395549069682593,
      "loss": 3.7925,
      "step": 5950
    },
    {
      "epoch": 0.4378203841873871,
      "grad_norm": 0.03629770502448082,
      "learning_rate": 0.0002737365924844947,
      "loss": 3.7831,
      "step": 6000
    },
    {
      "epoch": 0.4414688873889487,
      "grad_norm": 0.048876747488975525,
      "learning_rate": 0.00027351769427216345,
      "loss": 3.7472,
      "step": 6050
    },
    {
      "epoch": 0.44511739059051025,
      "grad_norm": 0.059854354709386826,
      "learning_rate": 0.00027329879605983215,
      "loss": 3.8124,
      "step": 6100
    },
    {
      "epoch": 0.4487658937920718,
      "grad_norm": 0.07895169407129288,
      "learning_rate": 0.0002730798978475009,
      "loss": 3.7448,
      "step": 6150
    },
    {
      "epoch": 0.45241439699363334,
      "grad_norm": 0.07532142847776413,
      "learning_rate": 0.0002728609996351696,
      "loss": 3.7715,
      "step": 6200
    },
    {
      "epoch": 0.4560629001951949,
      "grad_norm": 0.036540623754262924,
      "learning_rate": 0.00027264210142283836,
      "loss": 3.755,
      "step": 6250
    },
    {
      "epoch": 0.4597114033967565,
      "grad_norm": 0.05499234050512314,
      "learning_rate": 0.0002724232032105071,
      "loss": 3.7566,
      "step": 6300
    },
    {
      "epoch": 0.46335990659831805,
      "grad_norm": 0.20882061123847961,
      "learning_rate": 0.0002722043049981758,
      "loss": 3.7867,
      "step": 6350
    },
    {
      "epoch": 0.4670084097998796,
      "grad_norm": 0.24420703947544098,
      "learning_rate": 0.00027198540678584457,
      "loss": 3.8599,
      "step": 6400
    },
    {
      "epoch": 0.47065691300144114,
      "grad_norm": 0.08462422341108322,
      "learning_rate": 0.00027176650857351327,
      "loss": 3.7701,
      "step": 6450
    },
    {
      "epoch": 0.4743054162030027,
      "grad_norm": 0.058603543788194656,
      "learning_rate": 0.00027154761036118203,
      "loss": 3.7709,
      "step": 6500
    },
    {
      "epoch": 0.4779539194045643,
      "grad_norm": 0.036258019506931305,
      "learning_rate": 0.00027132871214885073,
      "loss": 3.7483,
      "step": 6550
    },
    {
      "epoch": 0.48160242260612585,
      "grad_norm": 0.056518469005823135,
      "learning_rate": 0.0002711098139365195,
      "loss": 3.7614,
      "step": 6600
    },
    {
      "epoch": 0.4852509258076874,
      "grad_norm": 0.04797118529677391,
      "learning_rate": 0.00027089091572418824,
      "loss": 3.8187,
      "step": 6650
    },
    {
      "epoch": 0.48889942900924893,
      "grad_norm": 0.037991564720869064,
      "learning_rate": 0.00027067201751185694,
      "loss": 3.7599,
      "step": 6700
    },
    {
      "epoch": 0.4925479322108105,
      "grad_norm": 0.15909089148044586,
      "learning_rate": 0.0002704531192995257,
      "loss": 3.734,
      "step": 6750
    },
    {
      "epoch": 0.49619643541237207,
      "grad_norm": 0.05104684457182884,
      "learning_rate": 0.00027023422108719445,
      "loss": 3.8102,
      "step": 6800
    },
    {
      "epoch": 0.49984493861393364,
      "grad_norm": 0.04384302347898483,
      "learning_rate": 0.00027001532287486315,
      "loss": 3.736,
      "step": 6850
    },
    {
      "epoch": 0.5034934418154952,
      "grad_norm": 0.03357456624507904,
      "learning_rate": 0.0002697964246625319,
      "loss": 3.7849,
      "step": 6900
    },
    {
      "epoch": 0.5071419450170568,
      "grad_norm": 0.08118319511413574,
      "learning_rate": 0.00026957752645020066,
      "loss": 3.7579,
      "step": 6950
    },
    {
      "epoch": 0.5107904482186183,
      "grad_norm": 0.05475417152047157,
      "learning_rate": 0.00026935862823786937,
      "loss": 3.7692,
      "step": 7000
    },
    {
      "epoch": 0.5144389514201799,
      "grad_norm": 0.038107018917798996,
      "learning_rate": 0.00026913973002553807,
      "loss": 3.7602,
      "step": 7050
    },
    {
      "epoch": 0.5180874546217414,
      "grad_norm": 0.05635403096675873,
      "learning_rate": 0.0002689208318132069,
      "loss": 3.7427,
      "step": 7100
    },
    {
      "epoch": 0.521735957823303,
      "grad_norm": 0.04352715238928795,
      "learning_rate": 0.0002687019336008756,
      "loss": 3.7403,
      "step": 7150
    },
    {
      "epoch": 0.5253844610248646,
      "grad_norm": 0.05898762494325638,
      "learning_rate": 0.0002684830353885443,
      "loss": 3.7781,
      "step": 7200
    },
    {
      "epoch": 0.5290329642264261,
      "grad_norm": 0.03815086930990219,
      "learning_rate": 0.00026826413717621303,
      "loss": 3.816,
      "step": 7250
    },
    {
      "epoch": 0.5326814674279877,
      "grad_norm": 0.039125699549913406,
      "learning_rate": 0.0002680452389638818,
      "loss": 3.7858,
      "step": 7300
    },
    {
      "epoch": 0.5363299706295492,
      "grad_norm": 0.1979663372039795,
      "learning_rate": 0.0002678263407515505,
      "loss": 3.7172,
      "step": 7350
    },
    {
      "epoch": 0.5399784738311108,
      "grad_norm": 0.06419413536787033,
      "learning_rate": 0.00026760744253921925,
      "loss": 3.7665,
      "step": 7400
    },
    {
      "epoch": 0.5436269770326724,
      "grad_norm": 0.044370170682668686,
      "learning_rate": 0.000267388544326888,
      "loss": 3.7519,
      "step": 7450
    },
    {
      "epoch": 0.5472754802342339,
      "grad_norm": 0.04969647526741028,
      "learning_rate": 0.0002671696461145567,
      "loss": 3.8177,
      "step": 7500
    },
    {
      "epoch": 0.5509239834357955,
      "grad_norm": 0.21754583716392517,
      "learning_rate": 0.00026695074790222546,
      "loss": 3.7718,
      "step": 7550
    },
    {
      "epoch": 0.554572486637357,
      "grad_norm": 0.0563935711979866,
      "learning_rate": 0.00026673184968989416,
      "loss": 3.7828,
      "step": 7600
    },
    {
      "epoch": 0.5582209898389185,
      "grad_norm": 0.0720403641462326,
      "learning_rate": 0.0002665129514775629,
      "loss": 3.716,
      "step": 7650
    },
    {
      "epoch": 0.5618694930404802,
      "grad_norm": 0.12610460817813873,
      "learning_rate": 0.0002662940532652316,
      "loss": 3.7034,
      "step": 7700
    },
    {
      "epoch": 0.5655179962420417,
      "grad_norm": 0.05261334776878357,
      "learning_rate": 0.00026607515505290037,
      "loss": 3.7634,
      "step": 7750
    },
    {
      "epoch": 0.5691664994436033,
      "grad_norm": 0.035278305411338806,
      "learning_rate": 0.00026585625684056913,
      "loss": 3.8057,
      "step": 7800
    },
    {
      "epoch": 0.5728150026451648,
      "grad_norm": 0.07556525617837906,
      "learning_rate": 0.00026563735862823783,
      "loss": 3.7993,
      "step": 7850
    },
    {
      "epoch": 0.5764635058467263,
      "grad_norm": 0.037905529141426086,
      "learning_rate": 0.0002654184604159066,
      "loss": 3.8026,
      "step": 7900
    },
    {
      "epoch": 0.580112009048288,
      "grad_norm": 0.04708358272910118,
      "learning_rate": 0.0002651995622035753,
      "loss": 3.725,
      "step": 7950
    },
    {
      "epoch": 0.5837605122498495,
      "grad_norm": 0.03775063157081604,
      "learning_rate": 0.00026498066399124404,
      "loss": 3.7516,
      "step": 8000
    },
    {
      "epoch": 0.5874090154514111,
      "grad_norm": 0.07850445061922073,
      "learning_rate": 0.0002647617657789128,
      "loss": 3.8048,
      "step": 8050
    },
    {
      "epoch": 0.5910575186529726,
      "grad_norm": 0.055150218307971954,
      "learning_rate": 0.0002645428675665815,
      "loss": 3.7952,
      "step": 8100
    },
    {
      "epoch": 0.5947060218545341,
      "grad_norm": 0.04658922925591469,
      "learning_rate": 0.00026432396935425025,
      "loss": 3.7206,
      "step": 8150
    },
    {
      "epoch": 0.5983545250560958,
      "grad_norm": 0.12437862902879715,
      "learning_rate": 0.000264105071141919,
      "loss": 3.7605,
      "step": 8200
    },
    {
      "epoch": 0.6020030282576573,
      "grad_norm": 0.04028536006808281,
      "learning_rate": 0.0002638861729295877,
      "loss": 3.7845,
      "step": 8250
    },
    {
      "epoch": 0.6056515314592189,
      "grad_norm": 0.04889799281954765,
      "learning_rate": 0.0002636672747172564,
      "loss": 3.7051,
      "step": 8300
    },
    {
      "epoch": 0.6093000346607804,
      "grad_norm": 0.0449523963034153,
      "learning_rate": 0.0002634483765049252,
      "loss": 3.8262,
      "step": 8350
    },
    {
      "epoch": 0.6129485378623419,
      "grad_norm": 0.03449857980012894,
      "learning_rate": 0.0002632294782925939,
      "loss": 3.7999,
      "step": 8400
    },
    {
      "epoch": 0.6165970410639036,
      "grad_norm": 0.040457263588905334,
      "learning_rate": 0.0002630105800802626,
      "loss": 3.7261,
      "step": 8450
    },
    {
      "epoch": 0.6202455442654651,
      "grad_norm": 0.03202451765537262,
      "learning_rate": 0.0002627916818679314,
      "loss": 3.7404,
      "step": 8500
    },
    {
      "epoch": 0.6238940474670267,
      "grad_norm": 0.035345982760190964,
      "learning_rate": 0.00026257278365560013,
      "loss": 3.7975,
      "step": 8550
    },
    {
      "epoch": 0.6275425506685882,
      "grad_norm": 0.07346878945827484,
      "learning_rate": 0.00026235388544326884,
      "loss": 3.7793,
      "step": 8600
    },
    {
      "epoch": 0.6311910538701497,
      "grad_norm": 0.041771192103624344,
      "learning_rate": 0.0002621349872309376,
      "loss": 3.763,
      "step": 8650
    },
    {
      "epoch": 0.6348395570717114,
      "grad_norm": 0.038838401436805725,
      "learning_rate": 0.00026191608901860635,
      "loss": 3.7834,
      "step": 8700
    },
    {
      "epoch": 0.6384880602732729,
      "grad_norm": 0.042152874171733856,
      "learning_rate": 0.00026169719080627505,
      "loss": 3.7367,
      "step": 8750
    },
    {
      "epoch": 0.6421365634748345,
      "grad_norm": 0.061544034630060196,
      "learning_rate": 0.0002614782925939438,
      "loss": 3.7704,
      "step": 8800
    },
    {
      "epoch": 0.645785066676396,
      "grad_norm": 0.07182334363460541,
      "learning_rate": 0.00026125939438161256,
      "loss": 3.8205,
      "step": 8850
    },
    {
      "epoch": 0.6494335698779575,
      "grad_norm": 0.023128552362322807,
      "learning_rate": 0.00026104049616928126,
      "loss": 3.7306,
      "step": 8900
    },
    {
      "epoch": 0.6530820730795192,
      "grad_norm": 0.1577276885509491,
      "learning_rate": 0.00026082159795695,
      "loss": 3.786,
      "step": 8950
    },
    {
      "epoch": 0.6567305762810807,
      "grad_norm": 0.03565877676010132,
      "learning_rate": 0.0002606026997446187,
      "loss": 3.7752,
      "step": 9000
    },
    {
      "epoch": 0.6603790794826423,
      "grad_norm": 0.027303671464323997,
      "learning_rate": 0.00026038380153228747,
      "loss": 3.7949,
      "step": 9050
    },
    {
      "epoch": 0.6640275826842038,
      "grad_norm": 0.054411400109529495,
      "learning_rate": 0.0002601649033199562,
      "loss": 3.7443,
      "step": 9100
    },
    {
      "epoch": 0.6676760858857653,
      "grad_norm": 0.04960021376609802,
      "learning_rate": 0.00025994600510762493,
      "loss": 3.7069,
      "step": 9150
    },
    {
      "epoch": 0.671324589087327,
      "grad_norm": 0.09200795739889145,
      "learning_rate": 0.0002597271068952937,
      "loss": 3.7692,
      "step": 9200
    },
    {
      "epoch": 0.6749730922888885,
      "grad_norm": 0.03438730165362358,
      "learning_rate": 0.0002595082086829624,
      "loss": 3.7694,
      "step": 9250
    },
    {
      "epoch": 0.6786215954904501,
      "grad_norm": 0.035373587161302567,
      "learning_rate": 0.00025928931047063114,
      "loss": 3.7894,
      "step": 9300
    },
    {
      "epoch": 0.6822700986920116,
      "grad_norm": 0.043179698288440704,
      "learning_rate": 0.00025907041225829984,
      "loss": 3.7602,
      "step": 9350
    },
    {
      "epoch": 0.6859186018935731,
      "grad_norm": 0.028434082865715027,
      "learning_rate": 0.0002588515140459686,
      "loss": 3.79,
      "step": 9400
    },
    {
      "epoch": 0.6895671050951347,
      "grad_norm": 0.023637497797608376,
      "learning_rate": 0.00025863261583363735,
      "loss": 3.6746,
      "step": 9450
    },
    {
      "epoch": 0.6932156082966963,
      "grad_norm": 0.04148092865943909,
      "learning_rate": 0.00025841371762130606,
      "loss": 3.7167,
      "step": 9500
    },
    {
      "epoch": 0.6968641114982579,
      "grad_norm": 0.037714675068855286,
      "learning_rate": 0.0002581948194089748,
      "loss": 3.8069,
      "step": 9550
    },
    {
      "epoch": 0.7005126146998194,
      "grad_norm": 0.1351778656244278,
      "learning_rate": 0.00025797592119664357,
      "loss": 3.7434,
      "step": 9600
    },
    {
      "epoch": 0.7041611179013809,
      "grad_norm": 0.04339450225234032,
      "learning_rate": 0.00025775702298431227,
      "loss": 3.7455,
      "step": 9650
    },
    {
      "epoch": 0.7078096211029425,
      "grad_norm": 0.0386631041765213,
      "learning_rate": 0.00025753812477198097,
      "loss": 3.7262,
      "step": 9700
    },
    {
      "epoch": 0.7114581243045041,
      "grad_norm": 0.029608257114887238,
      "learning_rate": 0.0002573192265596497,
      "loss": 3.7308,
      "step": 9750
    },
    {
      "epoch": 0.7151066275060657,
      "grad_norm": 0.044863514602184296,
      "learning_rate": 0.0002571003283473185,
      "loss": 3.7836,
      "step": 9800
    },
    {
      "epoch": 0.7187551307076272,
      "grad_norm": 0.03969186171889305,
      "learning_rate": 0.0002568814301349872,
      "loss": 3.7858,
      "step": 9850
    },
    {
      "epoch": 0.7224036339091887,
      "grad_norm": 0.033198028802871704,
      "learning_rate": 0.00025666253192265594,
      "loss": 3.805,
      "step": 9900
    },
    {
      "epoch": 0.7260521371107503,
      "grad_norm": 0.04711172729730606,
      "learning_rate": 0.0002564436337103247,
      "loss": 3.7364,
      "step": 9950
    },
    {
      "epoch": 0.7297006403123119,
      "grad_norm": 0.04380730539560318,
      "learning_rate": 0.0002562247354979934,
      "loss": 3.7527,
      "step": 10000
    },
    {
      "epoch": 0.7333491435138735,
      "grad_norm": 0.03492945432662964,
      "learning_rate": 0.00025600583728566215,
      "loss": 3.788,
      "step": 10050
    },
    {
      "epoch": 0.736997646715435,
      "grad_norm": 0.040144167840480804,
      "learning_rate": 0.0002557869390733309,
      "loss": 3.787,
      "step": 10100
    },
    {
      "epoch": 0.7406461499169965,
      "grad_norm": 0.3803487718105316,
      "learning_rate": 0.0002555680408609996,
      "loss": 3.7476,
      "step": 10150
    },
    {
      "epoch": 0.7442946531185581,
      "grad_norm": 0.03278876096010208,
      "learning_rate": 0.00025534914264866836,
      "loss": 3.7914,
      "step": 10200
    },
    {
      "epoch": 0.7479431563201197,
      "grad_norm": 0.050172772258520126,
      "learning_rate": 0.0002551302444363371,
      "loss": 3.8092,
      "step": 10250
    },
    {
      "epoch": 0.7515916595216813,
      "grad_norm": 0.0357128269970417,
      "learning_rate": 0.0002549113462240058,
      "loss": 3.7576,
      "step": 10300
    },
    {
      "epoch": 0.7552401627232428,
      "grad_norm": 0.02880084700882435,
      "learning_rate": 0.0002546924480116745,
      "loss": 3.7878,
      "step": 10350
    },
    {
      "epoch": 0.7588886659248043,
      "grad_norm": 0.041418302804231644,
      "learning_rate": 0.0002544735497993433,
      "loss": 3.7377,
      "step": 10400
    },
    {
      "epoch": 0.7625371691263659,
      "grad_norm": 0.05331307649612427,
      "learning_rate": 0.00025425465158701203,
      "loss": 3.7702,
      "step": 10450
    },
    {
      "epoch": 0.7661856723279274,
      "grad_norm": 0.037208449095487595,
      "learning_rate": 0.00025403575337468073,
      "loss": 3.8176,
      "step": 10500
    },
    {
      "epoch": 0.7698341755294891,
      "grad_norm": 0.1868438571691513,
      "learning_rate": 0.0002538168551623495,
      "loss": 3.7435,
      "step": 10550
    },
    {
      "epoch": 0.7734826787310506,
      "grad_norm": 0.03965349867939949,
      "learning_rate": 0.00025359795695001824,
      "loss": 3.7943,
      "step": 10600
    },
    {
      "epoch": 0.7771311819326121,
      "grad_norm": 0.04679163917899132,
      "learning_rate": 0.00025337905873768694,
      "loss": 3.7727,
      "step": 10650
    },
    {
      "epoch": 0.7807796851341737,
      "grad_norm": 0.05407506600022316,
      "learning_rate": 0.0002531601605253557,
      "loss": 3.7681,
      "step": 10700
    },
    {
      "epoch": 0.7844281883357352,
      "grad_norm": 0.052542489022016525,
      "learning_rate": 0.0002529412623130244,
      "loss": 3.6909,
      "step": 10750
    },
    {
      "epoch": 0.7880766915372969,
      "grad_norm": 0.04365282878279686,
      "learning_rate": 0.00025272236410069316,
      "loss": 3.8204,
      "step": 10800
    },
    {
      "epoch": 0.7917251947388584,
      "grad_norm": 0.03474139794707298,
      "learning_rate": 0.0002525034658883619,
      "loss": 3.7248,
      "step": 10850
    },
    {
      "epoch": 0.7953736979404199,
      "grad_norm": 0.03610363230109215,
      "learning_rate": 0.0002522845676760306,
      "loss": 3.7517,
      "step": 10900
    },
    {
      "epoch": 0.7990222011419815,
      "grad_norm": 0.039271511137485504,
      "learning_rate": 0.00025206566946369937,
      "loss": 3.7778,
      "step": 10950
    },
    {
      "epoch": 0.802670704343543,
      "grad_norm": 0.039804987609386444,
      "learning_rate": 0.00025184677125136807,
      "loss": 3.7286,
      "step": 11000
    },
    {
      "epoch": 0.8063192075451047,
      "grad_norm": 0.16726399958133698,
      "learning_rate": 0.0002516278730390368,
      "loss": 3.7227,
      "step": 11050
    },
    {
      "epoch": 0.8099677107466662,
      "grad_norm": 0.04762920364737511,
      "learning_rate": 0.0002514089748267055,
      "loss": 3.7642,
      "step": 11100
    },
    {
      "epoch": 0.8136162139482277,
      "grad_norm": 0.03606169670820236,
      "learning_rate": 0.0002511900766143743,
      "loss": 3.7091,
      "step": 11150
    },
    {
      "epoch": 0.8172647171497893,
      "grad_norm": 0.033103834837675095,
      "learning_rate": 0.00025097117840204304,
      "loss": 3.7931,
      "step": 11200
    },
    {
      "epoch": 0.8209132203513508,
      "grad_norm": 0.039384398609399796,
      "learning_rate": 0.00025075228018971174,
      "loss": 3.7181,
      "step": 11250
    },
    {
      "epoch": 0.8245617235529125,
      "grad_norm": 0.04030264914035797,
      "learning_rate": 0.0002505333819773805,
      "loss": 3.7372,
      "step": 11300
    },
    {
      "epoch": 0.828210226754474,
      "grad_norm": 0.0351419635117054,
      "learning_rate": 0.00025031448376504925,
      "loss": 3.761,
      "step": 11350
    },
    {
      "epoch": 0.8318587299560355,
      "grad_norm": 0.046962421387434006,
      "learning_rate": 0.00025009558555271795,
      "loss": 3.796,
      "step": 11400
    },
    {
      "epoch": 0.8355072331575971,
      "grad_norm": 0.042807839810848236,
      "learning_rate": 0.0002498766873403867,
      "loss": 3.7763,
      "step": 11450
    },
    {
      "epoch": 0.8391557363591586,
      "grad_norm": 0.054092634469270706,
      "learning_rate": 0.00024965778912805546,
      "loss": 3.7508,
      "step": 11500
    },
    {
      "epoch": 0.8428042395607203,
      "grad_norm": 0.03967931121587753,
      "learning_rate": 0.00024943889091572416,
      "loss": 3.7436,
      "step": 11550
    },
    {
      "epoch": 0.8464527427622818,
      "grad_norm": 0.03748660907149315,
      "learning_rate": 0.00024921999270339286,
      "loss": 3.7681,
      "step": 11600
    },
    {
      "epoch": 0.8501012459638433,
      "grad_norm": 0.06830700486898422,
      "learning_rate": 0.0002490010944910617,
      "loss": 3.7668,
      "step": 11650
    },
    {
      "epoch": 0.8537497491654049,
      "grad_norm": 0.04274776205420494,
      "learning_rate": 0.0002487821962787304,
      "loss": 3.8394,
      "step": 11700
    },
    {
      "epoch": 0.8573982523669664,
      "grad_norm": 0.03649476543068886,
      "learning_rate": 0.0002485632980663991,
      "loss": 3.7252,
      "step": 11750
    },
    {
      "epoch": 0.861046755568528,
      "grad_norm": 0.0675521269440651,
      "learning_rate": 0.00024834439985406783,
      "loss": 3.7771,
      "step": 11800
    },
    {
      "epoch": 0.8646952587700896,
      "grad_norm": 0.045794982463121414,
      "learning_rate": 0.0002481255016417366,
      "loss": 3.7628,
      "step": 11850
    },
    {
      "epoch": 0.8683437619716511,
      "grad_norm": 0.036884695291519165,
      "learning_rate": 0.0002479066034294053,
      "loss": 3.7131,
      "step": 11900
    },
    {
      "epoch": 0.8719922651732127,
      "grad_norm": 0.05138828232884407,
      "learning_rate": 0.00024768770521707404,
      "loss": 3.7466,
      "step": 11950
    },
    {
      "epoch": 0.8756407683747742,
      "grad_norm": 0.11742337793111801,
      "learning_rate": 0.0002474688070047428,
      "loss": 3.7503,
      "step": 12000
    },
    {
      "epoch": 0.8792892715763359,
      "grad_norm": 0.03942801430821419,
      "learning_rate": 0.0002472499087924115,
      "loss": 3.7665,
      "step": 12050
    },
    {
      "epoch": 0.8829377747778974,
      "grad_norm": 0.16825155913829803,
      "learning_rate": 0.00024703101058008026,
      "loss": 3.7422,
      "step": 12100
    },
    {
      "epoch": 0.8865862779794589,
      "grad_norm": 0.03790099173784256,
      "learning_rate": 0.00024681211236774896,
      "loss": 3.7001,
      "step": 12150
    },
    {
      "epoch": 0.8902347811810205,
      "grad_norm": 0.048273518681526184,
      "learning_rate": 0.0002465932141554177,
      "loss": 3.8242,
      "step": 12200
    },
    {
      "epoch": 0.893883284382582,
      "grad_norm": 0.05202070251107216,
      "learning_rate": 0.00024637431594308647,
      "loss": 3.7517,
      "step": 12250
    },
    {
      "epoch": 0.8975317875841436,
      "grad_norm": 0.0339135117828846,
      "learning_rate": 0.00024615541773075517,
      "loss": 3.7364,
      "step": 12300
    },
    {
      "epoch": 0.9011802907857052,
      "grad_norm": 0.05662667378783226,
      "learning_rate": 0.0002459365195184239,
      "loss": 3.7294,
      "step": 12350
    },
    {
      "epoch": 0.9048287939872667,
      "grad_norm": 0.04340604320168495,
      "learning_rate": 0.0002457176213060926,
      "loss": 3.7599,
      "step": 12400
    },
    {
      "epoch": 0.9084772971888283,
      "grad_norm": 0.03602934628725052,
      "learning_rate": 0.0002454987230937614,
      "loss": 3.7934,
      "step": 12450
    },
    {
      "epoch": 0.9121258003903898,
      "grad_norm": 0.051781877875328064,
      "learning_rate": 0.0002452798248814301,
      "loss": 3.757,
      "step": 12500
    },
    {
      "epoch": 0.9157743035919514,
      "grad_norm": 0.05975133180618286,
      "learning_rate": 0.00024506092666909884,
      "loss": 3.8141,
      "step": 12550
    },
    {
      "epoch": 0.919422806793513,
      "grad_norm": 0.02861228585243225,
      "learning_rate": 0.0002448420284567676,
      "loss": 3.7278,
      "step": 12600
    },
    {
      "epoch": 0.9230713099950745,
      "grad_norm": 0.03803494572639465,
      "learning_rate": 0.0002446231302444363,
      "loss": 3.7608,
      "step": 12650
    },
    {
      "epoch": 0.9267198131966361,
      "grad_norm": 0.040464699268341064,
      "learning_rate": 0.00024440423203210505,
      "loss": 3.7474,
      "step": 12700
    },
    {
      "epoch": 0.9303683163981976,
      "grad_norm": 0.035990022122859955,
      "learning_rate": 0.0002441853338197738,
      "loss": 3.7693,
      "step": 12750
    },
    {
      "epoch": 0.9340168195997592,
      "grad_norm": 0.12842823565006256,
      "learning_rate": 0.0002439664356074425,
      "loss": 3.7205,
      "step": 12800
    },
    {
      "epoch": 0.9376653228013208,
      "grad_norm": 0.04690799117088318,
      "learning_rate": 0.00024374753739511124,
      "loss": 3.7619,
      "step": 12850
    },
    {
      "epoch": 0.9413138260028823,
      "grad_norm": 0.03609974682331085,
      "learning_rate": 0.00024352863918278,
      "loss": 3.8023,
      "step": 12900
    },
    {
      "epoch": 0.9449623292044439,
      "grad_norm": 0.048785604536533356,
      "learning_rate": 0.00024330974097044872,
      "loss": 3.6927,
      "step": 12950
    },
    {
      "epoch": 0.9486108324060054,
      "grad_norm": 0.033076245337724686,
      "learning_rate": 0.00024309084275811745,
      "loss": 3.7412,
      "step": 13000
    },
    {
      "epoch": 0.952259335607567,
      "grad_norm": 0.028776904568076134,
      "learning_rate": 0.0002428719445457862,
      "loss": 3.7878,
      "step": 13050
    },
    {
      "epoch": 0.9559078388091286,
      "grad_norm": 0.02954328991472721,
      "learning_rate": 0.00024265304633345493,
      "loss": 3.7784,
      "step": 13100
    },
    {
      "epoch": 0.9595563420106901,
      "grad_norm": 0.08677668124437332,
      "learning_rate": 0.00024243414812112366,
      "loss": 3.7609,
      "step": 13150
    },
    {
      "epoch": 0.9632048452122517,
      "grad_norm": 0.03310631215572357,
      "learning_rate": 0.0002422152499087924,
      "loss": 3.7892,
      "step": 13200
    },
    {
      "epoch": 0.9668533484138132,
      "grad_norm": 0.03703955188393593,
      "learning_rate": 0.00024199635169646114,
      "loss": 3.7195,
      "step": 13250
    },
    {
      "epoch": 0.9705018516153748,
      "grad_norm": 0.0360308475792408,
      "learning_rate": 0.00024177745348412984,
      "loss": 3.7718,
      "step": 13300
    },
    {
      "epoch": 0.9741503548169363,
      "grad_norm": 0.041493192315101624,
      "learning_rate": 0.00024155855527179857,
      "loss": 3.6603,
      "step": 13350
    },
    {
      "epoch": 0.9777988580184979,
      "grad_norm": 0.07808513939380646,
      "learning_rate": 0.00024133965705946733,
      "loss": 3.6857,
      "step": 13400
    },
    {
      "epoch": 0.9814473612200595,
      "grad_norm": 0.13988623023033142,
      "learning_rate": 0.00024112075884713606,
      "loss": 3.7568,
      "step": 13450
    },
    {
      "epoch": 0.985095864421621,
      "grad_norm": 0.040506236255168915,
      "learning_rate": 0.00024090186063480479,
      "loss": 3.7456,
      "step": 13500
    },
    {
      "epoch": 0.9887443676231826,
      "grad_norm": 0.04095887765288353,
      "learning_rate": 0.00024068296242247351,
      "loss": 3.7838,
      "step": 13550
    },
    {
      "epoch": 0.9923928708247441,
      "grad_norm": 0.02780856378376484,
      "learning_rate": 0.00024046406421014227,
      "loss": 3.727,
      "step": 13600
    },
    {
      "epoch": 0.9960413740263057,
      "grad_norm": 0.03426589444279671,
      "learning_rate": 0.000240245165997811,
      "loss": 3.7816,
      "step": 13650
    },
    {
      "epoch": 0.9996898772278673,
      "grad_norm": 0.04505148530006409,
      "learning_rate": 0.00024002626778547973,
      "loss": 3.7641,
      "step": 13700
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.6197409629821777,
      "eval_runtime": 3.4942,
      "eval_samples_per_second": 28.619,
      "eval_steps_per_second": 3.72,
      "step": 13705
    }
  ],
  "logging_steps": 50,
  "max_steps": 68525,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0189959796359168e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
